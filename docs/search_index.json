[
["index.html", "IMA Math-to-Industry Bootcamp 2019: Statistics! 1 Home", " IMA Math-to-Industry Bootcamp 2019: Statistics! 1 Home This material was compiled by Alicia Johnson (Macalester College) for the Summer 2018 “Math-to-Industry Boot Camp” at the Institute of Mathematics &amp; Its Applications (IMA) at the University of Minnesota, and updated by David Shuman (Macalester College) for the Summer 2019 “Math-to-Industry Boot Camp.” This work is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License. "],
["syllabus.html", " 2 Syllabus", " 2 Syllabus "],
["statistics-bootcamp-goals-and-approach.html", "2.1 Statistics Bootcamp Goals and Approach", " 2.1 Statistics Bootcamp Goals and Approach Hone your statistical, data, and computing literacy Instead of covering all statistical modeling and inference techniques in 5 days (impossible!), focus on a couple of foundational and generalizable tools: linear regression and simple classification. In doing so, we’ll bypass topics in traditional stat intros Favor statistical applications using real data over statistical theory so that you walk away with a sophisticated set of tools with real applications Play around with the RStudio statistical software. In doing so, focus on the patterns in and potential of this software. Don’t worry about memorizing syntax - this will come with experience. For example, by the end of the week you’ll likely be comfortable with ggplot() and lm() functions simply because we’ll use them a lot! Do some messy stuff. Too often, stat classes are taught with data that are nice and tidy. In the real world, data are messy and require cleaning/wrangling. As discussed in this New York Times article, “Data scientists…spend from 50 percent to 80 percent of their time mired in this more mundane labor of collecting and preparing unruly digital data, before it can be explored for useful nuggets.” Though data wrangling isn’t the focus of this bootcamp, it will be a useful and necessary part of it. Don’t worry / get too distracted by the extra coding this requires. The goal is simply for you to start recognizing the messiness of real world data and to build up confidence in dealing with it "],
["schedule.html", "2.2 Schedule", " 2.2 Schedule The schedule is in flux and likely to change throughout the week. Pre-bootcamp Homework Get up and running with RStudio. Day 1: How can we model/explain the variability in our sample data? Exploring and explaining variability using visualizations and regression models. Topics will include: multivariate visualizations multivariate regression models with covariate, categorical, interaction, and transformation terms estimation of model parameters via least squares Day 2: How do we select a model? How good is the model? Selecting a statistical model using subset selection &amp; measuring model quality. Discussion will include: residual analysis \\(R^2\\) &amp; mean squared prediction error (MSPE) cross validation overfitting bias-variance trade-off Day 2 Bonus: Data wrangling An introduction to six main data verbs in the tidyverse: mutate, select, filter, arrange, group_by, and summarize Day 3: What does this sample model tell us about trends in the broader population? Using data from a sample to make inferences about a broader population of interest. Topics will include: sampling distributions &amp; the Central Limit Theorem standard error bootstrapping prediction &amp; confidence intervals Day 4: What does this sample model tell us about trends in the population? How can we carefully interpret and communicate our conclusions? Continuing inferential techniques (hypothesis testing) and discussing common pitfalls in statistical analyses: Simpson’s Paradox multicollinearity multiple testing statistical vs practical significance errors in hypothesis testing Day 5: What if our linear regression tools aren’t appropriate for my particular analysis? Extending our foundations to nonparametric regression and classification techniques for modeling categorical variables. Topics might include: nonparametric regression (eg: KNN, trees) logistic regression nonparametric classification tools (eg: trees &amp; forests) a nudge into machine learning, the topic of this course’s sequel "],
["software-requirements.html", "2.3 Software Requirements", " 2.3 Software Requirements Statistical applications utilize data. Working with modern (large, messy) data sets requires statistical software. We’ll exclusively use RStudio. Why? it’s free it’s open source it has a huge online community it’s the industry standard it can be used to create reproducible and elegant documents (eg: your bootcamp materials!) Additional benefits of RStudio include: compelling visualization tools (e.g., ggplot2, plotly, leaflet, dygraphs, shiny, DiagrammeR), including dashboards and interactive apps that allow for automatic data updating efficient data wrangling and exploratory data analysis tools (the tidyverse) powerful statistical modeling and machine learning tools large online community continuously developing new, open packages makes workflows reproducible with R Markdown computing codes and narratives stored in the same document -&gt; easy for others to understand or pick up on your analysis results automatically generated from source code -&gt; when data changes, easy to update amenable to collaborative work (e.g., interaction with GitHub, internal package development) Here is a case study on how Airbnb Uses R To get started, take the following two steps in the given order. Further, if you already have R/RStudio, make sure to update to the most recent versions before the bootcamp. Download &amp; install R: https://mirror.las.iastate.edu/CRAN/ Download &amp; install RStudio Desktop (Open Source License): https://www.rstudio.com/products/rstudio/download/ Be sure to download the free version!! What’s the difference between R and RStudio? Mainly, RStudio requires R – thus it does everything R does and more. We will be using RStudio exclusively. You’ll take a quick tour in your pre-bootcamp homework. -->"],
["resources.html", " 3 Resources", " 3 Resources Instructor contact info: David Shuman (dshuman@macalester.edu) Website with all course materials: https://www.macalester.edu/~dshuman1/ima/bootcamp/index.html Additional resources: R for Data Science, by Hadley Wickham and Garrett Grolemund, 2017 Modern Data Science with R, by Benjamin Baumer, Daniel Kaplan, and Nicholas Horton, 2017 An Introduction to Statistical Learning, Gareth James, Daniela Witten, Trevor Hastie, and Robert Tibshirani, 2013 Collection of favorite cheat sheets All RStudio cheat sheets R Markdown: The Definitive Guide by Yihui Xie, J. J. Allaire, and Garrett Grolemund, 2018 R Markdown reference ggplot2 reference R Programming Wikibook Debugging in R: document and video Colors in R A list of useR groups and R-Ladies groups around the world "],
["course-notes.html", " 4 Course Notes", " 4 Course Notes "],
["day-1-visualizing-modeling-variability.html", "4.1 Day 1: Visualizing &amp; Modeling Variability", " 4.1 Day 1: Visualizing &amp; Modeling Variability 4.1.1 Getting Started If you arrive early, now’s a good time to get started! Find today’s notes at https://dshuman1.github.io/IMA_bootcamp_2019/day-1-visualizing-modeling-variability.html &amp; take the following steps: Open a new RMarkdown (Rmd) document: Erase everything in the Rmd. At the top, type the following: # Day 1 Notes ```{r warning = FALSE, message = FALSE} library(ggplot2) library(dplyr) library(readr) library(choroplethr) ``` Knit your document! It’s helpful to do this early and often. If you get any errors, it’s likely because you haven’t yet installed the packages above. If this is the case, return to Exercise 5.4 of the pre-bootcamp homework. Overall workshop plan (&amp; how it ties in with the machine learning module): Today’s plan: Welcome / intros A motivating example with a pre-bootcamp homework recap Visualizing relationships Modeling relationships with linear regression 4.1.2 Pre-Boot Camp Review Statistics is the practice of using data from a sample to make inferences about some broader population of interest. Data is a key word here. In the pre-bootcamp homework, you explored the first simple steps of a statistical analysis: familiarizing yourself with the structure of your data and conducting some simple univariate analyses. We’ll build upon this foundation in a motivating example. The Story: Since the 2016 election, people have examined the following questions: How did Trump’s support vary from county to county? How does this compare to trends in past elections? In what ways was Trump’s support associated with county demographics? We’ll explore these questions using county-level election outcomes made available by Tony McGovern on github: elect&lt;-read.csv(&quot;https://www.macalester.edu/~dshuman1/ima/bootcamp/data/county_election_results.csv&quot;) Explore the structure of the data # Check out the first rows of elect. ID the cases &amp; variables # How much data do we have? # What are the variable names? # Compute the total votes nationwide for each party Explore win_2016 (categorical) # Construct a bar chart ggplot(___, aes(___)) + geom____() Explore perrep_2016 (quantitative) # Blank canvas ggplot(elect, aes(x = perrep_2016)) # Histogram ggplot(elect, aes(x = perrep_2016)) + geom____() # Density plot ggplot(elect, aes(x = perrep_2016)) + geom____() Numerically summarize the trend &amp; variability in perrep_2016 # Trump&#39;s mean &amp; median support # Variance &amp; st dev in Trump&#39;s support # Calculate the exact 2.5th &amp; 97.5th percentiles 4.1.3 Explaining Variability The main goal in statistical modeling is often to explain the variation in one particular variable from case to case (the response variable) using the values of one or more other variables (the explanatory or predictor variables). THINK We now have a good sense for the trends &amp; variability in Trump’s support from county to county. What other variables (ie. county features) might explain some of this variability? In examining relationships between variables, we distinguish between the response &amp; predictors: response variable: Trump’s percent of the vote (the variable whose variability we would like to explain) predictors (aka explanatory variables aka features): percent white, per capita income, state color, etc (variables that might explain some of the variability in the response) None of these predictors are contained within the elect data, but we’re in luck: the df_county_demographics data set within the choroplethr package contains county level demographic data the RedBlueStates data at https://www.macalester.edu/~ajohns24/Data/RedBluePurple.csv categorizes each county as belonging to a blue/red/purple state based on state categorizations at http://www.270towin.com/. JOINING DATA SETS Step 1: load new data Think: What are the sources of these data? When were they collected? Do they suit our purposes? # Load demographic data from choroplethr package data(&quot;df_county_demographics&quot;) head(df_county_demographics, 3) ## region total_population percent_white percent_black percent_asian ## 1 1001 54907 76 18 1 ## 2 1003 187114 83 9 1 ## 3 1005 27321 46 46 0 ## percent_hispanic per_capita_income median_rent median_age ## 1 2 24571 668 37.5 ## 2 4 26766 693 41.5 ## 3 5 16829 382 38.3 # Load RedBluePurple data RedBlue &lt;- read.csv(&quot;https://www.macalester.edu/~ajohns24/Data/RedBluePurple.csv&quot;) head(RedBlue, 3) ## region polyname abb StateColor ## 1 1001 alabama AL red ## 2 1003 alabama AL red ## 3 1005 alabama AL red Step 2: join the 3 datasets into a single dataset The 3 data sets (elect, df_county_demographics, RedBlue) need to share a common identifying variable. In this case, they all contain the region variable, and we can join them according to it: # Join elect and df_county_demographics all_data &lt;- left_join(elect,df_county_demographics,by=c(&quot;region&quot;=&quot;region&quot;)) # Join all_data with RedBlue all_data &lt;- left_join(all_data, RedBlue,by=c(&quot;region&quot;=&quot;region&quot;)) names(all_data) ## [1] &quot;region&quot; &quot;county&quot; &quot;total_2008&quot; ## [4] &quot;dem_2008&quot; &quot;gop_2008&quot; &quot;oth_2008&quot; ## [7] &quot;total_2012&quot; &quot;dem_2012&quot; &quot;gop_2012&quot; ## [10] &quot;oth_2012&quot; &quot;total_2016&quot; &quot;dem_2016&quot; ## [13] &quot;gop_2016&quot; &quot;oth_2016&quot; &quot;perdem_2016&quot; ## [16] &quot;perrep_2016&quot; &quot;winrep_2016&quot; &quot;perdem_2012&quot; ## [19] &quot;perrep_2012&quot; &quot;winrep_2012&quot; &quot;total_population&quot; ## [22] &quot;percent_white&quot; &quot;percent_black&quot; &quot;percent_asian&quot; ## [25] &quot;percent_hispanic&quot; &quot;per_capita_income&quot; &quot;median_rent&quot; ## [28] &quot;median_age&quot; &quot;polyname&quot; &quot;abb&quot; ## [31] &quot;StateColor&quot; Step 3: if all else fails… If you run into errors (likely due to a missing package), you can load the data from here: ```r all_data &lt;- read.csv(&quot;https://www.macalester.edu/~ajohns24/Data/electionDemographics16.csv&quot;) ``` 4.1.4 Visualizing Relationships Basic Rules for Constructing Visualizations Instead of memorizing which plot is appropriate for which situation, it’s best to recognize patterns in constructing viz: Each quantitative variable requires a new axis. If we run out of axes, we can illustrate the scale of a quantitative variable using color or discretize it into groups &amp; treat it as categorical. Each categorical variable requires a new way to “group” the graphic (eg: using colors, shapes, separate facets, etc to capture the grouping) It is helpful to visual the data table. Here arejust 6 counties: perrep_2016 perrep_2012 median_rent StateColor winrep_2016 abb 87.94 83.75 367 red TRUE OK 79.95 75.11 391 red TRUE MS 35.82 41.37 742 purple FALSE GA 80.15 72.84 420 purple TRUE MO 73.53 72.52 948 blue TRUE CO 46.55 44.91 582 blue TRUE MN THINK Before visualizing the relationships among these variables, we need to understand what features these should have. Sketches: How might we visualize the relationships among the following sets of variables? For each one, draw a sketch of a graphic on paper. perrep_2016 vs perrep_2012 perrep_2016 vs StateColor perrep_2016 vs percent_white and StateColor (in 1 plot) perrep_2016 vs percent_white and median_rent (in 1 plot) Run through the following exercises which introduce different approaches to visualizing relationships. In doing so, don’t just construct a plot, examine what it tells us about relationship trends &amp; strength (degree of variability from the trend) as well as outliers or deviations from the trend. For each plot, think: what’s the take-home message? Scatterplots of 2 quantitative variables: perrep_2016 vs perrep_2012 Each quantitative variable has an axis. Each case is represented by a dot. # Start with a blank canvas ggplot(all_data, aes(y = perrep_2016, x = perrep_2012)) # Add a scatterplot layer ggplot(all_data, aes(y = perrep_2016, x = perrep_2012)) + geom_point() # Use text labels instead of points ggplot(all_data, aes(y = perrep_2016, x = perrep_2012)) + geom_text(aes(label = abb)) # Another predictor ggplot(all_data, aes(y = perrep_2016, x = median_rent)) + geom_point() # Another predictor ggplot(all_data, aes(y = perrep_2016, x = percent_white)) + geom_point() Side-by-side plots of 1 quantitative variable vs 1 categorical variable: perrep_2016 vs StateColor # Density plots by group ggplot(all_data, aes(x = perrep_2016, fill = StateColor)) + geom_density() # Add transparency &amp; fix colors ggplot(all_data, aes(x = perrep_2016, fill = StateColor)) + geom_density(alpha = 0.5) + scale_fill_manual(values = c(&quot;blue&quot;,&quot;purple&quot;,&quot;red&quot;)) # Split groups into separate plots ggplot(all_data, aes(x = perrep_2016, fill = StateColor)) + geom_density() + scale_fill_manual(values = c(&quot;blue&quot;,&quot;purple&quot;,&quot;red&quot;)) + facet_wrap( ~ StateColor) # Histograms instead ggplot(all_data, aes(x = perrep_2016, fill = StateColor)) + geom_histogram() + scale_fill_manual(values = c(&quot;blue&quot;,&quot;purple&quot;,&quot;red&quot;)) + facet_wrap( ~ StateColor) # Side-by-side box plots ggplot(all_data, aes(x=StateColor,y = perrep_2016, fill = StateColor)) + geom_boxplot() + scale_fill_manual(values = c(&quot;blue&quot;,&quot;purple&quot;,&quot;red&quot;)) # Change to individual states ggplot(all_data, aes(x=abb,y = perrep_2016, fill = StateColor)) + geom_boxplot() + scale_fill_manual(values = c(&quot;blue&quot;,&quot;purple&quot;,&quot;red&quot;)) Scatterplots of 1 quantitative variable vs 1 categorical &amp; 1 quantitative variable: perrep_2016 vs percent_white and StateColor If percent_white and StateColor both explain some of the variability in perrep_2016, why not include both in our analysis?! Let’s. # Scatterplot colored by group ggplot(all_data, aes(y = perrep_2016, x = percent_white, color = StateColor)) + scale_color_manual(values = c(&quot;blue&quot;,&quot;purple&quot;,&quot;red&quot;)) + geom_point(alpha = 0.5) # Scatterplots split by group ggplot(all_data, aes(y = perrep_2016, x = percent_white, color = StateColor)) + geom_point(alpha = 0.5) + scale_color_manual(values = c(&quot;blue&quot;,&quot;purple&quot;,&quot;red&quot;)) + facet_wrap( ~ StateColor) Plots of 3 quantitative variables: perrep_2016 vs percent_white and median_rent # Scatterplot: use color to represent 3rd variable ggplot(all_data, aes(y = perrep_2016, x = percent_white, color = median_rent)) + geom_point(alpha=0.5) # Scatterplot: discretize the 3rd variable ggplot(all_data, aes(y = perrep_2016, x = percent_white, color = cut(median_rent, 2))) + geom_point(alpha = 0.5) Extra: Maps! There is, of course, a geographical component to these data. Though we won’t cover spatial models in bootcamp, the visuals still help us tell a story. If the required choroplethrMaps package isn’t working for you, work with a neighbor (don’t spend too much time with this picky package). # Load choroplethrMaps library(choroplethrMaps) # Map of Trump wins all_data &lt;- mutate(all_data, value = winrep_2016) county_choropleth(all_data) # Map of Trump support all_data &lt;- mutate(all_data, value = perrep_2016) county_choropleth(all_data) # Map of percent white all_data &lt;- mutate(all_data, value=percent_white) county_choropleth(all_data) Glyph-Ready Data In the layered grammar of graphics used by ggplot, we can add different aesthetics (features such as position, size, shape, color) to each glyph (mark/symbol such as a point or bar or density curve) It is important to recognize that we first need to rearrange the data so that one row (case) of the table corresponds to each glyph. The data we just used were already in this so called “glyph-ready form” We’ll talk more as the bootcamp proceeds about how to rearrange data so that they are in this form using data wrangling commands 4.1.5 Linear Regression Models Just as when exploring single variables, there are limitations in relying solely on visualizations to analyze relationships among 2+ variables. Statistical models provide rigorous numerical summaries of relationship trends. Before going into details, examine the plots below and draw a model that captures the trend of the relationships being illustrated. Linear regression can be used to model each of these relationships. “Linear” here indicates that the linear regression model of a response variable is a linear combination of explanatory variables. It does not mean that the relationship itself is linear!! In general, let \\(y\\) be our response variable and (\\(x_1, x_2, ..., x_k\\)) be \\(k\\) explanatory variables. Then the (population) linear regression model of \\(y\\) vs the \\(x_i\\) is \\[y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\cdots + \\beta_k x_k\\] where \\(\\beta_0\\) = intercept coefficient average \\(y\\) value when \\(x_1=x_2=\\cdots=x_k=0\\) \\(\\beta_i\\) = \\(x_i\\) coefficient when holding constant all other \\(x\\), the change \\(y\\) when we increase \\(x_i\\) by 1 In RStudio, we construct sample estimates of linear regression models using the lm() (linear model) function. Consider a simple example: my_model &lt;- lm(y ~ x1 + x2, data = mydata) summary(my_model) Today we’ll focus on visualizing, constructing, and interpreting models. We’ll talk more tomorrow about model quality &amp; deviations from the model trend. IMPORTANT: Be sure to interpret the coefficients in a contextually meaningful way that tells the audience about the relationships of interest (as opposed to simply providing a definition). Models with 1 quantitative predictor: perrep_2016 vs median_age Visualize the relationship, add a regression line, &amp; construct the regression model: ggplot(all_data, aes(x = median_age, y = perrep_2016)) + geom_point(alpha = 0.25) + geom_smooth(method = &quot;lm&quot;) model1 &lt;- lm(perrep_2016 ~ median_age, data=all_data) summary(model1) Write out the estimated model formula: perrep_2016 = ___ + ___ median_age Interpret all model coefficients. What is the model’s estimate of the percentage that voted Republican in 2016 for a county with a median age of 40? a median age of 22? # Method 1 new_ages&lt;-c(22,40) model1$coefficients[1]+model1$coefficients[2]*new_ages # Method 2 new_vals=data.frame(median_age=new_ages) predict(model1,new_vals) # Method 3 library(mosaic) f&lt;-makeFun(model1) f(new_ages) Models with 1 categorical predictor: perrep_2016 vs StateColor Visualize the relationship. ggplot(all_data, aes(x = perrep_2016, fill = StateColor)) + geom_density(alpha = 0.5)+ scale_fill_manual(values=c(&quot;blue&quot;,&quot;purple&quot;,&quot;red&quot;)) % group_by(StateColor) %>% summarize(means = mean(perrep_2016, na.rm = TRUE)) ``` --> Construct the regression model and write out the estimated model formula: perrep_2016 = ___ + ___ StateColorpurple + ___ StateColorred model2 &lt;- lm(perrep_2016 ~ StateColor, data = all_data) summary(model2) Huh?! RStudio splits categorical predictors up into a reference group (the first alphabetically) and indicators for the other groups. Here, blue states are the reference group and \\[\\text{StateColorpurple} = \\begin{cases} 1 &amp; \\text{ if purple} \\\\ 0 &amp; \\text{ otherwise} \\\\ \\end{cases} \\;\\;\\;\\; \\text{ and } \\;\\;\\;\\; \\text{StateColorred} = \\begin{cases} 1 &amp; \\text{ if red} \\\\ 0 &amp; \\text{ otherwise} \\\\ \\end{cases}\\] In other words, the StateColor variable is turned into 3 “dummy variables”: \\[\\left(\\begin{array}{c} \\text{red} \\\\ \\text{purple} \\\\ \\text{purple} \\\\ \\text{blue} \\\\ \\text{red} \\end{array}\\right) \\;\\;\\; \\to \\;\\;\\; \\text{StateColorblue} = \\left(\\begin{array}{c} 0 \\\\ 0 \\\\ 0 \\\\ 1 \\\\ 0 \\end{array}\\right), \\;\\; \\text{StateColorpurple} = \\left(\\begin{array}{c} 0 \\\\ 1 \\\\ 1 \\\\ 0 \\\\ 0 \\end{array}\\right), \\;\\; \\text{StateColorred} = \\left(\\begin{array}{c} 1 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 1 \\end{array}\\right)\\] Since these sum to 1, we only need to put 2 into our model and leave the other out as a reference level. With these ideas in mind, interpret all coefficients in your model. With these ideas in mind, we can interpret all coefficients in our model. Specifically, we can plug in 0’s and 1’s to obtain 3 separate model values for the blue, purple, and red states. For example, for purple states, we have that the predicted value is Intercept + 1*StateColorpurple + 0*StateColorred = 55.665309 + 6.476411 = 62.14172, which not surprisingly is equal to the mean percentage Republican in all counties in purple states. Models with 1 quantitative predictor &amp; 1 categorical predictor: perrep_2016 vs median_age and StateColor Construct the regression model, visualize the relationship, and add regression lines for each state color. NOTE: Unfortunately, as you’ll see in homework, the geom_smooth lines don’t directly correspond to the model, so we need to add lines manually. model3 &lt;- lm(perrep_2016 ~ median_age + StateColor, data = all_data) summary(model3) ggplot(all_data, aes(x = median_age, y = perrep_2016, color = StateColor)) + geom_point(alpha = 0.25) + geom_line(aes(y=model3$fitted.values))+ scale_color_manual(values=c(&quot;blue&quot;,&quot;purple&quot;,&quot;red&quot;)) Write out the estimated model formula: perrep_2016 = ___ + ___ median_age + ___ StateColorpurple + ___ StateColorred Again, the presence of a categorical variable results in the separation of the model formula by group. To this end, plug in 0’s and 1’s to obtain 3 separate model formulas for the blue, purple, and red states: perrep_2016 = ___ + ___ median_age. Putting it all together, interpret all coefficients in your model. NOTE The median_age coefficient in model3 differs from that in model1 and the StateColor coefficients in model3 differ from those in model2. This is because a predictor’s coefficients are defined / interpreted differently depending on what other predictors are in the model. Models with 2 quantitative predictors: perrep_2016 vs median_age and median_rent Visualize the relationships and construct the regression model: ggplot(all_data, aes(y = perrep_2016, x = median_age, color = median_rent)) + geom_point(alpha = 0.5) model4 &lt;- lm(perrep_2016 ~ median_age + median_rent, data = all_data) summary(model4) Write out the estimated model formula: perrep_2016 = ___ + ___ median_age + ___ median_rent Is this the formula for a line? Multiple lines? A plane? Interpret the coefficient on median_age. Models with 2 categorical predictors: perrep_2016 vs StateColor and income_bracket For illustration’s sake, let’s split county per_capita_income (in $) into 2 income brackets: above or below $25,000: # Define income brackets all_data &lt;- all_data %&gt;% mutate(income_bracket = cut(per_capita_income, breaks = c(0,25000,65000), labels = c(&quot;low&quot;,&quot;high&quot;))) Visualize the relationship and construct the model. ggplot(all_data, aes(x = perrep_2016, fill = income_bracket)) + geom_density(alpha = 0.5) + facet_wrap(~ StateColor) model5 &lt;- lm(perrep_2016 ~ StateColor + income_bracket, data = all_data) summary(model5) Write out the estimated model formula. How many possible combinations are there of these two explanatory variables? What are the predicted values for all combinations? "],
["day-2a-data-wrangling.html", "4.2 Day 2a: Data Wrangling", " 4.2 Day 2a: Data Wrangling Getting started: As you settle in, start a new Rmd and load the following packages at the top: library(tidyverse) library(lubridate) library(DT) library(mosaic) library(fivethirtyeight) library(boot) If you don’t have boot installed, then install it first. In the console: install.packages(&quot;boot&quot;, dependencies = TRUE) Then add this command to load a portion of the data on US births: Birthdays&lt;-select(Birthdays,state,date,year,births) Today’s plan: Discuss yesterday’s homework &amp; tie up any loose ends Data wrangling Model assumptions Measuring model quality residual analysis \\(R^2\\) &amp; MSPE cross validation overfitting bias-variance trade-off 4.2.1 US Births The number of daily births in the US varies over the year and from day to day. What’s surprising to many people is that the variation from one day to the next can be huge: some days have only about 80% as many births as others. Why? In this activity we’ll use basic data wrangling skills to understand some drivers of daily births. The data table Birthdays in the mosaicData package gives the number of births recorded on each day of the year in each state from 1969 to 1988.1 Table 4.1: A subset of the initial birthday data. state date year births AK 1969-01-01 1969 14 AL 1969-01-01 1969 174 AR 1969-01-01 1969 78 AZ 1969-01-01 1969 84 CA 1969-01-01 1969 824 CO 1969-01-01 1969 100 4.2.2 Data Wrangling Introduction 4.2.2.1 Tidy Data Additional reading: Wickham, Tidy Data Wickham and Grolemund, Tidy Data There are different ways to store and represent the same data. In order to be consistent and to also take advantage of the vectorized nature of R, the tidyverse packages we’ll use provide a set of three interrelated rules/conventions for a dataset to be tidy: Each variable must have its own column. Each observation must have its own row. Each value must have its own cell. One of the first things we’ll often do when acquiring new data is to “tidy it” into this form. For now, we can already start thinking of a data frame (tibble) as a table whose rows are the individual cases and whose columns are the variables on which we have information for each individual case. The data import cheat sheet has a figure summarizing this principle. 4.2.2.2 Data Verbs Additional reading: Wickham and Grolemund, Data Transformation Kaplan, Data Computing, Chapters 7 and 9 There are six main data transformation verbs in the dplyr library. Each verb takes an input data frame along with additional arguments specifying the action, and returns a new data frame. We’ll examine them in three pairs. 4.2.2.2.1 Verbs that change the variables (columns) but not the cases (rows) The first two verbs change which variables (columns) are included in the data frame, but preserve the same set of cases (rows). select() chooses which columns to keep, or put another way, deletes those colummns that are not selected. To specify the columns, we can either list them out, or use functions like starts_with(), ends_with(), or contains() to specify the titles of the variables we wish to keep. mutate() adds one or more columns to the data frame. Each column is a function of the other columns that is applied on a row by row basis. For example, we can use arithmetic operations like adding two other variables or logical operations like checking if two columns are equal, or equal to a target number. Example 4.1 (select and mutate) Add two new variables to the Birthdays data: one that has only the last two digits of the year, and one that states whether there were more than 100 births in the given state on the given date. Then form a new table that only has three columns: the state and your two new columns. What does the following operation return: select(Birthdays,ends_with(&quot;te&quot;))? Solution. The commands for the first two parts are BirthdaysExtra &lt;- mutate(Birthdays, year_short=year-1900, busy_birthday=(births&gt;100)) BirthdaysExtraTable &lt;- select(BirthdaysExtra,state, year_short,busy_birthday) The operation in (c) selects only the first two columns state and date. 4.2.2.2.2 Verbs that change the cases (rows) but not the variables (columns) The next two verbs change which cases (rows) are included in the data frame, but preserve the same set of variables (columns). filter() deletes some of the rows by specifying which rows to keep. arrange() reorders the rows according to a specified criteria. To sort in reverse order based on the variable x, use arrange(desc(x)). Example 4.2 (filter and arrange) Create a table with only births in Massachusetts in 1979, and sort the days from those with the most births to those with the fewest. Solution. We want to filter and then arrange: MABirths1979 &lt;- filter(Birthdays, state==&quot;MA&quot;, year==1979) MABirths1979Sorted &lt;- arrange(MABirths1979, desc(births)) DT::datatable(MABirths1979Sorted, options = list(pageLength = 6),caption=&quot;Birthdays in Massachusetts in 1979, sorted from those dates with the most births to those dates with the fewest births.&quot;) When filtering, we often use logical comparison operators like ==, &gt;, &lt;, &gt;= (greater than or equal to), &lt;= (less than or equal to), and %in%, which compares the value to a list of entries.2 For example, if we want all births in AK, CA, and MA, we can write filter(Birthdays, state %in% c(&quot;AK&quot;,&quot;CA&quot;,&quot;MA&quot;)) The c() here is for concatenate, which is how we form vectors in R. 4.2.2.2.3 Grouped summaries summarise() (or equivalently summarize()) takes an entire data frame as input and outputs a single row with one or more summary statistics, such as mean, sum, sd, n_distinct(), or n() (which, like tally(), just counts the number of entries). summarise(Birthdays,total_births=sum(births), average_births=mean(births), nstates=n_distinct(state),ncases=n()) ## total_births average_births nstates ncases ## 1 70486538 189 51 372864 So summarise changes both the cases and the variables. Alone, summarise is not all that useful, because we can also access individual variables directly with the dollar sign. For example, to find the total and average births, we can write sum(Birthdays$births) ## [1] 70486538 mean(Birthdays$births) ## [1] 189 Rather, we will mostly use it to create grouped summaries, which brings us to the last of the six main data verbs. group_by() groups the cases of a data frame by a specified set of variables. The size of the stored data frame does not actually change (neither the cases nor the variables change), but then other functions can be applied to the specified groups instead of the entire data set. We’ll often use group_by in conjunction with summarise to get a grouped summary. Example 4.3 (Grouped summary) Find the average number of daily births (per state) in each year. Find the average number of daily births in each year, by state. Solution. We have to first group by the desired grouping and then perform a summarise. BirthdaysYear&lt;-group_by(Birthdays,year) summarise(BirthdaysYear, average=mean(births)) ## # A tibble: 20 x 2 ## year average ## &lt;int&gt; &lt;dbl&gt; ## 1 1969 192. ## 2 1970 200. ## 3 1971 191. ## 4 1972 175. ## 5 1973 169. ## 6 1974 170. ## 7 1975 169. ## 8 1976 170. ## 9 1977 179. ## 10 1978 179. ## 11 1979 188. ## 12 1980 194. ## 13 1981 195. ## 14 1982 198. ## 15 1983 196. ## 16 1984 197. ## 17 1985 202. ## 18 1986 202. ## 19 1987 205. ## 20 1988 210. BirthdaysYearState&lt;-group_by(Birthdays,year,state) summarise(BirthdaysYearState, average=mean(births)) ## # A tibble: 1,020 x 3 ## # Groups: year [20] ## year state average ## &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 1969 AK 18.6 ## 2 1969 AL 174. ## 3 1969 AR 91.3 ## 4 1969 AZ 93.3 ## 5 1969 CA 954. ## 6 1969 CO 110. ## 7 1969 CT 134. ## 8 1969 DC 75.3 ## 9 1969 DE 27.6 ## 10 1969 FL 292. ## # … with 1,010 more rows 4.2.2.3 Piping Additional reading: Wickham and Grolemund, Combining Multiple Operations with the Pipe Wickham and Grolemund, Pipes Pipes offer an efficient way to execute multiple operations at once. Here is a more efficient way to redo Example 4.2 with the pipe: QuickMABirths1979&lt;- Birthdays %&gt;% filter(state==&quot;MA&quot;,year==1979) %&gt;% arrange(desc(births)) With the pipe notation, x%&gt;%f(y) becomes f(x,y), where in the first line here, x is Birthdays, the function f is filter, and y is state==&quot;MA&quot;,year==1979. The really nice thing about piping is that you can chain together a bunch of different operations without having to save the intermediate results. This is what we have done above by chaining together a filter followed by an arrange. 4.2.2.4 Manipulating Dates Additional reading: Wickham and Grolemund, Date and Times with lubridate The date variable in Birthdays prints out in the conventional, human-readable way. But it is actually in a format (called POSIX date format) that automatically respects the order of time. The lubridate package contains helpful functions that will extract various information about any date. Here are some you might find useful: year() month() week() yday() — gives the day of the year as a number 1-366. This is often called the “Julian day.” mday() — gives the day of the month as a number 1-31 wday() — gives the weekday (e.g. Monday, Tuesday, …). Use the optional argument label=TRUE to have the weekday spelled out rather than given as a number 1-7. Using these lubridate functions, you can easily look at the data in more detail. For example, we can add columns to the date table for month and day of the week:3 Birthdays&lt;- Birthdays %&gt;% mutate(month=month(date,label=TRUE), weekday=wday(date,label=TRUE)) Here is what the data table looks like with our new columns: Table 4.2: A subset of the birthday data with additional variables. state date year births month weekday AK 1969-01-01 1969 14 Jan Wed AL 1969-01-01 1969 174 Jan Wed AR 1969-01-01 1969 78 Jan Wed AZ 1969-01-01 1969 84 Jan Wed CA 1969-01-01 1969 824 Jan Wed CO 1969-01-01 1969 100 Jan Wed Example 4.4 Make a table showing the five states with the most births between September 9, 1979 and September 11, 1979, inclusive. Arrange the table in descending order of births. Solution. The plan of attack is to first filter the dates, then group by state, then use a summarise to add up totals for each state, and finally arrange them in descending order to find the top 5.4 SepTable&lt;- Birthdays %&gt;% filter(date &gt;= ymd(19790909), date &lt;=ymd(19790911)) %&gt;% group_by(state) %&gt;% summarise(total=sum(births)) %&gt;% arrange(desc(total)) %&gt;% head(n=5) knitr::kable( SepTable[,], caption = &#39;States with the most births between September 9, 1979 and September 11, 1979, inclusive.&#39; ) Table 4.3: States with the most births between September 9, 1979 and September 11, 1979, inclusive. state total CA 3246 TX 2347 NY 1943 IL 1673 OH 1408 4.2.3 Exercises: Baby Names We are going to practice the six data verbs on the babynames dataset: Table 4.4: A subset of the babynames data, which runs from 1880-2015 and is provided by the US Social Security Administration. year sex name n prop 1880 F Mary 7065 0.0724 1880 F Anna 2604 0.0267 1880 F Emma 2003 0.0205 1880 F Elizabeth 1939 0.0199 1880 F Minnie 1746 0.0179 1880 F Margaret 1578 0.0162 Exercise 4.1 Add a new boolean (true or false) variable called has2000 that indicates whether there were more than 2000 babies of that sex with that name in each year. Display the first six rows of your new table. Exercise 4.2 Find the number of total babies per year, sorted by most babies to least babies. Exercise 4.3 Find the twelve most popular names overall (i.e., totaled over all year and sexes), ordered by popularity. Exercise 4.4 Find the most popular names for males, over all years and ordered by popularity. Exercise 4.5 Find the most popular names for females, over all years and ordered by popularity. Exercise 4.6 Calculate the number of babies born each decade, and arrange them in descending order. Calculating the decade may be the trickiest part of this question! Exercise 4.7 Calculate the most popular name for each year. Print out the answer for the years 2006-2015. This is tricky, but try Googling for hints if you are stuck. The fivethirtyeight package has more recent data.↩ Important note about = vs. ==: A single = is an assignment operator that assigns the value after the equal sign to the variable before the equal sign. We saw an example of this above with year_short=year-1900. In order to compare whether two values are the same, we need to use the double equal == as in year==1979.↩ The label=TRUE argument tells month to return a string abbreviation for the month instead of the month’s number.↩ The verbs head(n=5), tail(n=3) are often used just after an arrange to keep, e.g., only the first 5 entries or last 3 entries, where n specifies the number of entries to keep.↩ "],
["day-2b-model-assumptions-measuring-model-quality.html", "4.3 Day 2b: Model Assumptions &amp; Measuring Model Quality", " 4.3 Day 2b: Model Assumptions &amp; Measuring Model Quality MOTIVATION One of the most famous quotes in statistics is the following from George Box (1919–2013): “All models are wrong, but some are useful.” Thus far, we’ve constructed models from sample data &amp; used these to tell stories about the relationships among variables of interest. We haven’t yet discussed the quality of these models. Today, we’ll focus on the following questions: Does our model meet the model assumptions? How well does our model explain the variability in the response? How accurate are the predictions calculated from this model? 4.3.1 Regression Assumptions &amp; Residual Analysis Let \\(y\\) be a response variable with a set of \\(k\\) predictors \\((x_{1}, x_{2}, ..., x_{k})\\). Then the population linear regression model is \\[y = \\beta_0 + \\beta_1 x_{1} + \\beta_2 x_{2} + \\cdots + \\beta_k x_{k} + \\varepsilon\\] where \\(\\beta_0 + \\beta_1 x_{1} + \\beta_2 x_{2} + \\cdots + \\beta_k x_{k}\\) captures the trend of the relationship \\(\\epsilon\\) reflects individual deviation from the trend (residual) In “ordinary” least squares regression, there are 2 key assumptions: Assumption 1: The observations of (\\(y,x_1,x_2,...,x_k\\)) for any case are independent of the observations for any other case. Assumption 2: At any set of predictor values \\((x_{1}^*, x_{2}^*, \\ldots, x_{k}^*)\\), \\[\\varepsilon \\sim N(0,\\sigma^2)\\] That is: the expected value of the residuals is \\(E(\\varepsilon) =0\\) In words: Across the entire model, responses are balanced above &amp; below the trend. Thus the model accurately describes the “shape” and “location” of the trend. homoskedasticity: the variance of the residuals \\(Var(\\varepsilon) = \\sigma^2\\) In words: Across the entire model, variability from the trend is roughly constant. the \\(\\varepsilon\\) are normally distributed In words: individual responses are normally distributed around the trend (closer to the trend and then tapering off) Let’s build some intuition for these assumptions. Come up with some examples that violate Assumption 1. For the plots below, indicate which parts of Assumption 2 hold. For each part a, b, and c of Assumption 2, discuss the consequences/severity of violating the assumption. Checking the model assumptions Recall Galton’s examination of the relationship between a person’s height and the height of their father: data(Galton) ggplot(Galton, aes(y = height, x = father)) + geom_point() + geom_smooth(method=&quot;lm&quot;) galton_mod &lt;- lm(height ~ father, data = Galton) summary(galton_mod) ## ## Call: ## lm(formula = height ~ father, data = Galton) ## ## Residuals: ## Min 1Q Median 3Q Max ## -10.268 -2.669 -0.209 2.634 11.933 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 39.1104 3.2271 12.12 &lt;2e-16 *** ## father 0.3994 0.0466 8.57 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 3.45 on 896 degrees of freedom ## Multiple R-squared: 0.0758, Adjusted R-squared: 0.0748 ## F-statistic: 73.5 on 1 and 896 DF, p-value: &lt;2e-16 Do the assumptions appear to hold for this model? We quickly lose the ability to visualize a model as the number of predictors increases. Instead of checking the assumptions by eye, we can construct residual plots. First, put the data together: # Combine the observed responses, predictions, &amp; residuals mod_results &lt;- data.frame(observed = Galton$height, predicted = galton_mod$fitted.values, residual = galton_mod$residuals) head(mod_results, 3) ## observed predicted residual ## 1 73.2 70.46 2.738 ## 2 69.2 70.46 -1.262 ## 3 69.0 70.46 -1.462 We can then check out two plots: # Plot residuals vs predictions ggplot(mod_results, aes(y = residual, x = predicted)) + geom_point() + geom_hline(yintercept = 0) #a Q-Q plot of the residuals ggplot(mod_results, aes(sample = residual)) + geom_qq() Residual Analysis Summary Assumption Consequence Diagnostic Solution independence inaccurate inference common sense / context use a different modeling technique \\(E(\\varepsilon)=0\\) lack of model fit plot of residuals vs predictions transform \\(x\\) and/or \\(y\\) \\(Var(\\varepsilon)=\\sigma^2\\) inaccurate inference plot of residuals vs predictions transform \\(y\\) normality of \\(\\varepsilon\\) if extreme, inaccurate inference Q-Q plot if extreme, transform \\(y\\) 4.3.2 Measuring model quality: \\(R^2\\) &amp; MSPE Meeting the model assumptions isn’t the only important piece of the model evaluation process. We also need a sense of the accuracy in using this model to understand and make predictions about \\(y\\). For this exercise, we’ll use data from the fivethirtyeight article The Ultimate Halloween Candy Power Ranking. These data were produced from this experiment which presented subjects with a series of head-to-head candy matchups and asked them to indicate which candy they preferred. You can load these data from the fivethirtyeight package: data(&quot;candy_rankings&quot;) ?candy_rankings # Store under a shorter name candy &lt;- candy_rankings Let’s explore the structure of these data! # Check out the head head(candy) # Arrange from least to most popular candy %&gt;% arrange(winpercent) # Arrange from most to least popular candy %&gt;% arrange(desc(winpercent)) Models Our ultimate goal is to understand the variability in winpercent from candy to candy: ggplot(candy, aes(x = winpercent)) + geom_histogram(color = &quot;white&quot;, binwidth = 5) Visualize the following relationships. Which appears to be best? # Plot: winpercent vs chocolate # model_1: winpercent vs chocolate # Plot: winpercent vs sugarpercent # model_2: winpercent vs sugarpercent # Plot: winpercent vs chocolate &amp; sugarpercent # model_3: winpercent vs chocolate &amp; sugarpercent (NO interaction) Check out the residuals. Which model did the best job of predicting the winpercent for 100 Grand, the first candy in the dataset? Mean Squared Prediction Error (MSPE) To measure the overall quality of the models, we need to combine the residuals for all of the cases / candy. It might be tempting to calculate the mean residual but this is always 0 within rounding: mean(model_1$residual) ## [1] 7.755e-16 We’ll consider 2 different methods, the first being the mean square prediction error: \\[MSPE = \\frac{1}{n}\\sum_{i=1}^n (y_i - \\hat{y}_i)^2 = \\frac{(y_1 - \\hat{y}_1)^2 + (y_2 - \\hat{y}_2)^2 + \\cdots (y_n - \\hat{y}_n)^2}{n}\\] # Calculate the MSPE for model_1 # Calculate the MSPE for model_2 # Calculate the MSPE for model_3 R2 \\(R^2\\) is a more interpretable measure of model quality. \\(R^2\\) is restricted to be between 0 and 1 - it is the proportion of the variability in \\(y\\) that’s explained by the model. Thus an \\(R^2\\) of 0 indicates that the model doesn’t explain any of the variability in \\(y\\); an \\(R^2\\) of 1 indicates that the model perfectly explains the variability in \\(y\\): In fact, \\(R^2\\) is related to MSPE! Letting \\(\\overline{y} = \\frac{1}{n} \\sum_{i=1}^n y_i\\) denote the sample mean of \\(y\\), the total sum of squares measures the total squared deviations of \\(y_i\\) from \\(\\overline{y}\\): \\[TSS = \\sum_{i=1}^n (y_i - \\overline{y})^2 = (y_1 - \\overline{y})^2 + (y_2 - \\overline{y})^2 + \\cdots (y_n - \\overline{y})^2\\] Then \\(R^2\\) can be calculated by the MSPE and TSS: \\[R^2 = 1 - \\frac{MSPE}{TSS/n} = 1 - \\frac{\\sum_{i=1}^n (y_i - \\hat{y}_i)^2}{\\sum_{i=1}^n (y_i - \\overline{y})^2}\\] We don’t have to calculate this by hand. It’s reported in the model summary() table: summary(model_1) summary(model_2) summary(model_3) NOTE: \\(R^2\\) can also be calculated by a ratio of variances: \\[R^2 = \\frac{\\text{Var}(\\hat{y}_i)}{\\text{Var}(y_i)} = 1 - \\frac{\\text{Var}( y_i - \\hat{y}_i)}{\\text{Var}(y_i)}\\] In conclusion (so far) Which of the models is “best” with respect to MSPE? With respect to \\(R^2\\)? In general, what happens to MSPE &amp; \\(R^2\\) as we add more terms to the model? In general, as we add more terms to the model, MSPE decreases and \\(R^2\\) increases. As we will discuss, this does not necessarily mean we should keep adding more and more predictor variables to our models. 4.3.3 An experiment Split up into 6 groups. Each group will be given their own set of data that includes measurements on 40 adult males, including a measure of body fat percentage, fatSiri: group_data &lt;- read.csv(&quot;https://www.macalester.edu/~ajohns24/data/bodyfat?????.csv&quot;) # Remove 2 variables so you&#39;re not tempted to use them group_data &lt;- group_data %&gt;% select(-c(fatBrozek, fatFreeWeight)) Working within your group, use these data to develop the best predictive model of fatSiri. (For example, what model would you give to a doctor that wished to predict body fat percentage from physical measurements?) The rules: Even if you know some clever tools and techniques that would help with this task, stick to intuition &amp; tools you’ve learned in the bootcamp thus far. Record the following in this Google sheet: A name for your group. Your estimated sample model, specifying both your chosen set of predictors \\(x_i\\) &amp; sample coefficients \\(\\hat{\\beta}_i\\): \\[\\text{fatSiri} = \\hat{\\beta}_0 + \\hat{\\beta}_1 x_1 + \\hat{\\beta}_2 x_2 + \\cdots + \\hat{\\beta}_k x_k\\] Your model’s MSPE. 4.3.4 Measuring model quality: cross validation Our post-experiment discussions highlight a couple of important themes: Training and testing our model using the same data results in overly optimistic assessments of model quality. For example, in-sample or training errors (ie. MSPE calculated using the same data that we used to train the model) tend to be smaller than testing errors (ie. MSPE calculated using data not used to train the model). Adding terms to a model might improve measures of model quality calculated using the same data that were used to build the model, but can result in overfitting. We’ll consider a different measure of model quality that addresses some of these concerns: cross validation. Throughout this discussion, we’ll all use the same data and compare the following 2 models: body_data &lt;- read.csv(&quot;https://www.macalester.edu/~ajohns24/data/bodyfat50.csv&quot;) # Fit models model_1 &lt;- lm(fatSiri ~ weight, body_data) model_12 &lt;- lm(fatSiri ~ poly(weight,12), body_data) # Plot the models ggplot(body_data, aes(y = fatSiri, x = weight)) + geom_point() + stat_smooth(method=&quot;lm&quot;, se=FALSE) ggplot(body_data, aes(y = fatSiri, x = weight)) + geom_point() + stat_smooth(method=&quot;lm&quot;, formula=y~poly(x, 12), se=FALSE) You can but don’t have to confirm the in-sample \\(R^2\\) and MSPE for these 2 models: Model In-sample \\(R^2\\) In-sample MSPE model_1 0.5424 32.48 model_12 0.6403 25.53 Think Which model, model_1 or model_12, has the “best” \\(R^2\\) and MSPE measurements? Suppose we observe 50 more adults. Which model do you think would do a better job at predicting these adults’ fatSiri from their weight? Why? Validation In practice, we only have one sample of data. We need to use this one sample to both train and test our model. Consider a simple strategy where we randomly select half of the sample to train the model and test the model on the other half. To ensure that we all get the same samples and can reproduce our results, we’ll set the random number generating seed to the same number (2000). We’ll discuss this in detail as a class! # There are 40 people dim(body_data) ## [1] 40 19 # Set the random number seed set.seed(2000) # Randomly sample half (20) of these for training data_train &lt;- sample_n(body_data, size = 20) dim(data_train) ## [1] 20 19 # Take the the other 20 for testing data_test &lt;- dplyr::setdiff(body_data, data_train) dim(data_test) ## [1] 20 19 Using the training data: Fit the model of fatSiri by weight (the simple one) and calculate the training MSPE. # Fit the model train_mod &lt;- lm(fatSiri ~ weight, data = ___) # Calculate training MSPE How well does this model generalize to the test set? Use the training model to predict fatSiri for the test cases: # Make predictions test_predictions &lt;- predict(train_mod, newdata = data.frame(weight = ___)) # Calculate residuals test_residuals &lt;- ___ # Calculate testing MSPE mean(test_residuals^2) Compare the MSPE of the training and test sets. Did the training model over- or under-estimate its prediction error? That is, are the results better or worse than what was promised by the training model? Extra evidence: Compare the regression model for the training set (blue) to that for the test set (red): We could use the MSPE for the test set to measure how well our model generalizes to the population. But what might be the flaws in this approach? Can you think of a better idea? We’ll discuss this as a class before moving on… 2-fold cross validation The validation approach we used above used data_train to build the model and then tested this model on data_test. Let’s reverse the roles! Fit the model using data_test and test it on data_train. (Calculate the testing MSPE.) Part a gave you a new measure of model quality. Instead of picking either this measure or the one you calculated in the previous exercise (when the roles were reversed), average them! This average is an estimate of the 2-fold cross validation error. The general k-fold cross validation algorithm is described below. \\(k\\)-Fold Cross Validation (CV) Divide the data into \\(k\\) groups / folds of equal size. Repeat the following procedures for each fold \\(j \\in \\{1,2,...,k\\}\\): Divide the data into a test set (fold \\(j\\)) &amp; training set (the other \\(k-1\\) folds). Fit a model using the training set. Use this model to predict the responses for the \\(n_j\\) cases in fold \\(j\\): \\(\\hat{y}_1, ..., \\hat{y}_{n_j}\\) Calculate the MSPE for fold \\(j\\): \\[\\text{MSPE}_j = \\frac{1}{n_j}\\sum_{i=1}^{n_j} (y_i - \\hat{y}_i)^2\\] Calculate the “cross validation error”, ie. the average MSPE from the \\(k\\) folds: \\[\\text{CV}_{(k)} = \\frac{1}{k} \\sum_{j=1}^k \\text{MSPE}_j\\] In pictures: 10-fold CV 40-fold cross validation, aka “Leave-one-out CV (LOOCV)” Using the tools you learned in your earlier programming module, you could write a for-loop to perform cross validation. However, somebody already did that work! The cv.glm() function in the boot package calculates cross validation error for us. First, use it to calculate the 40-fold cross validation error. # First refit the model using ALL data &amp; glm() model_1_glm &lt;- glm(fatSiri ~ weight, body_data, family = &quot;gaussian&quot;) # Then calculate the error model_1_cv40 &lt;- cv.glm(body_data, model_1_glm, K = 40) # The error is the first reported value model_1_cv40$delta ## [1] 35.35 35.31 Explain why, for this data, a 40-fold cross validation procedure would also be called “leave-one-out”. Hint: There are 40 cases in our dataset. In practice, \\(k=10\\) and \\(k=7\\) are common choices for cross validation. This has been shown to hit the ‘sweet spot’ between the extremes of \\(k = n\\) (LOOCV) and \\(k=2\\). Why? What advantages do you think 10-fold CV has over 2-fold CV? What advantages do you think 10-fold CV has over LOOCV? Using CV to compare models Calculate 10-fold cross validation errors for both of our original models: model_1: fatSiri ~ weight model_12: fatSiri ~ poly(weight,12) NOTE: since the 10 folds are randomly chosen, set the random number seed so that you get the same folds each time. model_1_glm &lt;- glm(fatSiri ~ weight, body_data, family = &quot;gaussian&quot;) model_12_glm &lt;- glm(fatSiri ~ poly(weight,12), body_data, family = &quot;gaussian&quot;) # Set the seed set.seed(2018) # 10-fold CV error for model_1_glm model_1_cv10 &lt;- ___ model_1_cv10$delta # 10-fold CV error for model_12_glm model_12_cv10 &lt;- ___ model_12_cv10$delta Recall that model_1 had an in-sample MSPE of 32.48 and model_12 had an in-sample MSPE of 25.53. Within both models, how do the in-sample errors compare to the CV errors? Which model has the best in-sample errors? Which model has the best CV error? Which model would you choose? Parsimonious Models Reflecting back, these exercises illustrate the importance of parsimony in a statistical analysis. A parsimonious analysis is one that balances simplicity with the desire for the highest \\(R^2\\) (for example). In the case of model building, increasing the number of predictors increases \\(R^2\\). BUT: The greater the number of predictors, the more complicated the model is to implement and interpret; The greater the number of predictors, the greater the risk of overfitting the model to our particular sample of data. That is, the greater the risk of our model losing the general trend, hence, the model’s generalizability to the greater population. "],
["day-3a-more-data-wrangling-changing-cases.html", "4.4 Day 3a: More Data Wrangling - Changing Cases", " 4.4 Day 3a: More Data Wrangling - Changing Cases Getting started: As you settle in, install these packages in your console: install.packages(&quot;infer&quot;, dependencies = TRUE) install.packages(&quot;broom&quot;, dependencies = TRUE) install.packages(&quot;gsheet&quot;, dependencies = TRUE) Then start a new Rmd and load the following packages at the top. library(ggplot2) library(tidyverse) library(lubridate) library(babynames) BabyNames&lt;-babynames colnames(BabyNames)[4]&lt;-&quot;count&quot; library(infer) library(broom) library(gsheet) Today’s plan: Discuss Day 2 homework &amp; tie up any other loose ends More data wrangling: changing cases Sampling variability / sampling distributions Confidence intervals 4.4.1 Spread, Gather, and Wide and Narrow Data Formats Additional reading: Wickham and Grolemund on spreading and gathering, or Chapter 11 of Data Computing by Kaplan As we are transforming data, it is important to keep in mind what constitutes each case (row) of the data. For example, in the initial BabyName data below, each case is a single name-sex-year combination. So if we have the same name and sex but a different year, that would be a different case. Table 4.5: Each case is one name-sex-year combination. year sex name count prop 1880 F Mary 7065 0.0724 1880 F Anna 2604 0.0267 1880 F Emma 2003 0.0205 1880 F Elizabeth 1939 0.0199 1880 F Minnie 1746 0.0179 1880 F Margaret 1578 0.0162 It is often necessary to rearrange your data in order to create visualizations, run statistical analysis, etc. We have already seen some ways to rearrange the data to change the case. For example, what is the case after performing the following command? BabyNamesTotal&lt;-BabyNames %&gt;% group_by(name,sex) %&gt;% summarise(total=sum(count)) Each case now represents one name-sex combination: Table 4.6: Narrow format where each case is one name-sex combination. name sex total Aaban M 87 Aabha F 28 Aabid M 5 Aabriella F 15 Aada F 5 Aadam M 218 In this activity, we are going to learn two new operations to reshape and reorganize the data: spread() and gather(). 4.4.1.1 Spread Example 4.5 We want to find the common names that are the most gender neutral (used roughly equally for males and females). How should we rearrange the data? Well, one nice way would be to have a single row for each name, and then have separate variables for the number of times that name is used for males and females. Using these two columns, we can then compute a third column that gives the ratio between these two columns. That is, we’d like to transform the data into a wide format with each of the possible values of the sex variable becoming its own column. The operation we need to perform this transformation is spread(). It takes a value (total in this case) representing the variable to be divided into multiple new variables, and a key (the original variable sex in this case) that identifies the variable in the initial narrow format data whose values should become the names of the new variables in the wide format data. The entry fill=0 specifies that if there are, e.g., no females named Aadam, we should include a zero in the corresponding entry of the wide format table. BabyWide&lt;-BabyNamesTotal %&gt;% spread(key=sex,value=total,fill=0) Table 4.7: A wide format with one case per name enables us to examine gender balance. name F M Aaban 0 87 Aabha 28 0 Aabid 0 5 Aabriella 15 0 Aada 5 0 Aadam 0 218 Now we can choose common names with frequency greater than 25,000 for both males and females, and sort by the ratio to identify gender-neutral names. Neutral&lt;-BabyWide %&gt;% filter(M&gt;25000,F&gt;25000) %&gt;% mutate(ratio = pmin(M/F,F/M)) %&gt;% arrange(desc(ratio)) Table 4.8: The most gender-neutral common names, in wide format. name F M ratio Kerry 48476 49482 0.9797 Riley 87347 89585 0.9750 Jackie 90427 78245 0.8653 Frankie 32626 40105 0.8135 Jaime 49552 66314 0.7472 Peyton 62481 45428 0.7271 Casey 75402 109122 0.6910 Pat 40124 26732 0.6662 Jessie 166088 109560 0.6597 Kendall 54919 33334 0.6070 Jody 55655 31109 0.5590 Avery 100768 49216 0.4884 4.4.1.2 Gather Next, let’s filter these names to keep only those with a ratio of 0.5 or greater (no more than 2 to 1), and then switch back to narrow format. We can do this with the following gather() operation. It gathers the columns listed (F,M) at the end into a single column whose name is given by the key (sex), and includes the values in a column called total. NeutralNarrow&lt;-Neutral %&gt;% filter(ratio&gt;=.5) %&gt;% gather(key=sex,value=total,F,M)%&gt;% select(name,sex,total)%&gt;% arrange(name) Table 4.9: Narrow format for the most gender-neutral common names. name sex total Casey F 75402 Casey M 109122 Frankie F 32626 Frankie M 40105 Jackie F 90427 Jackie M 78245 4.4.2 Summary Graphic Here is a nice summary graphic of gather and spread from the RStudio cheat sheet on data import: 4.4.3 The Daily Show Guests The data associated with this article is available in the fivethirtyeight package, and is loaded into Daily below. It includes a list of every guest to ever appear on Jon Stewart’s The Daily Show. Note that when multiple people appeared together, each person receives their own line. Daily&lt;-daily_show_guests year google_knowledge_occupation show group raw_guest_list 1999 singer 1999-07-26 Musician Donny Osmond 1999 actress 1999-07-27 Acting Wendie Malick 1999 vocalist 1999-07-28 Musician Vince Neil 1999 film actress 1999-07-29 Acting Janeane Garofalo 1999 comedian 1999-08-10 Comedy Dom Irrera 1999 actor 1999-08-11 Acting Pierce Brosnan 1999 director 1999-08-12 Media Eduardo Sanchez and Daniel Myrick 1999 film director 1999-08-12 Media Eduardo Sanchez and Daniel Myrick 1999 american television personality 1999-08-16 Media Carson Daly 1999 actress 1999-08-17 Acting Molly Ringwald 1999 actress 1999-08-18 Acting Sarah Jessica Parker 4.4.3.1 Popular guests Exercise 4.8 Create the following table containing 19 columns. The first column should have the ten guests with the highest number of total apperances on the show, listed in descending order of number of appearances. The next 17 columns should show the number of appearances of the corresponding guest in each year from 1999 to 2015 (one per column). The final column should show the total number of appearances for the corresponding guest over the entire duration of the show (these entries should be in decreasing order). Hint: the function rowSums() adds up all of the entries in each row of a table. Try using it in a mutate. 4.4.3.2 Recreating a graphic The original data has 18 different entries for the group variable: unique(Daily$group) ## [1] &quot;Acting&quot; &quot;Comedy&quot; &quot;Musician&quot; &quot;Media&quot; ## [5] NA &quot;Politician&quot; &quot;Athletics&quot; &quot;Business&quot; ## [9] &quot;Advocacy&quot; &quot;Political Aide&quot; &quot;Misc&quot; &quot;Academic&quot; ## [13] &quot;Government&quot; &quot;media&quot; &quot;Clergy&quot; &quot;Science&quot; ## [17] &quot;Consultant&quot; &quot;Military&quot; In order to help you recreate the first figure from the article, I have added a new variable with three broader groups: (i) entertainment; (ii) politics, business, and government, and (iii) commentators. We will learn in the next activity what the inner_join in this code chunk is doing. DailyGroups&lt;-read_csv(&quot;https://www.macalester.edu/~dshuman1/data/112/daily-group-assignment.csv&quot;) Daily&lt;-Daily%&gt;% inner_join(DailyGroups,by=c(&quot;group&quot;=&quot;group&quot;)) year google_knowledge_occupation show group raw_guest_list broad_group 1999 actor 1999-01-11 Acting Michael J. Fox Entertainment 1999 comedian 1999-01-12 Comedy Sandra Bernhard Entertainment 1999 television actress 1999-01-13 Acting Tracey Ullman Entertainment 1999 film actress 1999-01-14 Acting Gillian Anderson Entertainment 1999 actor 1999-01-18 Acting David Alan Grier Entertainment 1999 actor 1999-01-19 Acting William Baldwin Entertainment Exercise 4.9 Using the group assignments contained in the broad_group variable, recreate the graphic from the article, with three different lines showing the fraction of guests in each group over time. Hint: first think about what your case should be for the glyph-ready form. 4.4.4 Gathering Practice A typical situation that requires a gather command is when the columns represent the possible values of a variable. Table 4.10 shows example data set from opendataforafrica.org with different years in different columns. Lesotho&lt;-read_csv(&quot;https://www.macalester.edu/~dshuman1/data/112/Lesotho.csv&quot;) Table 4.10: Financial statistics about Lesotho. Category 2010 2011 2012 2013 2014 Total Population 2.01 2.03 2.05 2.07 2.10 Gross Domestic Product 2242.30 2560.99 2494.60 2267.96 1929.28 Average Interest Rate on Loans 11.22 10.43 10.12 9.92 10.34 Inflation Rate 3.60 4.98 6.10 5.03 4.94 Average Interest Rate on Deposits 3.68 2.69 2.85 2.85 2.73 Exercise 4.10 (Gathering practice) Make a side-by-side bar chart with the year on the horizontal axis, and three side-by-side vertical columns for average interest rate on deposits, average interest rate on loans, and inflation rate for each year. In order to get the data into glyph-ready form, you’ll need to use gather. Hint: gather uses the dplyr::select() notation, so you can, e.g., list the columns you want to select, use colon notation, or use contains(a string). See Wickham and Grolemund for more information. "],
["day-3b-sampling-distributions-confidence-intervals.html", "4.5 Day 3b: Sampling Distributions &amp; Confidence Intervals", " 4.5 Day 3b: Sampling Distributions &amp; Confidence Intervals MOTIVATION Thus far, our analyses have been exploratory in nature. We’ve used sample data to visualize relationships, build models of relationship trends, and evaluate those models: Motivating Example Scientists are developing a new pregnancy test. Doctors gave both the new test and the “gold standard” to a group of 100 pregnant women. The new test was accurate for 88 of these women whereas the standard test was only accurate for 86. Exploratory question What trends did we observe in our sample of data? Was the new test more accurate than the old in detecting pregnancy among pregnant women? Inferential question Do these data provide enough evidence to conclude that the new test is better at detecting pregnancy among pregnant women? The point: we’ll start looking beyond just the trends of our sample data and exploring how to use sample data to make inferences about the larger population of interest. The first step in reaching this goal is understanding sampling distributions, a reflection of the potential error in our sample information. 4.5.1 Simulation study: sampling variability Recall the 2016 election returns for the population of “all” counties outside Alaska: politics &lt;- read.csv(&quot;https://www.macalester.edu/~ajohns24/data/electionDemographics16.csv&quot;) And features of each county, such as median rent: Based on these complete population data on all counties outside Alaska, we know that the relationship trend between Trump’s 2016 support and the median rent in a county is: \\[\\text{perrep_2016} = 84.583 - 0.041 * \\text{median_rent}\\] 4.5.1.1 Different Samples, Different Estimates FORGET THAT YOU KNOW ALL OF THE ABOVE. Let’s pretend that we are working within the typical scenario - we do not have access to the entire population of interest. Instead, we need to estimate the true trend (regression line) using data from a randomly selected sample of counties. That is, we’ll use the sample model to estimate the true, but unknown population model. Exercise 4.11 (Sampling and randomness in RStudio) We’ll be taking some random samples of counties throughout this activity. The underlying random number generator plays a role in the random sample we happen to get: # Try the following chunk A FEW TIMES sample_n(politics, size = 2, replace = FALSE) # Try the following FULL chunk A FEW TIMES set.seed(155) sample_n(politics, size = 2, replace = FALSE) NOTE: If we set.seed(some positive integer) before taking a random sample, we’ll get the same results. This reproducibility is important: we get the same results every time we knit the Rmd we can share our work with others and ensure they get our same answers it would not be great if you submitted your work to, say, a journal, and weren’t able to back up / confirm / reproduce your results! Exercise 4.12 (Class experiment) Let’s each take a sample and see what we get. a. Let your seed be your birthday month (1 or 2 digits) and day (2 digits). For example January 5 is 105, September 10 is 910, October 5 is 1005, and December 10 is 1210. Set this in RStudio: set.seed(YOURSEED) b. Take a random sample of 10 counties using the syntax above, and save it as sample1. c. Construct and plot your sample model. How close is this estimate to the actual population model (perrep_2016 = 84.58 - 0.04 * median_rent)? d. Take another sample of 10 counties, save it as sample2, and repeat part (c). How does this compare to the population model? The sample model calculated from sample_1? e. Indicate your sample_1 and sample_2 intercept and slope estimates in this survey. Exercise 4.13 (Comparing estimates) Import each student’s estimates from Google sheets: results &lt;- gsheet2tbl(&#39;https://docs.google.com/spreadsheets/d/1HN8ccQEFLflaBSwygRD-f7UEun7Xi8qx_rMsiguMMsI/edit?usp=sharing&#39;) Compare the intercepts: ggplot(results, aes(x = intercept)) + geom_histogram(color = &quot;white&quot;) Compare the slopes: ggplot(results, aes(x = slope)) + geom_histogram(color = &quot;white&quot;) Compare the resulting models to the true population model in red: ggplot(politics, aes(x = median_rent, y = perrep_2016)) + geom_abline(data = results, aes(intercept = intercept, slope = slope), alpha = 0.75) + geom_smooth(method = &quot;lm&quot;, color = &quot;red&quot;, se = FALSE) 4.5.1.2 Simulation Study Our little experiment reflects very few of the more than \\(_{3112}C_{10} &gt; 2.3*10^{28}\\) different samples of 10 counties that we could get from the entire population of 3112 counties!! In this section, you’ll run a simulation to study just how different these estimates could be. Exercise 4.14 (Taking multiple samples) Whereas sample_n() takes a single sample of size \\(n\\) from a dataset, rep_sample_n takes multiple samples of size \\(n\\). To get a feel for it, take 4 samples of size 2. The replicate variable in the output indicates the sample (1, 2, 3, 4) to which each sampled case corresponds. example1 &lt;- rep_sample_n(politics, size = 2, reps = 4, replace = FALSE) dim(example1) example1 Exercise 4.15 (500 samples of size 10) a. To get a sense for the wide variety of samples we might get, take 500 samples of size \\(n\\) = 10. Store these as samples_10. set.seed(155) samples_10 &lt;- rep_sample_n(politics, size = 10, reps = 500, replace = FALSE) b. Each sample produces a different estimate of the population model between perrep_2016 and median_rent. Plot these 500 sample model estimates on the same frame: ggplot(samples_10, aes(x = median_rent, y = perrep_2016, group = replicate)) + geom_smooth(method = &quot;lm&quot;, se = FALSE, size = 0.5) Exercise 4.16 (500 sample slopes) Let’s focus on the slopes of these 500 sample models. a. Save the 500 median_rent (slope) coefficients, stored under the estimate variable in the slopes_10 data frame. slopes_10 &lt;- samples_10 %&gt;% group_by(replicate) %&gt;% do(lm(perrep_2016 ~ median_rent, data=.) %&gt;% tidy()) %&gt;% filter(term == &quot;median_rent&quot;) # Check it out head(slopes_10) dim(slopes_10) b. Construct a histogram of the 500 sample estimates of the true slope. This histogram approximates a sampling distribution of the sample slopes. ggplot(slopes_10, aes(x = estimate)) + geom_histogram(color = &quot;white&quot;, binwidth = 0.01) + lims(x = c(-0.20, 0.15)) c. Describe the sampling distribution: What’s its general shape? Where is it centered? Roughly what is its spread? i.e., what is the range of estimates you observed? Exercise 4.17 (Increasing sample size) Suppose we increased our sample size from n=10 to n=50. What impact do you anticipate this having on the sampling distribution of sample slopes: Around what value would you expect the distribution of sample slopes to be centered? What general shape would you expect the distribution to have? In comparison to estimates based on the samples of size 10, do you think the estimates based on samples of size 50 will be closer to or farther from the true slope (on average)? Why? Exercise 4.18 (500 samples of size 50) Test your intuition. Fill in the blanks to repeat the simulation process with samples of size n = 50. # Take 500 samples of size n = 50 set.seed(155) samples_50 &lt;- rep_sample_n(politics, size = ___, reps = ___, replace = FALSE) # Plot the 500 sample model estimates ggplot(___, aes(x = ___, y = ___, group = ___)) + geom_smooth(method = &quot;lm&quot;, se = FALSE, size = 0.5) # Store the 500 slope estimates slopes_50 &lt;- ___ %&gt;% group_by(___) %&gt;% do(lm(___ ~ ___, data = .) %&gt;% tidy()) %&gt;% filter(term == &quot;median_rent&quot;) # Construct a histogram of the 500 sample slope estimates. ggplot(___, aes(x = estimate)) + geom_histogram(color = &quot;white&quot;, binwidth = 0.01) + lims(x = c(-0.20, 0.15)) Exercise 4.19 (500 samples of size 200) Finally, repeat the simulation process with samples of size \\(n\\) = 200. # Take 500 samples of size n = 200 set.seed(155) samples_200 &lt;- rep_sample_n(politics, size = ___, reps = ___, replace = FALSE) # Plot the 500 sample model estimates ggplot(___, aes(x = ___, y = ___, group = ___)) + geom_smooth(method = &quot;lm&quot;, se = FALSE, size = 0.5) # Store the 500 slope estimates slopes_200 &lt;- ___ %&gt;% group_by(___) %&gt;% do(lm(___ ~ ___, data=.) %&gt;% tidy()) %&gt;% filter(term == &quot;median_rent&quot;) # Construct a histogram of the 500 sample slope estimates. ggplot(___, aes(x = estimate)) + geom_histogram(color = &quot;white&quot;, binwidth = 0.01) + lims(x = c(-0.20, 0.15)) Exercise 4.20 (Impact of sample size) a. Compare the sampling distributions of the sample slopes for the estimates based on sizes 10, 50, and 200 by plotting them on the same frame: # Combine the estimates &amp; sample size into a new data set simulation_data &lt;- data.frame( estimates = c(slopes_10$estimate, slopes_50$estimate, slopes_200$estimate), sample_size = rep(c(&quot;10&quot;,&quot;50&quot;,&quot;200&quot;), each = 500)) #Construct density plot ggplot(simulation_data, aes(x = estimates, color = sample_size)) + geom_density() + labs(title = &quot;SAMPLING Distributions&quot;) b. Calculate the mean and standard deviation in sample slopes calculated from samples of size 10, 50, and 200. NOTE: We call the standard deviation “standard error” here – an estimate’s deviation from the mean reflects its error. simulation_data %&gt;% group_by(sample_size) %&gt;% summarize(mean(estimates), sd(estimates)) c. Interpret the three standard errors. Exercise 4.21 (Properties of sampling distributions) In light of your these investigations, complete the following statements. a. For all sample sizes, the shape of the sampling distribution is ???. b. As sample size increases: The average sample slope estimate INCREASES / DECREASES / IS FAIRLY STABLE. The standard deviation of the sample slopes INCREASES / DECREASES / IS FAIRLY STABLE. c. Thus, as sample size increases, our sample slopes become MORE RELIABLE / LESS RELIABLE. 4.5.1.3 Reflection Consider a simple population model \\[y = \\beta_0 + \\beta_1 x\\] In general, we don’t know \\(\\beta_0\\) or \\(\\beta_1\\). We are either working with (\\(x,y\\)) data on a sample of subjects from the broader population of interest or with a population that is in flux. Thus, our sample data give us an estimate of the population model: \\[y = \\hat{\\beta}_0 + \\hat{\\beta}_1 x\\] What we know about the sample model: our estimates \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\) will vary depending upon what data we happen to get there is error in these estimates These concepts are captured in the sampling distribution of a sample estimate \\(\\hat{\\beta}\\) (eg: \\(\\hat{\\beta}_0\\) or \\(\\hat{\\beta_1}\\)). Specifically, the sampling distribution of \\(\\hat{\\beta}\\) is a distribution of all possible \\(\\hat{\\beta}\\) we could observe based on all possible samples of the same size \\(n\\) from the population. It captures how \\(\\hat{\\beta}\\) can vary from sample to sample. Impact of sample size Below are sampling distributions of the sample slopes and sample models calculated from sample sizes of 10, 50, and 200 counties. Notice that as sample size n increases: there is less variability (and more consistency) in the possible estimates from different samples we are less likely to get estimates that are far from the truth Standard error of \\(\\hat{\\beta}\\) The standard deviation of the sampling distribution, which is called the standard error, measures the typical error in the sample slopes calculated from sample to sample. The greater the standard error, the less “reliable” the sample estimate. As we’ve seen, standard error decreases as sample size \\(n\\) increases. For example, in our simulation, our estimates based on samples of size 10, 50, &amp; 200 had the following standard deviations: sd(slopes_10$estimate) ## [1] 0.03433 sd(slopes_50$estimate) ## [1] 0.01141 sd(slopes_200$estimate) ## [1] 0.00528 These observed standard deviations are merely approximations of the “true” standard error. In linear algebraic notation: \\[\\begin{array}{ll} \\text{population model}: &amp; y = X \\beta \\\\ \\text{sample estimate of $\\beta$}: &amp; \\hat{\\beta} = \\left(X^TX\\right)^{-1}X^Ty \\\\ \\text{standard error of $\\hat{\\beta}$}: &amp; s.e.\\left(\\hat{\\beta}\\right) = \\sqrt{\\sigma^2\\left(X^TX\\right)^{-1}} \\\\ \\end{array}\\] where \\(\\sigma^2\\) is the variance of the residuals. When the linear regression model assumptions are met, the standard error \\(\\hat{\\beta}\\) is proportional to \\(\\frac{1}{\\sqrt{n}}\\) where \\(n\\) is the number of observations in the sample. One hitch The simulation above was just pretend. In practice we do NOT take 500 samples from the population - we take and use only ONE sample. Thus, we can’t actually observe the sampling distribution. However, the properties of the sampling distributions that we observed are guaranteed by Probability theory. IF the regression model assumptions are met and our sample size \\(n\\) is “big enough”, then the Central Limit Theorem (CLT) guarantees that the sampling distribution of all possible \\(\\hat{\\beta}_i\\) we could observe from all possible samples of size \\(n\\) is approximately Normally distributed around \\(\\beta_i\\) with standard error \\(s.e.(\\hat{\\beta}_i)\\): \\[\\hat{\\beta}_i \\stackrel{\\cdot}{\\sim} N(\\beta_i, s.e.(\\hat{\\beta}_i))\\] The CLT for the sampling distribution of \\(\\hat{\\beta}_i\\) in pictures: Connecting this to the 68-95-99.7 Rule, the CLT guarantees that (approximately)… 68% of samples will produce \\(\\hat{\\beta}_i\\) within 1 st. err. of \\(\\beta_i\\) 95% of samples will produce \\(\\hat{\\beta}_i\\) within 2 st. err. of \\(\\beta_i\\) 99.7% of samples will produce \\(\\hat{\\beta}_i\\) within 3 st. err. of \\(\\beta_i\\) quantity 4.5.2 Reporting estimates with measures of error Again, PRETEND we don’t have access to the population data and instead have to observe data on ONE sample of 50 counties. Be sure to use the exact code below! # Sample 50 counties set.seed(18) sample_50 &lt;- sample_n(politics, size = 50, replace = FALSE) Calculate a sample model estimate using sample_50: sample_mod &lt;- lm(perrep_2016 ~ median_rent, sample_50) coef(summary(sample_mod)) ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 86.78205 3.563150 24.355 1.141e-28 ## median_rent -0.04489 0.006022 -7.455 1.483e-09 When interpreting and reporting our estimated sample model, it’s crucial to incorporate a measure of the potential error in these estimates. This is commonly done using standard errors &amp; confidence intervals. In fact, we see confidence intervals reported in everyday news. Example: On June 18, 2017, Gallup reported that Trump’s approval rating is at 45% with a “margin of sampling error of \\(\\pm\\) 3 percentage points at the 95% confidence level.” Use this info to calculate a confidence interval for Trump’s approval rating among all Americans. Does this evidence support the claim that less than half of Americans approve of the job Trump’s doing? Exercise 4.22 (Estimating the standard error) Our sample estimate of slope \\(\\beta_1\\) is \\(\\hat{\\beta}_1 =-0.04489\\). With only 1 sample of data (not 500), we can estimate \\(\\text{s.e.}(\\hat{\\beta}_1)\\) by \\(\\sqrt{\\hat{\\sigma}^2(X^TX)^{-1}}\\). This estimate is reported in the Std. Error column of the summary table. Report this error rounded to 5 decimal places. How close is this error estimate to the standard deviation we observed in the slopes_50$estimate, 0.006022? Interpret the standard error. Does this seem big or small relative to the slope? (Looking ahead: confidence intervals provide some perspective here.) Exercise 4.23 (Constructing confidence intervals) In light of the standard error 0.006022 that you reported in part (a) above, the CLT for \\(\\hat{\\beta}_1\\) guarantees that \\[\\hat{\\beta}_1 \\stackrel{\\cdot}{\\sim} N\\left(\\beta_1, 0.006022^2\\right)\\] where \\(\\beta_1\\) is the “unknown” population slope. Thus by the 68-95-99.7 Rule, approximately 95% of samples will produce an estimate \\(\\hat{\\beta}_1\\) that’s within 2 standard errors of \\(\\beta_1\\). In turn, \\(\\beta_1\\) is within 2 standard errors of 95% of all possible \\(\\hat{\\beta}_1\\). Calculate the range of values that are within 2 standard errors of OUR sample slope (\\(\\hat{\\beta}_1\\) = -0.04489): \\[\\begin{split} \\hat{\\beta_1} \\pm 2 s.e.(\\hat{\\beta}_1) &amp; = \\hspace{2in}\\\\ \\end{split}\\] This is the 95% confidence interval (CI) for \\(\\beta_1\\)! That is: though we don’t know \\(\\beta_1\\) (pretend), we are “95% confident” that it falls within the range of this interval. We can get a more accurate confidence interval in RStudio: confint(sample_mod) Use the confidence interval to evaluate the claim that \\(\\beta_1 &lt; 0\\), ie. that there’s a negative association between the median rent and the Republican votes in 2016. 4.5.3 Confidence interval simulation study Exercise 4.24 As we saw in our simulation study of 500 samples, some samples are lucky and some aren’t. Is sample_50 one of the lucky samples? That is, did this sample produce a confidence interval that contains the true slope \\(\\beta_1 = -0.0409\\)? Exercise 4.25 The use of “95% confidence” (instead of 100% confidence) indicates that such unlucky samples are possible. But what exactly does “95% confidence” mean? To answer this question, let’s repeat our experiment 100 times. Try this on your own first. Hints are provided below as is a complete solution. # Set the seed to 2018 # Take 100 samples of size 50 from politics samples_50 &lt;- # From each sample: calculate a confidence interval for the slope # (Only keep the slope CIs, not the intercept CIs) CIs_50 &lt;- # Give meaningful variable names names(CIs_50) &lt;- c(&quot;replicate&quot;, &quot;variable&quot;, &quot;lower&quot;, &quot;upper&quot;) # Define new variables: # lucky = whether the CI covers -0.0409 # estimate = sample slope estimate CIs_50 &lt;- # Check it out head(CIs_50) HINTS # Set the seed to 2018 # Take 100 samples of size 50 from politics samples_50 &lt;- ___ # From each sample: calculate a confidence interval for the slope # (Only keep the slope CIs, not the intercept CIs) CIs_50 &lt;- ___ %&gt;% group_by(___) %&gt;% do(___ %&gt;% tidy()) %&gt;% filter(___) # Give meaningful variable names names(CIs_50) &lt;- c(&quot;replicate&quot;, &quot;variable&quot;, &quot;lower&quot;, &quot;upper&quot;) # Define new variables: # lucky = whether the CI covers -0.0409 # estimate = sample slope estimate CIs_50 &lt;- CIs_50 %&gt;% ___(lucky = ___, estimate = ___) # Check it out head(CIs_50) SOLUTIONS # Set the seed to 2018 set.seed(2018) # Take 100 samples of size 50 samples_50 &lt;- rep_sample_n(politics, size = 50, reps = 100, replace = FALSE) # From each sample: calculate a confidence interval for the slope # (Only keep the slope CIs, not the intercept CIs) CIs_50 &lt;- samples_50 %&gt;% group_by(replicate) %&gt;% do(confint(lm(perrep_2016 ~ median_rent, data=.), level = 0.95) %&gt;% tidy()) %&gt;% filter(.rownames == &quot;median_rent&quot;) # Give meaningful variable names names(CIs_50) &lt;- c(&quot;replicate&quot;, &quot;variable&quot;, &quot;lower&quot;, &quot;upper&quot;) # Define new variables: # lucky = whether the CI covers -0.0409 # estimate = sample slope estimate CIs_50 &lt;- CIs_50 %&gt;% mutate(lucky = (lower &lt; -0.0409 &amp; upper &gt; -0.0409), estimate = (upper + lower) / 2) # Check it out head(CIs_50) Exercise 4.26 (Visualizing the simulated CIs) The code above produced 100 different 95% confidence intervals from 100 different samples of size 50. Check out a plot of these 100 intervals. In this plot, the 100 horizontal lines represent the 100 95% CIs. Each interval is centered at the corresponding sample slope \\(\\hat{\\beta}_1\\) (represented by a dot). Intervals that do NOT cover the true \\(\\beta = -0.0409\\) are highlighted in red. Based on this plot, explain the meaning of “95% confidence”. ggplot(CIs_50, aes(y = replicate, x = lower, color = lucky)) + geom_segment(aes(x = lower, xend = upper, y = replicate, yend = replicate)) + geom_point(aes(x = estimate, y = replicate)) + lims(x = c(-0.20, 0.15)) + geom_vline(xintercept = -0.0409) Interpreting CIs &amp; Confidence Levels The easy (but vague) way: We are ‘95% confident’ that \\(\\beta_1\\) is between the lower &amp; upper bounds of this CI. The correct way: Using this CI method, approximately 95% of all possible samples will produce 95% CI’s that cover \\(\\beta_1\\). The other 5% are based on unlucky samples that produce unusually low or high estimates \\(\\hat{\\beta}_1\\). Mathematically: \\[P(\\hat{\\beta}_1 \\in (\\beta_1 \\pm 2 s.e.(\\hat{\\beta}_1))) \\approx 0.95\\] The incorrect way: We cannot say that “there’s a 95% chance that \\(\\beta_1\\) is in the 95% CI.” Technically, \\(\\beta_1\\) is either in the interval or it’s not, so the probability is simply 1 or 0. Mathematically: \\[P(\\beta_1 \\in (\\hat{\\beta}_1 \\pm 2 s.e.(\\hat{\\beta}_1))) \\in \\{0,1\\} \\] NOTE: This is a consequence of using frequentist methods. There’s a competing Bayesian philosophy which is outside the scope of this workshop. Exercise 4.27 (Impact of sample size) Suppose we increase our sample size from 50 to 200 counties. What does your intuition say will happen to the 95% CIs? Will the new CIs be narrower or wider? Approximately what percentage of these will cover \\(\\beta_1 = -0.0409\\)? Check your intuition. Rerun the simulation with a sample size of 200. Exercise 4.28 (Impact of confidence level) Traditionally, people typically use a 95% confidence level. Consider lowering the confidence level from 95% to 68%. What does your intuition say will happen to the CIs? Will the new CIs be narrower or wider?Approximately what percentage of these will cover \\(\\beta_1 = -0.0409\\)? Similarly, what would happen if we increased the confidence level from 95% to 99.97% CIs? Test your intuition. Repeat the simulation, changing the level argument in the do() statement. 4.5.4 Extra Above, you ran a simulation under a few different sample sizes. Any time you want to perform a similar operation multiple times, you can streamline the process by writing a function. Try the following. Write a function named add_ab() that calculates the sum of two given numbers, a and b. add_ab &lt;- function(a,b){ #define add_ab as a function of two arguments, a &amp; b a + b #what you want the function to do } #a right bracket to end the function definition # Try it out! add_ab(a=2, b=3) add_ab(a=1, b=-2) Write a function that calculates more than one quantity, say the sum and difference of a and b. new_ab &lt;- function(a,b){ sum_ab &lt;- a + b diff_ab &lt;- a - b return(data.frame(sum_ab, diff_ab)) } #try it out! tryit &lt;- new_ab(a=2, b=3) tryit tryit$sum_ab tryit$diff_ab Write a function that runs the CI simulation for a sample of size n &amp; confidence level cl. "],
["day-4a-hypothesis-testing.html", "4.6 Day 4a: Hypothesis Testing", " 4.6 Day 4a: Hypothesis Testing Getting started: As you settle in, start a new Rmd and load the following packages at the top. ```{r warning = FALSE, message = FALSE} library(ggplot2) library(dplyr) library(infer) library(broom) library(mosaic) ``` Today’s plan: Discuss Day 3 homework &amp; tie up any other loose ends Hypothesis testing More advanced visualization techniques 4.6.1 Warm-up Conditional Probabilities What’s the relationship between each of the following pairs of unconditional (\\(P(A)\\)) &amp; conditional (\\(P(A|B)\\)) probabilities? \\[\\begin{array}{lcl} P(\\text{lung cancer}) &amp; \\hspace{.4in} &amp; P(\\text{lung cancer} \\; | \\; \\text{smoke}) \\\\ P(\\text{eat at McD&#39;s}) &amp; &amp; P(\\text{eat at McD&#39;s} \\; | \\; \\text{vegan}) \\\\ P(\\text{Queen of Hearts} | \\text{Hearts}) &amp; &amp; P(\\text{Hearts} \\; | \\; \\text{Queen of Hearts}) \\\\ \\end{array}\\] Exploratory analysis vs inference Recall the difference between exploratory and inferential questions: Exploratory question What trends did we observe in our sample of data? Inferential question Given the potential error in this sample information, what can we conclude about the trends in the broader population? To this end, we can calculate standard errors, construct confidence intervals, and conduct hypothesis tests. Regression Models (Thus Far) \\[\\begin{array}{ll} \\text{population model:} &amp; y = X \\beta + \\varepsilon \\;\\; \\text{ where } \\varepsilon \\sim N(0,\\sigma^2) \\\\ &amp; y \\text{ is an $n \\times 1$ vector of responses} \\\\ &amp; X \\text{ is an $n \\times (k+1)$ matrix of predictors} \\\\ &amp; \\beta \\text{ is a $k \\times 1$ vector of coefficients} \\\\ &amp; \\varepsilon \\text{ is an $n \\times 1$ vector of residuals} \\\\ &amp; \\\\ \\text{sample estimate of $\\beta$}: &amp; \\hat{\\beta} = (X^TX)^{-1}X^Ty \\\\ \\text{standard error of $\\hat{\\beta}$:} &amp; s.e.(\\hat{\\beta}) = \\sqrt{\\sigma^2(X^TX)^{-1}} \\\\ &amp; \\\\ \\text{prediction of $y$ at $x$:} &amp; \\hat{y} = x^T \\hat{\\beta} \\\\ \\text{s.e. for a prediction of trend at $x$:} &amp; s.e.(\\hat{y}) = \\sqrt{\\sigma^2 x^T (X^TX)^{-1} x} \\\\ \\text{s.e. for a prediction of individual case at $x$:} &amp; s.e.(\\hat{y}) = \\sqrt{\\sigma^2(1 + x^T (X^TX)^{-1} x)} \\\\ &amp; \\\\ \\text{Approximately:} &amp; \\hat{\\beta}_i \\stackrel{\\cdot}{\\sim} N(\\beta_i, (s.e.(\\hat{\\beta}_i)^2) \\\\ &amp; \\hat{y} \\stackrel{\\cdot}{\\sim} N(y, (s.e.(\\hat{y}))^2) \\\\ \\end{array}\\] EXAMPLE Let’s do some inference. Extraterrestrials have landed and scientists are busy studying their physical characteristics (put aside your ethics for now). Ignore everything you know about humans - ETs are different. Examine the following plots of the relationship between the ETs “brain” weight, “hand” length, and height that were calculated from a sample of ETs. Is there a “significant” relationship between brain weight &amp; height? What about between brain weight &amp; hand length? Those were trick questions - we can’t assess the significance of a relationship without an understanding of the potential error in our sample model. The following output includes the observed sample data for 7 ETs, the estimated model calculated from these 7 ETs, confidence bands that reflect the potential error in the estimated model trend, &amp; CIs for the slope coefficients. Is there a “significant” relationship between brain weight &amp; height? What about between brain weight &amp; hand length? confint(mod_height) ## 2.5 % 97.5 % ## (Intercept) -39.6174 15.601 ## height -0.2714 2.371 confint(mod_hand) ## 2.5 % 97.5 % ## (Intercept) -29.622 41.031 ## hand -2.255 2.854 More ETs have landed! There are 25 now. Is there a “significant” relationship between brain weight &amp; height? What about between brain weight &amp; hand length? confint(mod_height) ## 2.5 % 97.5 % ## (Intercept) -6.6081 0.7504 ## height 0.4633 0.8190 confint(mod_hand) ## 2.5 % 97.5 % ## (Intercept) -4.195 13.176 ## hand -0.246 1.136 More ETs have landed! There are 500 in total. Is there a “significant” relationship between brain weight &amp; height? What about between brain weight &amp; hand length? confint(mod_height) ## 2.5 % 97.5 % ## (Intercept) -4.715 -3.0658 ## height 0.659 0.7385 confint(mod_hand) ## 2.5 % 97.5 % ## (Intercept) 4.4557 7.9347 ## hand 0.1881 0.4512 4.6.2 Warning We answered the above inferential questions using CIs alone. CIs are great - they both give us a sense for a potential magnitude of the effect we’re estimating as well as whether the effect is significant. Unfortunately, many people mistakenly emphasize the importance of the latter over the former. This leads to bad practices: Why most published research findings are false The ASA’s Statement on p-values Still not significant Let’s keep this in mind as we explore hypothesis testing so that we don’t make the same mistakes. 4.6.3 Hypothesis testing concepts Let’s switch gears to human subjects. Early explorations of the relationship between human brain size and IQ were plagued by crude measurements (weighing brains after death). Willeman et al. developed a study that would use magnetic resonance imaging (MRI) to measure brain size. The MRI scans consisted of 18 horizontal MR images that were 5 mm thick and 2.5 mm apart. Further, each image covered a 256 x 256 pixel area. Any pixel with a non-zero gray scale was considered to be “part of the brain”. 4.6.3.1 Step 1: Set up the hypotheses First, we’ll consider the relationship between brain size (\\(y\\)) and height (\\(x\\)): \\[y = \\beta_0 + \\beta_1 x + \\varepsilon\\] Researchers had a hypothesis: taller people tend to have bigger brains. We can formalize this hypothesis and connect it to our model parameters. \\(H_0\\): Null Hypothesis Status quo hypothesis. Typically represents no effect. \\(H_a:\\) Alternative Hypothesis Claim being made about the population. NOTE In a statistical hypothesis test, we assume “innocent until proven guilty.” That is, assume \\(H_0\\) is true and put the burden of proof on \\(H_a\\). What are \\(H_0\\) and \\(H_a\\) in our example? 4.6.3.2 Step 2: Compare sample results to \\(H_0\\) To evaluate their hypothesis, Willeman et al. collected the following data on 38 subjects: Variable Description MRICount total pixel count of non-zero gray scale in 18 MRI scans (the larger the count, the larger the brain!) Height subject’s height in inches VIQ verbal IQ score # Load data brain &lt;- read.csv(&quot;https://www.macalester.edu/~ajohns24/data/BrainEESEE.csv&quot;) # Fit sample model brain_mod_1 &lt;- lm(MRICount ~ Height, data=brain) coef(summary(brain_mod_1)) ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 175332 167806 1.045 0.3030573 ## Height 10690 2448 4.366 0.0001023 # Plot sample model ggplot(brain, aes(x=Height, y=MRICount)) + geom_point() + geom_smooth(method=&quot;lm&quot;) Quick review: # CIs of model coefficients confint(brain_mod_1, level=0.95) ## 2.5 % 97.5 % ## (Intercept) -164994 515658 ## Height 5724 15656 # CIs of average brain size among all 72 inch people predict(brain_mod_1, newdata=data.frame(Height=72), interval=&quot;confidence&quot;, level=0.95) ## fit lwr upr ## 1 945013 918578 971448 # CI of brain size for Jo, a specific 72 inch person predict(brain_mod_1, newdata=data.frame(Height=72), interval=&quot;prediction&quot;, level=0.95) ## fit lwr upr ## 1 945013 821516 1068510 How consistent is our sample with \\(H_0\\), ie. no association between brain size and height? Let’s conduct an experiment &amp; simulation! Class shuffle You’ve each been given the MRICount and Height for 1-2 cases in our sample of 38 subjects. Tear your paper into 2 pieces, separating the MRICount value from the Height value. Hand your Height to the person on your left. If we re-plotted these data, what do you think we’d see? Would the patterns reflect \\(H_0: \\; \\beta_1 = 0\\) or \\(H_a: \\; \\beta_1 &gt; 0\\)? We can simulate this experiment in RStudio by reshuffling the Height values: # Shuffle the brain size y shuffled_brain &lt;- brain %&gt;% mutate(MRICount = sample(MRICount, size = 38, replace = FALSE)) # Compare the first 2 rows of original and shuffled data head(brain, 2) head(shuffled_brain, 2) # Plot the shuffled data ggplot(shuffled_brain, aes(x = Height, y = MRICount)) + geom_point() + geom_smooth(method = &quot;lm&quot;, se = FALSE) + geom_abline(intercept = 175332, slope = 10690, color = &quot;red&quot;, size = 1.5) Repeat! Pass your Height data to the person on your left. Repeated shuffling Simulate 100 sets of sample data we’d expect if \\(H_0\\) were true set.seed(2000) # Get 100 shuffled samples of size 38 shuffles &lt;- rep_sample_n(___) %&gt;% mutate(___) Examine sample models we’d expect if \\(H_0\\) were true Plot the model of MRICount by Height for each of the 100 shuffled samples. FIRST: What do you anticipate these will look like? ggplot(___, aes(___)) + geom_smooth(method = &quot;lm&quot;, se = FALSE) + geom_abline(intercept = 175332, slope = 10690, color = &quot;red&quot;, size = 1.5) Examine the sampling distribution of sample slopes assuming \\(H_0\\) were true Store and plot the slopes from the 100 shuffled sample models. FIRST: What do you anticipate these will look like? slopes &lt;- shuffles %&gt;% group_by(___) %&gt;% do(___) %&gt;% filter(___) ggplot(slopes, aes(___)) + geom_histogram(color = &quot;white&quot;, binwidth = 1500) + lims(x = c(-12000, 12000)) + geom_vline(xintercept = 10690, color = &quot;red&quot;, size = 1.5) Compare OUR sample results to those we’d expect IF \\(H_0\\) were true Is our sample slope compatible with \\(H_0\\)? How can you quantify this assessment? Test Statistic A one-number summary calculated from the sample data that measures the compatibility of the data with \\(H_0\\). Connecting to lm() output For the brain hypotheses, the reported test statistic is 4.366 (the t value). How was this calculated &amp; how do we interpret it? coef(summary(brain_mod_1)) ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 175332 167806 1.045 0.3030573 ## Height 10690 2448 4.366 0.0001023 It might help to reexamine the sampling distribution of slopes we’d expect to observe if \\(H_0\\) were true (left) and the standardized theoretical version (from the Central Limit Theorem, not simulation): p-value Test statistics and their interpretations vary from setting to setting, test to test. The p-value provides a universal summary of the compatibility of our data with \\(H_0\\). It is the probability of observing a test statistic as or more extreme than ours (relative to \\(H_a\\)) if \\(H_0\\) were indeed true: \\[\\text{p-value } = P\\left(\\text{test statistic } \\; | \\; H_0 \\right)\\] Common Misconception The p-value measures the compatibility of our data with \\(H_0\\), not the compatibility of \\(H_0\\) with our data. Thus the p-value cannot be interpreted as the probability that \\(H_0\\) is true: \\[\\text{p-value } = P\\left(\\text{test statistic } \\; | \\; H_0 \\right) \\ne P\\left(H_0 \\; | \\; \\text{ test statistic}\\right)\\] p-values Reexamine the sampling distribution of slopes we’d expect to observe if \\(H_0\\) were true. Based on either of these pictures alone, approximate the p-value. In fact, for our hypothesis test \\[\\text{p-value} = 0.000051\\] Interpret this p-value and identify how this p-value for our “one-sided” test was calculated from the “two-sided” p-value provided in RStudio. summary(brain_mod_1) ## ## Call: ## lm(formula = MRICount ~ Height, data = brain) ## ## Residuals: ## Min 1Q Median 3Q Max ## -103641 -44146 -12740 40782 155916 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 175332 167806 1.04 3e-01 ## Height 10690 2448 4.37 1e-04 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 59500 on 36 degrees of freedom ## Multiple R-squared: 0.346, Adjusted R-squared: 0.328 ## F-statistic: 19.1 on 1 and 36 DF, p-value: 0.000102 How is the p-value reported in the model summary table calculated (using theory, not simulation)?! By the Normal CLT, or more accurately, the “t” distribution: pnorm(10690, mean = 0, sd = 2448, lower = FALSE) * 2 ## [1] 1.261e-05 pnorm(4.37, lower = FALSE) * 2 ## [1] 1.242e-05 pt(4.37, df = 36, lower = FALSE) * 2 ## [1] 0.0001011 Based on this p-value, what would your conclusion be - is there significant evidence of an association between height and brain size?! Interpreting p-value The smaller the p-value, the more evidence we have against \\(H_0\\): Small p-value: Data like ours would be uncommon if \\(H_0\\) were indeed true, i.e. our data are not compatible with \\(H_0\\). Large p-value: Data like ours would be typical if \\(H_0\\) were indeed true, i.e. our data are compatible with \\(H_0\\). 4.6.3.3 Step 3: Form a conclusion Forming a conclusion is a nuanced process - it should not be seen as a black-and-white decision. To BEGIN To get a sense of scale, people often compare the p-value to a chosen significance level (typically 0.05) to determine whether our data provide sufficient evidence against \\(H_0\\). p-value \\(&lt; 0.05\\) Results are statistically significant at the 0.05 level. (Reject \\(H_0\\) in favor of \\(H_a\\).) p-value \\(\\ge 0.05\\) Results are not statistically significant at the 0.05 level. (Fail to reject \\(H_0\\).) To FOLLOW UP The above guidance is nice, but alone it produces an incomplete conclusion. p-values MUST be supplemented with information about the magnitude of the sample estimate and its corresponding standard error. Conclusion In light of the p-value = 0.000102/2 = 0.000051 for our brain hypotheses \\[\\begin{split} H_0: &amp; \\beta = 0 \\\\ H_a: &amp; \\beta &gt; 0 \\\\ \\end{split} \\] what would your conclusion be? Hypothesis Testing Framework It’s impossible (and repetitive) to cover every type of hypothesis test. Rather, we’ll focus on the foundations of hypothesis testing that transfer to every hypothesis test. Though goals vary from test to test, all hypothesis tests share a common structure: Set up hypotheses \\(H_0\\): Null Hypothesis Status quo hypothesis. Typically represents no effect. \\(H_a:\\) Alternative Hypothesis Claim being made about the population parameter. NOTE: In a statistical hypothesis test, we assume “innocent until proven guilty.” That is, assume \\(H_0\\) is true and put the burden of proof on \\(H_a\\). Compare our sample results to the null hypothesis A test statistic is a one-number summary of the data that we use to assess \\(H_0\\). This number is a quick measure of the compatibility of the data with \\(H_0\\). A p-value is the probability of observing a test statistic as or more extreme than ours if \\(H_0\\) were indeed true: \\(\\text{p-value} = P(\\text{test statistic } \\; | \\; H_0)\\) Make some sort of recommendation / conclusion Examine the effect size - is it meaningful? Examine the p-value. The smaller the p-value, the more evidence we have against \\(H_0\\): Small p-value \\(\\Rightarrow\\) our data are not compatible with \\(H_0\\). Large p-value \\(\\Rightarrow\\) our data are compatible with \\(H_0\\). 4.6.4 Hypothesis Testing Practice Let’s apply &amp; extend these ideas in a new context. A simple model of wage by married Consider the following population model of a person’s wage by their marital status: \\[\\text{wage} = \\beta_0 + \\beta_1 \\text{marriedSingle} + \\varepsilon\\] where the population coefficients \\(\\beta_i\\) are unknown. Let’s test the following hypotheses about the marriedSingle coefficient: \\[\\begin{split} H_0: &amp; \\;\\; \\beta_1 = 0 \\\\ H_a: &amp; \\;\\; \\beta_1 &lt; 0 \\\\ \\end{split} \\] Interpret \\(H_0\\) and \\(H_a\\). On a separate paper, sketch the sampling distribution of the sample estimates \\(\\hat{\\beta}_1\\) that we would expect to see if \\(H_0\\) were true (i.e. if \\(\\beta_1=0\\)). NOTE: Focus on the shape and center. We’ll take care of the spread next. Let’s test these hypotheses with the CPS85 sample data in the mosaic package. Based on the CIs alone, do you think we have enough evidence to “reject” \\(H_0\\)? wage_mod_1 &lt;- lm(wage ~ married, data = CPS85) summary(wage_mod_1) Let’s do a formal test. Report &amp; interpret the test statistic (as given by summary()). Using this test statistic with the 68-95-99.7 Rule, which of the following is true: 0 &lt; p-value &lt; 0.0015 0.0015 &lt; p-value &lt; 0.025 0.025 &lt; p-value &lt; 0.16 p-value &gt; 0.16 Report &amp; interpret the more accurate p-value (as given by summary()). What’s your conclusion about the hypotheses? Controlling for age Of course, since we haven’t controlled for important covariates, we should be wary of using the above result to argue that there’s wage discrimination against single people. To this end, consider the relationship between wage and married when controlling for age: \\[\\text{wage} = \\beta_0 + \\beta_1\\text{ marriedSingle} + \\beta_2\\text{ age} + \\varepsilon\\] You’ll test the following hypotheses: \\[ \\begin{split} H_0:&amp; \\;\\; \\beta_1 = 0 \\\\ H_a:&amp; \\;\\; \\beta_1 &lt; 0 \\\\ \\end{split} \\] Interpret \\(H_0\\) and \\(H_a\\). How does this differ from the previous model? Construct the sample model: wage_mod_2 &lt;- lm(wage ~ married + age, data=CPS85) coef(summary(wage_mod_2)) ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 6.62427 0.80953 8.183 2.063e-15 ## marriedSingle -0.60000 0.47981 -1.250 2.117e-01 ## age 0.07077 0.01946 3.636 3.040e-04 confint(wage_mod_2) ## 2.5 % 97.5 % ## (Intercept) 5.03400 8.2145 ## marriedSingle -1.54257 0.3426 ## age 0.03253 0.1090 Report &amp; interpret both the test statistic &amp; p-value. What’s your conclusion? Explain the main difference between your conclusions regarding wages and marriage status from wage_mod_1 and wage_mod_2. NOTE: Don’t just say that one is significant and the other is not. Explain why this makes intuitive sense. We’ve been focusing on whether there’s a significant association/correlation between wages and marital status, when and when not controlling for age. This is a different question than “is there a strong association/correlation?”. How would you answer the latter question? Summary: “t”-Tests for Model coefficients in RStudio Consider a population model \\[y = \\beta_0 + \\beta_1x_1 + \\cdots + \\beta_k x_k\\] where population coefficients \\(\\beta_i\\) are unknown. Then the p-value given in the last column of the \\(x_i\\) row of the model summary table corresponds to the following test: \\[ \\begin{split} H_0: &amp; \\beta_i = 0 \\\\ H_a: &amp; \\beta_i \\ne 0 \\\\ \\end{split} \\] In words: \\(H_0\\) represents “no \\(x_i\\) effect”, i.e. in the presence of the other \\(x_j\\) predictors, there’s no significant relationship between \\(x_i\\) and \\(y\\). \\(H_a\\) represents an “\\(x_i\\) effect”, i.e. even in the presence of the other \\(x_j\\) predictors, there’s a significant relationship between \\(x_i\\) and \\(y\\). Typically, we test a one-sided alternative \\(H_a: \\beta_i &lt; 0\\) or \\(H_a: \\beta_i &gt; 0\\). In this case, we divide the reported p-value by 2. Finally, RStudio uses a starring system to indicate significant explanatory terms. A key is given at the bottom of the summary table: *** if p-value &lt; 0.001 ** if p-value is between 0.001 and 0.01 * if p-value is between 0.01 and 0.05 . if p-value is between 0.05 and 0.1 4.6.5 Potential Errors in Hypothesis Testing Just as there’s error in our sample estimates and confidence intervals, there’s the potential for error in hypothesis testing. We’ll distinguish between 2 types of errors: Type I error (false positive) Reject \\(H_0\\) when \\(H_0\\) is actually true. Type II error (false negative) Don’t reject \\(H_0\\) when \\(H_0\\) is actually false. To explore these concepts, you’ll run another simulation study which studies the relationship between generic variables \\(y\\) and \\(x\\): \\[y = \\beta x + \\varepsilon, \\;\\; \\varepsilon \\sim N(0, \\sigma^2)\\] We’ll explore the following hypotheses under different scenarios: \\[\\begin{split} H_0: &amp; \\beta = 0 \\\\ H_a: &amp; \\beta \\ne 0 \\\\ \\end{split}\\] To generate data under different scenarios, copy and paste the data_sim() function. When called, this randomly generates 100 data sets of size n with pairs (x,y) where the relationship between these have \\(\\beta\\) = b and \\(\\sigma\\) = sig: data_sim &lt;- function(n, b, sig){ x &lt;- rnorm(100 * n) y &lt;- b * x + rnorm(100 * n, sd = sig) data.frame(replicate = rep(1:100, each = n), x, y) } Simulating Type I error rates Suppose \\(H_0\\) is true, ie. \\(\\beta = 0\\). Under this scenario, simulate 100 samples of size 50 with residual standard deviation \\(\\sigma = 1\\): set.seed(2018) sim_0 &lt;- data_sim(n = 50, b = 0, sig = 1) head(sim_0, 3) ## replicate x y ## 1 1 -0.42298 0.6974 ## 2 1 -1.54988 0.2453 ## 3 1 -0.06443 -0.1675 Plot the relationship between y and x for each of the 100 replicates: ggplot(sim_0, aes(y = y, x = x, group = replicate)) + geom_smooth(method = &quot;lm&quot;, se = FALSE) For each of these 100 samples, test &amp; store the hypotheses related to the x term: p_vals_0 &lt;- sim_0 %&gt;% group_by(replicate) %&gt;% do(lm(y ~ x, data = .) %&gt;% tidy()) %&gt;% filter(term == &quot;x&quot;) Construct a histogram of the 100 p-values. Draw a line at the \\(\\alpha = 0.05\\) significance level. ggplot(p_vals_0, aes(x = p.value)) + geom_histogram(color = &quot;white&quot;, binwidth = 0.1) + geom_vline(xintercept = 0.05, color = &quot;red&quot;) Any p-value below the red line (\\(&lt; 0.05\\)) represents a Type I error! These correspond to samples that were generated from the \\(H_0\\) population (with \\(\\beta = 0\\)), yet had sample slopes \\(\\hat{\\beta}\\) that were far enough away from 0 to lead to a rejection of \\(H_0\\). Use the mean() function to calculate the proportion of samples that produce Type I errors: mean(p_vals_0$p.value &lt; 0.05) NOTE: This is an estimate of the Type I error rate! Suppose we changed sample size n or residual standard deviation sig. What impact would this have on the Type I error rate? (Rerun the simulation if you need to convince yourself!) Simulating Type II error rates: \\(\\beta = 0.25\\) Suppose \\(H_0\\) is false, ie. \\(\\beta \\ne 0\\). For example, assume \\(\\beta = 0.25\\). Simulate 100 samples of size 50 with this parameter value &amp; residual standard deviation \\(\\sigma = 1\\): set.seed(2018) sim_1 &lt;- data_sim(n = 50, b = 0.25, sig = 1) head(sim_1, 3) Plot the relationship between y and x for each of the 100 replicates. For each of these 100 samples, test &amp; store the hypotheses related to the x term. Construct a histogram of the 100 p-values. Draw a line at the \\(\\alpha = 0.05\\) significance level. Any p-value above the red line (\\(&gt; 0.05\\)) represents a Type II error! These correspond to samples that were generated from the \\(H_a\\) population with \\(\\beta = 0.25\\), yet had sample slopes \\(\\hat{\\beta}\\) that were too close 0 to reject \\(H_0\\). Use the mean() function to calculate the proportion of samples that produce Type II errors. NOTE: This is an estimate of the Type II error rate! Simulating Type II error rates: \\(\\beta = 1\\) Repeat the previous exercise, but assuming that \\(\\beta = 1\\). Impact of sample size and residual standard deviation Return to the simulation with \\(\\beta = 0.25\\). Play around using different sample sizes n and residual standard deviations sig. Summarize the impact of these features on the Type II error rate. In conclusion… What’s the relationship between our chosen significance level (\\(\\alpha=0.05\\)) and the corresponding probability of making a Type I error? Explain the impact of “effect size” (\\(\\beta\\)), sample size, and residual standard deviation on Type II error. 4.6.6 Extra The following data set will be on homework. Let’s play around with it now if we have time. WARNING: Expect RStudio to run a bit slowly in this section. It’s the biggest data set we’ve worked with. If you choose, you can research the cache=TRUE argument for code chunks. If you choose to do so, you need to take care to “uncache” and then “recache” your cached code any time you make changes to that chunk. You’ve likely seen the “NiceRide” bike stations around the UM campus! They’re the bright green bikes that members and casual riders can rent for short rides. NiceRide shared data here on every single rental in 2016. The Rides data below is a subset of just 40,000 of the &gt;1,000,000 rides. Rides &lt;- read.csv(&quot;https://www.macalester.edu/~ajohns24/Data/NiceRide2016sub.csv&quot;) dim(Rides) head(Rides, 3) A quick codebook: Start.date = time/date at which the bike rental began Start.station = where the bike was picked up End.date = time/date at which the bike rental ended End.station = where the bike was dropped off Total.duration..seconds. = duration of the rental/ride in seconds Account.type = whether the rider is a NiceRide member or just a casual rider Consider the following set of questions. You’ll need to clean up some of the variables before answering them. You’ll also need to install &amp; load the lubridate and ggmap packages. Visualize &amp; model the relationship between a ride’s duration &amp; the membership status of the rider. Is it significant? Visualize &amp; model the relationship between a ride’s duration &amp; the month in which the ride took place. Is it significant? Specifically, what if you compare April vs May? Play around! There are a lot of other features of the NiceRide data! Merge the Rides with the locations of the Stations. What kind of research questions can you ask / patterns can you detect? Stations &lt;- read.csv(&quot;https://www.macalester.edu/~ajohns24/Data/NiceRideStations.csv&quot;) #join the Stations and Rides MergedRides &lt;- Rides %&gt;% left_join(Stations, by=c(Start.station = &quot;Station&quot;)) %&gt;% rename(start_lat=Latitude, start_long=Longitude) %&gt;% left_join(Stations, by=c(End.station = &quot;Station&quot;)) %&gt;% rename(end_lat=Latitude, end_long=Longitude) #plot a map of rides around Mpls MN &lt;- get_map(&quot;Minneapolis&quot;, zoom=13) ggmap(MN) + geom_segment(data=MergedRides, aes(x=start_long, y=start_lat, xend=end_long, yend=end_lat), alpha=0.07) Do the route distributions/choice differ by membership status? (Construct a visualization.) How if at all does duration change by time of day? By time of day and membership status? What other questions might we ask? Play around and see if you have any insight to add about riding patterns. -->"],
["homework.html", " 5 Homework", " 5 Homework "],
["pre-bootcamp-homework-intro-to-r-rstudio-and-r-markdown.html", "5.1 Pre-Bootcamp Homework: Intro to R, RStudio, and R Markdown", " 5.1 Pre-Bootcamp Homework: Intro to R, RStudio, and R Markdown 5.1.1 Objectives Welcome to statistic and data science portion of the Math-to-Industry Bootcamp! In this bootcamp, you will build your statistical, data, and computing literacy. Doing statistics and data science requires statistical software. Preparing for this aspect of the bootcamp is the focus of your pre-bootcamp homework. Specifically, the objectives of this assignment are to Download and install R and RStudio on your machine. Become familiar with the RStudio environment and learn how to perform basic calculations in the console. Begin working with data in RStudio, including importing data, examining the structure of data, visualizing single variables, and numerically computing summary statistics. Become familiar with using R Markdown to organize, communicate, and save your work in a reproducible format. You should allocate 2-3 hours for this assignment prior to the bootcamp. If you are stuck or have questions, feel free to email David Shuman (dshuman1@macalester.edu). David will also be available to answer any questions at the IMA from 2:30-3:30 on Wednesday, June 26th, the day before this section of the bootcamp begins. 5.1.2 Introduction to RStudio As you might guess from the name, “Data Science” requires data. Working with modern (large, messy) data sets requires statistical software. We’ll exclusively use RStudio. Why? it’s free it’s open source it has a huge online community it’s the industry standard it can be used to create reproducible and lovely documents (In fact, this tutorial that you’re currently reading was constructed entirely within RStudio!) 5.1.2.1 Download R &amp; RStudio To get started, take the following two steps in the given order. Further, if you already have R/RStudio, make sure to update to the most recent versions. Download &amp; install R: https://mirror.las.iastate.edu/CRAN/ Download &amp; install RStudio: https://www.rstudio.com/products/rstudio/download/ Be sure to download the free version!! What’s the difference between R and RStudio? Mainly, RStudio requires R – thus it does everything R does and more. We will be using RStudio exclusively. 5.1.2.2 RStudio Basics Once you open RStudio, you’ll see four panes, each serving a different function: The short video below provides a quick tour of RStudio and summarizes some basic features of the console (the main points of which are summarized below). Video summary: In the RStudio console we can perform simple calculations 2 + 3 ## [1] 5 utilize built-in RStudio functions to which we supply the necessary arguments: function(arguments) sqrt(9) ## [1] 3 sum(2, 3) ## [1] 5 sum(3, 2) ## [1] 5 rep(2, 3) ## [1] 2 2 2 rep(3, 2) ## [1] 3 3 install new (open source) RStudio packages that contain specialized functions written by other RStudio users. For example: install.packages(&quot;tidyverse&quot;, dependencies = TRUE) Debugging &amp; Anti-Frustration Tips Making mistakes is inevitable and necessary when learning a new language. The following will save you some time and frustration: Spelling and capitalization matter. this and ThiS are different. With the cursor at the &gt; in the console, use the up arrow to access previous lines without re-typing. You can also find previous lines in the History tab in the upper-right panel of RStudio. Type ?rep to get help and examples for the rep function (for example). Find help online! There’s a massive RStudio community at http://stackoverflow.com/ If you have a question, somebody’s probably already written about it. Or just google. Exercise 5.1 (Warm Up) Use RStudio as a simple calculator to do the following: a. Perform a simple calculation: calculate 90/3. b. RStudio has built-in functions to which we supply the necessary arguments: function(arguments). Use the built-in function sqrt to calculate the square root of 25. c. Use the built-in function rep to repeat the number “5” eight times. d. Use the seq function to create the vector (0, 3, 6, 9, 12). (The video doesnt cover this!) e. Create a new vector by concatenating three repetitions of the vector from the previous part. Exercise 5.2 (Assignment) We often want to store our output for later use (why?). The basic idea in RStudio: `name &lt;- output` Try the following syntax line by line. NOTE: RStudio ignores any content after the #. Thus we use this to ‘comment’ and organize our code. #type square_3 square_3 #calculate 3 squared 3^2 #store this as &quot;square_3&quot; square_3 &lt;- 3^2 #type square_3 again! square_3 #do some math with square_3 square_3 + 2 5.1.3 Working with Data in RStudio 5.1.3.1 Getting Started RStudio provides a powerful tool for working with data. The following video illustrates some of the basics. It features data related to the fivethirtyeight.com article Hip-Hop is Turning on Donald Trump that we can access through the fivethirtyeight RStudio package. A summary of the video is provided below. Video summary: Import and name data How we import data into RStudio depends on its format (eg: Excel spreadsheet, csv file, txt file) and storage locations (eg: online, within Wiki, desktop). In this example, we work with the hiphop_cand_lyrics data that are stored in the fivethirtyeight RStudio package. We store this under a shorter name (HipHop) using the assignment operator &lt;-. NOTE: RStudio ignores any content after the #. Thus we use this to ‘comment’ and organize our code. # load the fivethirtyeight package library(fivethirtyeight) # store hiphop_cand_lyrics under a shorter name HipHop &lt;- hiphop_cand_lyrics Tidy data Tidy data tables have three key features: Each row represents a unit of observation. Each column represents a variable (ie. an attribute of the cases that can vary from case to case). Each variable is one of two types: quantitative = numerical categorical = discrete possibilities/categories Each entry contains a single data value; no analysis, summaries, footnotes, comments, etc., and only one value per cell In the video example, cases are individual mentions of a presidential candidate in a song, and variables include the song name, artist, candidate, sentiment, etc. Though not the focus of this course, we can always tidy up our data if it comes to us in a non-tidy format! Examining data structure We can examine the basic structure of our data using the following functions: head(HipHop) # the first 6 rows ## # A tibble: 6 x 8 ## candidate song artist sentiment theme album_release_d… line url ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;ord&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; ## 1 Mike Huck… None… Aesop… neutral &lt;NA&gt; 2011 Withe… http:… ## 2 Mike Huck… Well… Soul … negative &lt;NA&gt; 2012 Might… http:… ## 3 Jeb Bush Awe Dez &amp;… neutral &lt;NA&gt; 2006 I hea… http:… ## 4 Jeb Bush The … Diabo… negative poli… 2006 What … http:… ## 5 Jeb Bush Mone… Goril… negative pers… 2007 I&#39;m c… http:… ## 6 Jeb Bush Hidd… K-Rino negative poli… 2012 The R… http:… dim(HipHop) # dimensions = number of cases &amp; variables ## [1] 377 8 names(HipHop) # variable labels/names ## [1] &quot;candidate&quot; &quot;song&quot; &quot;artist&quot; ## [4] &quot;sentiment&quot; &quot;theme&quot; &quot;album_release_date&quot; ## [7] &quot;line&quot; &quot;url&quot; Examining a single variable To access and focus on a single variable, we can use the $ notation: HipHop$candidate HipHop$album_release_date It’s important to understand the format/class of each variable (quantitative, categorical, date, etc) in both its meaning and its structure within RStudio: class(HipHop$candidate) ## [1] &quot;character&quot; class(HipHop$album_release_date) ## [1] &quot;integer&quot; If a variable is categorical (either in character or factor format), we can determine its levels / category labels: levels(HipHop$candidate) ## NULL levels(factor(HipHop$candidate)) ## [1] &quot;Ben Carson&quot; &quot;Bernie Sanders&quot; &quot;Chris Christie&quot; ## [4] &quot;Donald Trump&quot; &quot;Hillary Clinton&quot; &quot;Jeb Bush&quot; ## [7] &quot;Mike Huckabee&quot; &quot;Ted Cruz&quot; 5.1.3.2 Univariate Graphical Summaries Once we understand its structure, we can examine and tell a story with our data! Data visualization is the first natural step. Why? Visualizations help us understand what we’re working with: What are the scales of our variables? Are there any outliers, i.e. unusual cases? What are the patterns among our variables? This understanding will inform our next steps: What statistical tool / model is appropriate? Once our analysis is complete, visualizations are a powerful way to communicate our findings and tell a story. We’ll start with univariate visualizations using the ggplot() function. Though the learning curve is steep, its “grammar” is intuitive and generalizable once mastered. ggplot() is stored in the ggplot2 package. You only have to type this once every time you open RStudio (and once in your markdown document): library(ggplot2) IMPORTANT: The best way to learn about ggplot() is to just play around - don’t worry about memorizing the syntax. Rather, focus on the patterns and potential of their application. 5.1.3.2.1 Categorical Variables The appropriate visualization depends on whether the variable is categorical or quantitative. Consider the categorical candidate variable which contains the candidate mentioned in each song in the sample. A table provides a simple summary of the number of mentions of each candidate: table(HipHop$candidate) ## ## Ben Carson Bernie Sanders Chris Christie Donald Trump ## 1 2 2 268 ## Hillary Clinton Jeb Bush Mike Huckabee Ted Cruz ## 92 9 2 1 A bar chart provides a visualization of this table. In examining the bar chart, keep your eyes on variability (how are cases spread among the categories?) and take-home message. Try the code below that builds up from a simple to a customized bar chart. At each step determine how each piece of code contributes to the plot. Upon examining these plots, notice that Donald Trump is mentioned much more frequently than the other candidates. Hillary Clinton has far fewer mentions than Trump but far more than any other candidate. library(ggplot2) # set up a plotting frame ggplot(HipHop, aes(x = candidate)) # add a layer with the bars ggplot(HipHop, aes(x = candidate)) + geom_bar() # add axis labels ggplot(HipHop, aes(x = candidate)) + geom_bar() + labs(x = &quot;Mentioned candidate&quot;, y = &quot;Number of songs&quot;) # rotate the text ggplot(HipHop, aes(x = candidate)) + geom_bar() + labs(x = &quot;Mentioned candidate&quot;, y = &quot;Number of songs&quot;) + theme(axis.text.x = element_text(angle = 45, hjust = 1)) 5.1.3.2.2 Quantitative Variables Let’s focus on just “Hillary Clinton” mentions in hip hop (don’t worry about the code for doing this yet): justHC &lt;- HipHop %&gt;% filter(candidate == &quot;Hillary Clinton&quot;) The quantitative album_release_date variable contains the release date of each song that mentions Clinton. Quantitative variables require different summary tools than categorical variables. For example, a table of the quantitative album_release_date numbers isn’t very illuminating: table(justHC$album_release_date) ## ## 1993 1994 1996 1997 1998 1999 2000 2001 2002 2003 2004 2005 2007 2008 ## 2 2 2 2 2 3 3 3 1 1 2 2 4 17 ## 2009 2010 2011 2012 2013 2014 2015 2016 ## 5 5 4 11 8 4 6 3 Here we’ll focus on 2 of many methods for visualizing the distribution of a quantitative variable: histograms and density plots. In examining these viz, keep your eyes on the following: center (what’s a typical value?) variability (how spread out are the values?) shape (how are values distributed?) outliers take-home message 5.1.3.2.2.1 Histograms Histograms are constructed by (1) dividing up the observed range of the variable into ‘bins’ of equal width; and (2) counting up the number of cases that fall into each bin. Try out the code below that builds up from a simple to a customized histogram. At each step determine how each piece of code contributes to the plot. Upon examination, note the take-home message of the histograms: Clinton has been part of the hip hop landscape since the early 1990s, though her “popularity” peaked around 2008 and has decreased since. library(ggplot2) # set up a plotting frame ggplot(justHC, aes(x = album_release_date)) # add a histogram ggplot(justHC, aes(x = album_release_date)) + geom_histogram() # add axis labels ggplot(justHC, aes(x = album_release_date)) + geom_histogram() + labs(x = &quot;Release date&quot;, y = &quot;Number of songs&quot;) # make the bins wider ggplot(justHC, aes(x = album_release_date)) + geom_histogram(binwidth=5) + labs(x = &quot;Release date&quot;, y = &quot;Number of songs&quot;) 5.1.3.3 Density Plots Density plots are essentially smooth versions of the histogram. Instead of sorting cases into discrete bins, the “density” of cases is calculated across the entire range of values. The greater the number of cases, the greater the density! The density is then scaled so that the area under the density curve always equals 1 and the area under any fraction of the curve represents the fraction of cases that lie in that range. If you’ve taken Probability, you can think of the sample density curve as an estimate of the population probability density function. Try the following code. library(ggplot2) # set up the plotting frame ggplot(justHC, aes(x = album_release_date)) # add a density curve ggplot(justHC, aes(x = album_release_date)) + geom_density() # add color and transparency ggplot(justHC, aes(x = album_release_date)) + geom_density(fill=&quot;maroon&quot;,alpha=.75) # add axis labels ggplot(justHC, aes(x = album_release_date)) + geom_density(fill=&quot;maroon&quot;,alpha=.75) + labs(x = &quot;Album release date&quot;) After completing the above exercises, you should recognize some ggplot() patterns! ggplot() patterns ggplot(), short for “grammar of graphics” plot, is a powerful function. We’ll focus on the patterns of this function over the memorization of its syntax. For example, all of our ggplot() code had the following patterns: Line 1 of the code specified the name of our data and the variable of interest within that data: ggplot(MY DATA, aes(x = VARIABLE ON X AXIS)) where aes is short for aesthetics. Except for the last line of the plot code, lines end with +. This tells RStudio that we’re not done with the plot code yet. The second row of the code indicates what kind of plot we want to make. Thus far, we’ve seen geom_bar(), geom_histogram(), geom_density(). We can change the x-axis and y-axis labels using labs(x = &quot;MY X LABEL&quot;, y = &quot;MY Y LABEL&quot;) 5.1.3.3.0.1 Numerical Summaries We can also use RStudio to compute summary statistics, such as the mean, median, and standard deviation of a variable. Here is an example for the density plot above: mean(justHC$album_release_date) ## [1] 2008 median(justHC$album_release_date) ## [1] 2008 sd(justHC$album_release_date) ## [1] 6.049 Recall that the mean and median are two measures of the trend or typical value of a variable: mean The arithmetic average. Example: the mean of \\((x_1,...,x_5) = (1, 6, 3, 20, 6)\\) is \\[\\overline{x} = \\frac{\\sum_{i=1}^5 x_i}{5} = 7.2\\] median The 50th percentile. Example: the median of \\((x_1,...,x_5) = (1, 6, 3, 20, 6)\\) is 6 since this is the middle number when we sort the sample: \\((1, 3, 6, 6, 20)\\). Beyond the simple range of scores, variance and standard deviation are common measures of variability. Suppose we have \\(n\\) sample values \\((x_1,...,x_n)\\) with a sample mean of \\(\\overline{x}\\). Sample variance is loosely interpreted as the typical squared deviations of individual cases from the mean: \\[var(x) = \\frac{\\sum_{i=1}(x_i - \\overline{x})^2}{n-1}\\] Note that the variance has (original units)2 Sample standard deviation is loosely interpreted as the typical deviation of individual cases from the mean: \\[sd(x) = \\sqrt{var(x)}\\] Note that the standard deviation has units on the original scale, and is therefore easier to interpret than the variance! 5.1.4 R Markdown and Reproducible Research By this point, you’ve tried out some syntax in the RStudio console. There are pros and cons to the console: The console is good for testing out RStudio code and other temporary work. The console is bad for organizing, communicating, and saving your work. In the video below, you’ll learn about R Markdown which is great for organizing, communicating, and saving your work! In fact, this entire tutorial that you’re reading now was constructed using R Markdown! The basic idea is that you can use R Markdown to create an html / pdf file that includes your text, LaTeX equations, R code (what you type into RStudio), and R output (the corresponding results). In doing so… there’s no need to copy and paste between RStudio and Word you have documentation of how you arrived at your conclusions revisions are easy; you can change your code and update your entire document at the click of a button! your work is reproducible by others! Reproducible research is the idea that data analyses, and more generally, scientific claims, are published with their data and software code so that others may verify the findings and build upon them. - Reproducible Research, Coursera To use R Markdown, you will write an R Markdown formatted file in RStudio and then ask RStudio to knit it into an HTML document (or occasionally a PDF or MS Word document). The following video on R Markdown was compiled for a different course, but the majority of its content translates here! Other useful R Markdown resources: R Markdown Quick Tour R Markdown Cheatsheet Exercise 5.3 (Deduce the R Markdown Format) Look at this Sample RMarkdown in RStudio, and the HTML webpage it creates. How are bullets, italics, and section headers represented in the R Markdown file? How does R code appear in the R Markdown file? In the HTML webpage, do you see the R code, the output of the R code, or both? 5.1.5 Practice Work through each exercise below. Exercise 5.4 (Install packages) We’ll use several packages throughout the bootcamp. So that we can maximize our time together in class, it’s important that you install these packages before we meet. To this end, copy and paste the following into your RStudio CONSOLE (not an RMarkdown doc). It will take a while to run - better now than later! ``` install.packages(&quot;dplyr&quot;, dependencies = TRUE) install.packages(&quot;ggplot2&quot;, dependencies = TRUE) install.packages(&quot;fivethirtyeight&quot;, dependencies = TRUE) install.packages(&quot;devtools&quot;, dependencies = TRUE) install.packages(&quot;readr&quot;, dependencies = TRUE) install.packages(&quot;mosaic&quot;, dependencies = TRUE) install.packages(&quot;rvest&quot;, dependencies = TRUE) install.packages(&quot;tidyr&quot;, dependencies = TRUE) install.packages(&quot;DAAG&quot;, dependencies = TRUE) install.packages(&quot;infer&quot;, dependencies = TRUE) install.packages(&quot;boot&quot;, dependencies = TRUE) install.packages(&quot;gapminder&quot;, dependencies = TRUE) install.packages(&quot;lubridate&quot;, dependencies = TRUE) install.packages(&quot;ggmap&quot;, dependencies = TRUE) # Try but don&#39;t worry if the following don&#39;t work # We&#39;ll only use these a couple of times install.packages(&quot;choroplethr&quot;, dependencies = TRUE) install.packages(&quot;choroplethrMaps&quot;, dependencies = TRUE) ``` Exercise 5.5 (Start a new R Markdown document) Open a new R Markdown document in which you’ll record your work (both code &amp; text). Give this document an appropriate title and author name (you!). Knit your document to an html to make sure that process works before you try anything else. Exercise 5.6 (Require packages within your R Markdown file) Even though you have installed packages on your computer, you need to tell RStudio which packages you want to actively use in each R Markdown file. In the code chunk that starts with r setup at the top of this document, add the following three lines to load the packages that you’ll need to complete the exercises: ``` library(ggplot2) library(tidyverse) library(dplyr) ``` You can also delete everything below that first code chunk. Knit again. Hot tip: Knit often. This will make it easier to debug your code / identify errors. Exercise 5.7 (Load and explore data) The “World Prison Brief” conducted by the International Centre for Prison Studies provides insight into how incarceration rates vary from country to country. Statistics from the 2010 brief (courtesy chartsbin.com) are stored at http://www.macalester.edu/~ajohns24/data/WorldIncarceration.csv where incarceration rates are reported as the number of present incarcerations per 100,000 persons. You will need to use this data for the remaining exercises. Since it’s stored as a csv file on the internet, you can import the data into RStudio and store it under the name Prison with the following: Prison &lt;- read_csv(&quot;https://www.macalester.edu/~ajohns24/data/WorldIncarceration.csv&quot;) What are the cases in this data set? Use RStudio functions to: summarize the number of cases in Prison examine the first 6 cases of Prison list out the names of all variables in Prison Exercise 5.8 (Explore a categorical variable) a. Construct a table of the number of cases that fall into each continent. b. Construct a single visualization of the table above. For practice, change the axis labels. Exercise 5.9 (Explore a quantitative variable) a. Construct a histogram of all incarceration rates. i. change the bin width to 0.5 per 100,000 ii. change the bin width to 500 per 100,000 b. Comment on the “goldilocks” problem of choosing a bin width that’s neither too small nor too big. c. Construct a density plot of incarceration rates. d. Examine the histogram and density plot. Which visualization do you prefer? Why? What are the pros and cons of each? Exercise 5.10 (Summary statistics) These plots allow us to eyeball age trends and variability. Let’s numerically summarize some of these features. a. Calculate the mean and median incarceration rate across all countries. What are the units? b. Calculate the variance and standard deviation of incarceration rates among all countries. What are the units? c. Use quantile() to calculate the exact range of the middle 95% of ages. Exercise 5.11 (Modifications) No class will teach you everything you need to know. Thus, being able to find help online is an important skill. To this end, learn how to make the following modifications to your histogram from above. a. Add a title. b. To better distinguish between the bars, use color to outline each bar in &quot;white&quot;. c. Change the fill color of the bars from black (the default) to &quot;red&quot;. d. Add two vertical lines, one representing the mean and the other representing the median incarceration rate. Use two different colors. e. Change the limits of the x-axis to range from 0-1000. Exercise 5.12 (Knit document) Knit your R Markdown document. It should yield an html file with all of your completed work. Exercise 5.13 (Complete survey) I’d like to learn a bit more about you before the start of our time together. Please complete this short survey by June 26th. "],
["homework-1-visualizing-modeling-variability.html", "5.2 Homework 1: Visualizing &amp; Modeling Variability", " 5.2 Homework 1: Visualizing &amp; Modeling Variability Directions: There are two options to complete this homework: Option A. Write your answers in a blank document: Start a new RMarkdown document Load the following packages at the top of your Rmd: dplyr, ggplot2, fivethirtyeight, mosaic Type your answers for each exercise, with some sort of identifying header in between Option B. Write your answers in between the questions: Download the Rmd file for this homework, which you can find on the course website Knit the document before writing any answers to make sure there are no compilation errors After each exercise that requires a response, enter your text and/or code chunks. It might be helpful to first include an empty code chunk with the word “solution” replacing the “r” just after the three tick marks, and then write your answer. An example is included in the Rmd file Notes: When interpreting visualizations, models, etc, be sure to do so in a contextually meaningful way. This homework is a resource for you. Record all work that is useful for your current learning &amp; future reference. Further, try your best, but don’t stay up all night trying to finish all of the exercises! We’ll discuss any questions / material you didn’t get to tomorrow. Goals: The goal of this homework is to get extra practice with visualizing relationships, constructing models, and interpreting models. You’ll also explore three new ideas: interactions between predictors controlling for covariates residuals &amp; least squares estimation 5.2.1 Interaction In their research into the “campaign value of incumbency,” Benoit and Marsh (2008) collected data on the campaign spending of 464 candidates for the 2002 elections to the Irish Dail. The authors provide the following data set campaigns &lt;- read.csv(&quot;https://www.macalester.edu/~ajohns24/data/CampaignSpending.csv&quot;) which, for all 464 candidates, measures many variables including variable meaning votes1st number of “1st preference” votes the candidate received incumb No if the candidate was a challenger, Yes if the candidate was an incumbent totalexp number of Euros spent by the candidate’s campaign The votes received varies by candidate. Our goal is to explain some of this variability: ggplot(campaigns, aes(x = votes1st)) + geom_histogram(color = &quot;white&quot;) Warm-Up: Votes vs Incumbency We might be able to explain some of the variability in votes received by a candidate’s incumbency status. Construct a visualization of the relationship between votes1st and incumb. Construct a model of votes1st by incumb, write out the model formula, &amp; interpret all coefficients. From the model coefficients, compute the average votes received by incumbents and the average votes received by challengers. We’ll learn another method to do these computations tomorrow. Warm-Up: Votes vs Incumbency Status &amp; Campaign Spending Let’s add campaign spending to our analysis. Construct 1 visualization of the relationship of votes1st vs incumb and totalexp. Construct a model of votes1st by incumb and totalexp. Store this as model2. Write out the following formulas: the full model formula a simplified formula for challengers a simplified formula for incumbents Interpret all coefficients. Remember: your interpretation of the incumb coefficient here should be different than in the first model since the models contain different sets of predictors. Use this model to predict the number of votes received by the following candidates. HINT: plug in the correct values in your model formula. Candidate 1: a challenger that spends 10,000 Euros Candidate 2: an incumbent that spends 10,000 Euros You can check your predictions using the predict() function: predict(model2, newdata = data.frame(incumb=&quot;No&quot;, totalexp = 10000)) predict(model2, newdata = data.frame(incumb=&quot;Yes&quot;, totalexp = 10000)) Check out a plot of model2. (RStudio code is included FYI, but don’t worry about it for now!) ggplot(campaigns, aes(x = totalexp, y = votes1st, color = incumb)) + geom_point(size = 0.5) + geom_abline(intercept = 1031, slope = 0.1745, color = &quot;red&quot;) + geom_abline(intercept = 3795, slope = 0.1745, color = &quot;cyan3&quot;) Since the lines are parallel, this model assumes that incumbents and challengers enjoy the same return on campaign spending. However, notice from the plot that this may not be the best assumption. Rather, without the parallel model constraint, the trend looks more like this (you’ll make this plot later): To allow our models for challengers and incumbents to have different intercepts and different slopes, we can type totalexp * incumb instead of totalexp + incumb in the lm function: new_model &lt;- lm(votes1st ~ totalexp * incumb, campaigns) summary(new_model) The totalexp:incumbYes term that you see in the model summary is called an interaction term. Mathematically, it’s the product of these two variables, totalexp * incumbYes. With this in mind, write down the model formulas: the full model formula (of the form votes1st = a + b totalexp + c incumbYes + d totalexp * incumbYes) a simplified formula for challengers (of the form votes1st = a + b totalexp) a simplified formula for incumbents (of the form votes1st = a + b totalexp) Use this model to predict the number of votes received by the following candidates. Calculate these by hand &amp; then check your work using makeFun. Candidate 1: a challenger that spends 10,000 Euros Candidate 2: an incumbent that spends 10,000 Euros predict(new_model, newdata = data.frame(incumb=&quot;No&quot;, totalexp = 10000)) predict(new_model, newdata = data.frame(incumb=&quot;Yes&quot;, totalexp = 10000)) You can visualize this model by adding a geom_smooth() to your scatterplot: ggplot(campaigns, aes(x = totalexp, y = votes1st, col = incumb)) + geom_point() + geom_smooth(method = &quot;lm&quot;) You should notice the interaction between campaign spending &amp; incumbency status, i.e. that the relationship between votes &amp; spending differs for incumbents and challengers. With this in mind, comment on what the differing intercepts and differing slopes indicate about the relationship between the three variables of interest. Putting this all together, interpret all four model coefficients from new_model. Hint: As we have for models in the past, look back to the equations for the incumbent &amp; challenger models. Interaction In modeling \\(y\\), predictors \\(x_1\\) and \\(x_2\\) interact if the relationship between \\(x_1\\) and \\(y\\) differs for different values of \\(x_2\\). 5.2.2 Covariates In examining multivariate models, we’ve seen that adding explanatory variables to the model helps to better explain variability in the response. For example, compare the plot on the left which ignores the grouping variable vs the plot on the right that includes it: However, explaining variability isn’t the only reason to include multiple predictors in a model. When exploring the relationship between response \\(y\\) and predictor \\(x_1\\), there are typically covariates for which we want to control. For example, in comparing the effectiveness of 2 drug treatments, we might want to control for patients’ ages, health statuses, etc. We’ll explore the concept of controlling for covariates using the CPS85 data in the mosaic package. These data, obtained through the Current Population Survey, contains labor force characteristics for a sample of workers in 1985. Though out of date, these data provide important illustrations of key modeling concepts: data(CPS85) head(CPS85, 3) ## wage educ race sex hispanic south married exper union age sector ## 1 9.0 10 W M NH NS Married 27 Not 43 const ## 2 5.5 12 W M NH NS Married 20 Not 38 sales ## 3 3.8 12 W F NH NS Single 4 Not 22 sales You can access the “codebook” (description of the variables) by typing the following in your console: ?CPS85 We’ll use these data to explore the pay gap between married and single workers, illustrated and modeled below: ggplot(CPS85, aes(y=wage, x=married)) + geom_boxplot() cpsmod1 &lt;- lm(wage ~ married, data = CPS85) msummary(cpsmod1) ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 9.398 0.274 34.36 &lt;2e-16 *** ## marriedSingle -1.087 0.466 -2.33 0.02 * ## ## Residual standard error: 5.12 on 532 degrees of freedom ## Multiple R-squared: 0.0101, Adjusted R-squared: 0.00826 ## F-statistic: 5.44 on 1 and 532 DF, p-value: 0.0201 From the model coefficients we see that: On average, married workers make $9.40 per hour and single workers make $1.09 less per hour than married workers. Correlation does not imply causation If you’re single, this model probably didn’t inspire you to go out and find a spouse. Why? Just because there’s a relationship between wages and marital status doesn’t mean being single causes a person to earn less. List a few confounding variables that might explain this relationship between wages and marital status. Including covariates / confounding variables One variable that might explain the observed relationship between wages &amp; marital status is years of experience - people with greater years of experience both tend to make more money and to be older &amp; married. We can control for this covariate by including it in our model. Fill in the blanks to fit a model of wage by married and exper that includes an interaction term. cpsmod2 &lt;- lm(___, data = CPS85) summary(cpsmod2) Construct a visualization of this relationship that eliminates individual data points and focuses on the trend. Use this to explain what it means for exper &amp; married to interact. ggplot(CPS85, aes(y = wage, x = exper, color = married)) + geom_smooth(method = &quot;lm&quot;) Compare two workers that both have 10 years of experience but one is married and the other is single. By how much do their predicted wages to differ? Use the model formula to calculate this difference and the plot to provide intuition. Compare two workers that both have 20 years of experience but one is married and the other is single. By how much do their predicted wages to differ? If you’d like extra practice, interpret every coefficient in this model. Controlling for more covariates Taking experience level into account added some insight into the discrepancy between single and married workers’ wages. Let’s see what happens when we control for even more variables. To this end, model wages (wage) by marital status (married) while controlling for experience (exper), years of education (educ), and the job sector (sector) in which one works. For simplicity, we’ll eliminate all interaction terms: cpsmod3 &lt;- lm(wage ~ exper + educ + sector + married, data = CPS85) summary(cpsmod3) Note: This is difficult model to visualize since there are 2 quantitative variables and 2 categorical variables with a possible 16 category combinations (2 marital statuses * 8 sectors). If you had to draw it, it would look like 16 parallel planes. Compare two workers that both have 10 years of experience, 16 years of education, and work in the service industry. If one is married and the other is single, by how much do their predicted wages to differ? Compare two workers that both have 20 years of experience, 12 years of education, and work in the manuf (manufacturing) industry. If one is married and the other is single, by how much do their predicted wages to differ? In light of a &amp; b, interpret the marriedSingle coefficient. In conclusion, we saw a marriedSingle coefficient of -1.09 in cpsmod1 and a marriedSingle coefficient of -0.40 in cpsmod3. Explain the significance of the difference between these two measurements - what insight does it provide? Extra interpretation practice If you’d like extra practice, answer the following questions related to the coefficients in cpsmod3. What is the reference level of the sector variable? HINT: You need to know what the levels of this variable are. For fixed educ, married status, and exper, in what sector do workers make the most money? The least? Interpret the educ coefficient. Interpret the manag coefficient. 5.2.3 Least Squares Estimation Thus far you’ve been using RStudio to construct models and have focused on interpreting the output. Now let’s discuss how this first step happens, ie. how sample data are used to estimate population models. Due to their simplicity and the fact that these data helped inspire Francis Galton’s development of regression methodology in the 1880’s, we’ll use the data Galton collected on the heights of a person and their parents. From the mosaic package: data(Galton) ?Galton Let response variable \\(y\\) be a person’s height and \\(x_1\\) be the height of their father. Then the (population) linear regression model of \\(y\\) vs the \\(x_1\\) is \\[y = \\beta_0 + \\beta_1 x_1\\] Note that the \\(\\beta_i\\) represent the population coefficients. Galton didn’t have data for the entire population of interest thus didn’t know the “true” values of the \\(\\beta_i\\). Rather, he used sample data to estimate the \\(\\beta_i\\) by \\(\\hat{\\beta}_i\\). That is, the sample estimate of the population model is \\[y = \\hat{\\beta}_0 + \\hat{\\beta}_1 x_1\\] In choosing the “best” estimates \\(\\hat{\\beta}_i\\), he was looking for the coefficients that best described the relationship among the sample subjects. In the visual below, we can see that the red line does a better job at capturing this relationship than the blue line does: Mainly, on average, the individual points fall closer to the red line than the blue line. The distance between an individual observation and its model value (prediction) is called a residual. Residuals Let case \\(i\\) have observed response \\(y_i\\) and predictor \\(x_i\\). Then the model / predicted value for this case is \\[\\hat{y}_i = \\hat{\\beta}_0 + \\hat{\\beta}_1 x_i\\] The difference between the observed and predicted value is the residual \\(r_i\\): \\[r_i = y_i - \\hat{y}_i\\] We can use Galton’s data to estimate the population model: \\[y = \\hat{\\beta}_0 + \\hat{\\beta}_1 x = 39.11039 + 0.39938 x\\] htmodel &lt;- lm(height ~ father, data = Galton) summary(htmodel) ## ## Call: ## lm(formula = height ~ father, data = Galton) ## ## Residuals: ## Min 1Q Median 3Q Max ## -10.268 -2.669 -0.209 2.634 11.933 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 39.1104 3.2271 12.12 &lt;2e-16 *** ## father 0.3994 0.0466 8.57 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 3.45 on 896 degrees of freedom ## Multiple R-squared: 0.0758, Adjusted R-squared: 0.0748 ## F-statistic: 73.5 on 1 and 896 DF, p-value: &lt;2e-16 Consider the following 2 sample subjects with the following measurements and plotted below: ## family father mother sex height nkids ## 14 4 75 64 F 64.5 5 ## 849 193 65 64 M 67.0 6 Calculate the residuals (the length of the vertical lines) for both of these subjects. htmodel is an lm “object”. Not only does it contain info about the model coefficients, it contains the numerical values of the residuals (residuals) and model predictions (fitted.values) for each case in the data set. Create a data frame htmodel_results that stores the observed height, model predicted height, and residual for each case in Galton: htmodel_results &lt;- data.frame(observed = Galton$height, predicted = htmodel$fitted.values, residual = htmodel$residuals) head(htmodel_results) ## observed predicted residual ## 1 73.2 70.46 2.738 ## 2 69.2 70.46 -1.262 ## 3 69.0 70.46 -1.462 ## 4 69.0 70.46 -1.462 ## 5 73.5 69.26 4.236 ## 6 72.5 69.26 3.236 What is the relationship between the observed, predicted, and residual variables in htmodel_results? (This shouldn’t be a surprise - it’s just a confirmation of the definition of a residual.) Confirm that, within rounding error, the mean residual equals 0. This property always holds for regression models! Obtain summary statistics of the residuals. Where does this information appear in summary(htmodel)? EXTRA: A COMMENT ON THEORY I hope you learned about linear regression in linear algebra! Suppose we have a sample of \\(n\\) subjects. For subject \\(i \\in \\{1,...,n\\}\\) let \\(y_i\\) denote the observed response value and \\((x_{i1}, x_{i2},...,x_{ik})\\) denote the observed values of the \\(k\\) predictors. Then we can collect our response values into a vector \\(y\\), our predictor values into a matrix \\(X\\), and our regression coefficients into a vector \\(\\beta\\). Note that a column of 1s is included for an intercept term in \\(X\\): \\[ y = \\left(\\begin{array}{c} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_n \\end{array} \\right) \\;\\;\\;\\; \\text{ and } \\;\\;\\;\\; X = \\left(\\begin{array}{ccccc} 1 &amp; x_{11} &amp; x_{12} &amp; \\cdots &amp; x_{1k} \\\\ 1 &amp; x_{21} &amp; x_{22} &amp; \\cdots &amp; x_{2k} \\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\cdots &amp; \\vdots \\\\ 1 &amp; x_{n1} &amp; x_{n2} &amp; \\cdots &amp; x_{nk} \\\\ \\end{array} \\right) \\;\\;\\;\\; \\text{ and } \\;\\;\\;\\; \\beta = \\left(\\begin{array}{c} \\beta_1 \\\\ \\beta_2 \\\\ \\vdots \\\\ \\beta_k \\end{array} \\right)\\] Then we can express the model \\(y_i = \\beta_0 + \\beta_1 x_{i1} + \\cdots + \\beta_k x_{ik}\\) for \\(i \\in \\{1,...,n\\}\\) using linear algebra: \\[y = X\\beta\\] Further, let \\(\\hat{\\beta}\\) denote the vector of sample estimated \\(\\beta\\) and \\(\\hat{y}\\) denote the vector of predictions / model values: \\[\\hat{y} = X \\hat{\\beta}\\] Thus the residual vector is \\[y - \\hat{y} = X\\beta - X\\hat{\\beta}\\] and the sum of squared residuals is \\[(y - \\hat{y})^T(y - \\hat{y})\\] Challenge: Prove that the following formula for sample coefficients \\(\\hat{\\beta}\\) are the least squares estimates of \\(\\beta\\), ie. they minimize the sum of squared residuals: \\[\\hat{\\beta} = (X^TX)^{-1}X^Ty\\] "],
["homework-2a-data-wrangling.html", "5.3 Homework 2a: Data Wrangling", " 5.3 Homework 2a: Data Wrangling The number of daily births in the US varies over the year and from day to day. What’s surprising to many people is that the variation from one day to the next can be huge: some days have only about 80% as many births as others. Why? In this activity we’ll use basic data wrangling skills to understand some drivers of daily births. The data table Birthdays in the mosaicData package gives the number of births recorded on each day of the year in each state from 1969 to 1988.5 Table 5.1: A subset of the initial birthday data. state date year births AK 1969-01-01 1969 14 AL 1969-01-01 1969 174 AR 1969-01-01 1969 78 AZ 1969-01-01 1969 84 CA 1969-01-01 1969 824 CO 1969-01-01 1969 100 For this activity, we need to work with data aggregated across the states. Exercise 5.14 (Total Across States) Create a new data table, DailyBirths, that adds up all the births for each day across all the states. Your table should have 7305 rows and 2 columns. Plot out daily births vs date. 5.3.1 Seasonality Use your DailyBirths table from the first exercise as a starting point for all of the remaining exercises! Exercise 5.15 (Examine Seasonality) To examine seasonality in birth rates, look at the number of births by week of the year (1-53) month of the year (1-12) Julian day (1-366) When are the most babies born? The fewest? For each plot, you should end up with one data point per year. 5.3.2 Day of the Week Exercise 5.16 (Examine Patterns within the Week) To examine patterns within the week, make a box plot showing the number of births by day of the week. Interpret your results. 5.3.3 Holidays Exercise 5.17 (Two Year Sample) Pick a two-year span of the Birthdays that falls in the 1980s, say, 1980/1981. Extract out the data just in this interval, calling it MyTwoYears. (Hint: filter(), year()). Plot out the births in this two-year span day by day. Color each date according to its day of the week. Make sure to choose your font size, line widths, and color scheme to make your figure legible. Explain the pattern that you see. The plot you generate for Exercise 5.17 should be generally consistent with the weekend effect and seasonal patterns we have already seen; however, a few days each year stand out as excepetions. We are going to examine the hypothesis that these are holidays. You can find a data set listing US federal holidays at https://www.macalester.edu/~dshuman1/data/112/US-Holidays.csv. Read it in as follows:6 Holidays &lt;- read.csv(&quot;https://www.macalester.edu/~dshuman1/data/112/US-Holidays.csv&quot;) %&gt;% mutate(date = as.POSIXct(lubridate::dmy(date))) Exercise 5.18 (Holidays) Now let’s update the plot from Exercise 5.17 to include the holidays. Add a variable to MyTwoYears called is_holiday. It should be TRUE when the day is a holiday, and FALSE otherwise. One way to do this is with the transformation verb %in%, for instance, is_holiday = date %in% Holidays$date. Add a geom_point layer to your plot that sets the color of the points based on the day of the week and the shape of the points based on whether or not the day is a holiday. Finally, some holidays seem to have more of an effect than others. It would be helpful to label them. Use geom_text with the holiday data to add labels to each of the holidays. Hints: You’ll have to make up a y-coordinate for each label. You can set the orientation of each label with the angle aesthetic. You’ll also have to specify the separate data table for the holidays with a data=XXX inside of the geom_text(). 5.3.4 Superstition This article from FiveThirtyEight demonstrates that fewer babies are born on the 13th of each month, and the effect is even stronger when the 13th falls on a Friday. If you have extra time or want some extra practice, you can try to recreate the first graphic in the article. 5.3.5 Geography Exercise 5.19 (Examine the Effect of Geography) In any way you choose, explore the effect of geography on birth patterns. For example, do parents in Minnesota have fewer winter babies than in other states? Which states have the largest increases or decreases in their portion of US births over time? Is the weekend effect less strong for states with a higher percentage of their populations living in rural areas? Pick any issue (not all of these) that interests you, explore it, and create a graphic to illustrate your findings. You’ll have to start with Birthdays table, not DailyBirths. The fivethirtyeight package has more recent data.↩ The point of the lubridate::dmy() function is to convert the character-string date stored in the CSV to a POSIX date-number.↩ "],
["homework-2b-model-building-evaluation.html", "5.4 Homework 2b: Model Building &amp; Evaluation", " 5.4 Homework 2b: Model Building &amp; Evaluation Directions: Install the leaps, reshape2, &amp; broom packages in your console: install.packages(&quot;leaps&quot;, dependencies = TRUE) install.packages(&quot;reshape2&quot;, dependencies = TRUE) install.packages(&quot;broom&quot;, dependencies = TRUE) Start a new RMarkdown document. Load the following packages at the top of your Rmd: dplyr, ggplot2, fivethirtyeight, leaps, reshape2, boot, broom Unlike the foundational &amp; generalizable ggplot and dplyr syntax, much of the syntax in this homework is very specialized / context specific. Just keep your eyes on the big ideas - if you get the big ideas it’s easy enough to do a search for the appropriate code. When interpreting visualizations, models, etc, be sure to do so in a contextually meaningful way. This homework is a resource for you. Record all work that is useful for your current learning &amp; future reference. Further, try your best, but don’t stay up all night trying to finish all of the exercises! We’ll discuss any questions / material you didn’t get to tomorrow. Goals: In this homework you will: apply residual analysis, \\(R^2\\), and cross validation techniques; and explore new concepts in the iterative process of model building &amp; model evaluation: multicollinearity; subset selection; overfitting; and bias-variance trade-off. 5.4.1 Warm-up In January 2017, fivethirtyeight.com published an article on hate crime rates across the US. Load the data hate_crimes they used in this article via the fivethirtyeight package and examine the codebook: # Load data data(hate_crimes) # Check out the codebook ?hate_crimes We’ll start with a little review of data wrangling, visualization, &amp; modeling. Define some new variables The pre-election hate crime rates (avg_hatecrimes_per_100k_fbi) are on a scale of 365 days. The post-election hate crime rates (hate_crimes_per_100k_splc) are on a scale of 10 days. Add variables pre_crime and post_crime to the data set that transform these orginal variables to 1 day hate crime rates. Further, share_vote_trump is currently on a 0-1 scale. Convert this to a percent on a 0-100 scale. Name the new variable trump_vote. Before moving on, confirm that the features of your data match those here: summary(hate_crimes$pre_crime) ## Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s ## 0.0007 0.0035 0.0054 0.0065 0.0087 0.0300 1 summary(hate_crimes$post_crime) ## Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s ## 0.007 0.014 0.023 0.030 0.036 0.152 4 summary(hate_crimes$trump_vote) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 4.0 41.5 49.0 49.0 57.5 70.0 Using syntax (not simply leafing through the data), identify: the states with the lowest &amp; highest pre-election hate crime rates; the states with the lowest &amp; highest post-election hate crime rate. Skim the fivethirtyeight article. Summarize the drawbacks of the processes with which the pre- and post-election hate crime rates are measured. (These are the best data available, but it’s crucial to understand the limits of our analysis!) Outliers Construct &amp; interpret a visualization of post_crime rates vs trump_vote. You should notice an outlier. Identify the corresponding state using a filter(). What do you think about this outlier? Is it the result of a data entry typo or is there something unique about the outlying state? Remove the state corresponding to this outlier from the hate_crimes data set - if we didn’t, this one state could skew our analysis. Confirm that your dimensions now match those below. NOTE: States with missing change_rate data are also removed. dim(hate_crimes) ## [1] 46 15 mean(hate_crimes$post_crime, na.rm = TRUE) ## [1] 0.02776 Starting simple We can start with a simple research question: how, if at all, are post-election hate crime rates associated with the election results? Construct and visualize a model of post_crime by trump_vote. Of course, states with high post_crime rates might be the same ones with high pre_crime rates, no matter the outcome of the election. With this in mind, incorporate pre_crime as a covariate in your model (without an interaction): post_crime ~ trump_vote + pre_crime. Compare and contrast the trump_vote coefficients from your 2 models. What’s the take-home message? 5.4.2 Model Building: Subset selection In the exercise above, the appropriate model was clear. Our goal was to understand the relationship between post_crime and trump_vote when controlling for pre_crime, thus our model had the form post_crime ~ trump_vote + pre_crime. In other scenarios, the appropriate model is less clear. Consider a new goal: using any or all of the data available to us, develop the “best” model of post_crime rates. To satisfy this goal in general, we have to balance 2 sometimes conflicting criteria: prediction accuracy build a model that’s generalizable to the broader population simplicity build a model that uses the minimum number of “necessary” predictors. This model will be easier to interpret; eliminates unnecessary noise &amp; multicollinearity; cuts costs by not requiring the continued measurement of unnecessary predictors. With these criteria in mind, there are three broad model selection &amp; regularization methods. Subset selection Identify a subset of predictors \\(x_i\\) to include in our model of \\(y\\). Shrinkage / regularization Fit a model with all \\(x_i\\), but shrink / regularize their coefficients toward or to 0 to create sparse models. Dimension reduction Reduce the number of predictors by creating a new set of predictors that are linear combinations of the \\(x_i\\). (Thus the predictors tend to lose some meaning.) We’ll only discuss the first of these - subset selection! (You’ll address the third, dimension reduction, in your machine learning module.) In the context of the hate_crimes data, our next goal is to build the “best” model of post_crime using the available predictors. Our algorithms require a new data set, hate_sub, that removes the old redundant predictors (eg: share_vote_trump), the state label (it’s not a predictor), &amp; states with missing data: hate_sub &lt;- hate_crimes %&gt;% select(-c(state, hate_crimes_per_100k_splc, avg_hatecrimes_per_100k_fbi, share_vote_trump)) hate_sub &lt;- na.omit(hate_sub) # Confirm that your datasets have these dimensions! dim(hate_crimes) ## [1] 46 15 dim(hate_sub) ## [1] 44 11 The following visualization captures the correlation between each of the 10 predictors with post_crime as well as with each other. Our goal will be to whittle the set of predictors down to only the essentials. library(reshape2) # Correlation matrix cor_matrix &lt;- round(cor(hate_sub), 2) # Reshape the matrix cor_melt &lt;- melt(cor_matrix) # Visualize the correlation matrix ggplot(cor_melt, aes(x=Var1, y=Var2, fill=abs(value))) + geom_tile() + theme(axis.text.x = element_text(angle = 60, hjust = 1)) Forward stepwise selection: by hand We’ll discuss two subset selection approaches. The first is forward stepwise selection. To get a feel for this intuitive algorithm, we’ll do the first few steps by hand. There are 10 possible predictors of post_crime. Fit 10 separate models using these predictors. Record the predictor in the model with the highest \\(R^2\\). summary(lm(post_crime ~ median_house_inc, hate_sub))$r.squared summary(lm(post_crime ~ share_unemp_seas, hate_sub))$r.squared summary(lm(post_crime ~ share_pop_metro, hate_sub))$r.squared summary(lm(post_crime ~ share_pop_hs, hate_sub))$r.squared summary(lm(post_crime ~ share_non_citizen, hate_sub))$r.squared summary(lm(post_crime ~ share_white_poverty, hate_sub))$r.squared summary(lm(post_crime ~ gini_index, hate_sub))$r.squared summary(lm(post_crime ~ share_non_white, hate_sub))$r.squared summary(lm(post_crime ~ pre_crime, hate_sub))$r.squared summary(lm(post_crime ~ trump_vote, hate_sub))$r.squared Keep the predictor you identified in part a in your model. Identify which of the other 9 predictors would produce the greatest increase in \\(R^2\\) if added to this model. This will require you to fit 9 models like this: post_crime ~ part_a_predictor + other_predictor. Keep the predictors you identified in parts a &amp; b in your model. Identify which of the other 8 predictors would produce the greatest increase in \\(R^2\\) if added to this model. This will require you to fit 8 models like this: post_crime ~ part_a_predictor + part_b_predictor + other_predictor. Forward stepwise selection in RStudio Now that you have the hang of it, let’s complete our forward stepwise selection using the regsubsets() function in the leaps package. Execute the following code &amp; examine the output. The last table in forward_summary uses asterisks (*) to indicate the order in which it added variables to the model. Confirm that these match your work from above. # Do forward stepwise selection # nvmax = 10 indicates that we want to eventually use all 10 predictors forward_step &lt;- regsubsets(post_crime ~ ., hate_sub, method = &quot;forward&quot;, nvmax = 10) # Store &amp; print the summary information forward_summary &lt;- summary(forward_step) forward_summary Forward stepwise selection presents us with 10 models, starting with a model with only 1 predictor and adding variables one by one until all 10 predictors are in the model. forward_summary and forward_step contain information about each of these models: # R^2 values for each model forward_summary$rsq # R^2 value for the model with 5 predictors forward_summary$rsq[5] # Coefficients of the model with 5 predictors) coef(forward_step, 5) Construct a visualization of the \\(R^2\\) values for the models of each subset size (1–10): forward_rsq &lt;- data.frame(subset_size = c(1:10), rsq = forward_summary$rsq) ggplot(forward_rsq, aes(x = subset_size, y = rsq)) + geom_point() + geom_line() This plot confirms what we already know: \\(R^2\\) increases as we add new variables to our model. We can also use it to perform subset selection. Explain why this plot suggests that the 6-predictor model (or even the 3-predictor model) is optimal. Think: How does this compare to the 1-predictor and 10-predictor models? Finally, use coef() to identify which predictors are part of the 6-predictor model. Forward selection details There’s often multicollinearity among the predictors in a data set (ie. the predictors are correlated with one another). Why might this explain the plateau-ing in the \\(R^2\\) plot? Note that share_non_white entered the model before share_pop_hs though we’ve seen that, alone, the latter is a better predictor of post_crime. Explain how this happened and why, accordingly, the “best” forward selection model might not include the “best” predictors. Backward selection is another stepwise technique. Can you guess how this differs from forward selection? Best subset selection Best subset selection provides an alternative to forward stepwise selection. As the name suggests, best subset selection takes the following steps. Suppose there are \\(p\\) possible predictors: Build all possible models that use any combination of predictors \\(x_i\\). With respect to some chosen criterion (eg: \\(R^2\\), CV error, etc): Find the “best” model with 1 predictor. Find the “best” model with 2 predictors. … Find the “best” model with \\(p\\) predictors. Choose among the best of the best models. Prove that there are 1,024 possible models we could construct using different combinations of the predictors in hate_sub (not including interaction or transformation terms). We can again use regsubsets() to perform best subset selection: # Perform best subsets best_subsets &lt;- regsubsets(post_crime ~ ., hate_sub, nvmax = 10) # Store the summary information best_summary &lt;- summary(best_subsets) best_summary Construct a visualization of the \\(R^2\\) values for the models of each subset size (1–10). With this info in hand, identify which subset size you think is ‘optimal’ and which variables are part of this optimal subset. Does this set of variables match those suggested by the forward stepwise selection? (It might, it might not!) What are the trade-offs between the forward selection and best subsets algorithms? Final model evaluation After identifying your “optimal” set of predictors… Fit this model in R (using the hate_crimes not hate_sub data). Examine the coefficients - what’s the take-home message? Report &amp; interpret the \\(R^2\\) value for this model. Calculate &amp; interpret the 10-fold CV error for this model. Construct and comment on two plots that help you assess whether your model meets the assumption that \\(\\varepsilon \\sim N(0, \\sigma^2)\\). 5.4.3 Bias-variance trade-off In today’s discussion &amp; homework, we’ve seen that there’s a goldilocks problem in model building: if we use too few predictors, we lose some explanatory power; if we use too many, we risk overfitting the model to our own sample data. This conundrum is related to the bias-variance trade-off. Suppose the population model of \\(y\\) is a quadratic function of \\(x\\): \\[y = f(x) + \\varepsilon = x^2 + \\varepsilon \\;\\;\\;\\; \\text{ where } \\varepsilon \\sim N(0, 0.3^2)\\] In practice, you would not have this information. Instead, you would take a sample of, say, size \\(n=100\\) from this population and observe: You consider 3 different models of \\(y\\). These are plotted below against the true model (\\(f(x) = x^2\\)) for comparison (black line). \\[\\begin{array}{lcrl} \\text{model 1 (orange): } &amp; &amp; \\hat{f}(x)&amp; = \\hat{\\beta}_0 \\\\ \\text{model 2 (red): } &amp; &amp; \\hat{f}(x)&amp; = \\hat{\\beta}_0 + \\hat{\\beta}_1 x + \\hat{\\beta}_2 x^2\\\\ \\text{model 3 (blue): } &amp; &amp; \\hat{f}(x)&amp; = \\hat{\\beta}_0 + \\hat{\\beta}_1 x + \\hat{\\beta}_2 x^2 + \\cdots + \\hat{\\beta}_{10} x^{10}\\\\ \\end{array}\\] Of course, had you gotten a different sample of data, your results would be different. Press play &amp; examine the simulation below: Which model appears to vary the most from sample to sample? Which model appears to be the least biased in its approximation of the relationship between \\(x\\) and \\(y\\)? We can measure the bias-variance trade-off using the mean squared error. (Careful: mean squared error differs from mean squared prediction error.) First some notation. For some value \\(x^*\\) of predictor \\(x\\), let \\[\\begin{split} f(x^*) &amp; = \\text{ value of the &quot;true&quot; model at } x^* \\\\ \\hat{f}(x^*) &amp; = \\text{ value of the estimated model at } x^* \\\\ \\end{split}\\] As in the simulation above, the sample model \\(\\hat{f}(x)\\) varies from sample to sample. At any \\(x^*\\), the quality of \\(\\hat{f}(x^*)\\) can be measured by the mean squared error, the average or expected squared difference between \\(f(x^*)\\) and \\(\\hat{f}(x^*)\\): \\[\\begin{split} \\text{MSE}\\left(\\hat{f}(x^*)\\right) &amp; = E\\left[\\left(f(x^*) - \\hat{f}(x^*) \\right)^2\\right] = \\text{Var}(\\hat{f}(x^*)) + \\left[\\text{Bias}(\\hat{f}(x^*))\\right]^2 \\\\ \\end{split}\\] Properties of MSE The smaller the MSE the better the model estimate. MSE can be partitioned into two key pieces. Variance \\(\\text{Var}\\left(\\hat{f}(x^*)\\right)\\) measures the variability in \\(\\hat{f}(x^*)\\) from sample to sample. Bias measures the average or expected difference between \\(f(x^*)\\) and \\(\\hat{f}(x^*)\\): \\[\\text{Bias}\\left(\\hat{f}(x^*)\\right) = E\\left(f(x^*) - \\hat{f}(x^*) \\right)\\] Ideally, \\(\\hat{f}(x^*)\\) is unbiased (bias=0) and variance is small. Let’s apply these ideas to compare our 3 models using a simulation study. The syntax is complicated - we’ll take small steps in this direction during tomorrow’s class. For now, focus on the big ideas. If you have time, pick through the code. Our simulation proceeds as follows: Generate 1000 separate samples of 100 \\((x,y)\\) values from the underlying “true” population model, \\(f(y) = x^2 + \\varepsilon\\) where \\(\\varepsilon \\sim N(0, 0.3^2)\\). # Set the random number seed set.seed(2018) # Simulate 1000 samples of size 100 sample &lt;- rep(c(1:1000), each = 100) x &lt;- runif(100*1000) y &lt;- x^2 + rnorm(100*1000, mean = 0, sd = 0.3) sim_data &lt;- data.frame(sample, x, y) # Check it out head(sim_data) Using each of the 1000 separate samples, fit model 1. Subsequently, from each model, calculate the prediction \\(\\hat{f}(x^*)\\) of \\(y\\) at \\(x^*=0.9\\). This produces 1000 total model 1 predictions (x), 1 from each sample. mod_1_pred &lt;- sim_data %&gt;% group_by(sample) %&gt;% do(lm(y ~ x, data = .) %&gt;% predict(., newdata = data.frame(x = 0.9)) %&gt;% tidy()) Repeat for models 2 &amp; 3. mod_2_pred &lt;- sim_data %&gt;% group_by(sample) %&gt;% do(lm(y ~ poly(x, 2, raw = TRUE), data = .) %&gt;% predict(., newdata = data.frame(x = 0.9)) %&gt;% tidy()) mod_3_pred &lt;- sim_data %&gt;% group_by(sample) %&gt;% do(lm(y ~ poly(x, 10, raw = TRUE), data = .) %&gt;% predict(., newdata = data.frame(x = 0.9)) %&gt;% tidy()) Calculating MSE Consider the 1000 predictions, \\(\\hat{f}(x^*)\\) at \\(x^*=0.9\\), calculated from model 2. These predictions are stored as variable x in mod_2_pred. Check out the first 6 sample predictions &amp; construct a histogram of all 1000 model 2 predictions. Variance: calculate the variance among the 1000 model 2 predictions. Bias: calculate the average difference between the true model value (\\(f(0.9) = 0.9^2 = 0.81\\)) &amp; the model predictions. Combining parts b and c, calculate the (estimated) mean square error for model 2. Visually comparing model MSE Visually compare the 1000 predictions for each of the 3 models: # Combine all predictions into a single data frame sim &lt;- data.frame(est = c(mod_1_pred$x, mod_2_pred$x, mod_3_pred$x), mod = as.factor(rep(1:3, each = 1000))) head(sim) # Density plots of the predictions from each model ggplot(sim, aes(x = est, fill = mod)) + geom_density(alpha = 0.5) + geom_vline(xintercept = 0.81) Which model produces the most biased estimates of \\(f(x^*) = 0.81\\)? The least biased? Why does this make intuitive sense? Which model produces the most variable / least stable estimates of \\(f(x^*)\\)? The least variable? Why does this make intuitive sense? Based on this plot alone, which model would you choose? Comparing models via MSE We can support these observations by calculating the variance, bias, and MSE for each model. You did this “by hand” above for model 2. The group_by() and summarize() functions in dplyr provide a shortcut: sim %&gt;% group_by(mod) %&gt;% summarize(bias = mean(0.81 - est), var = var(est)) %&gt;% mutate(mse = bias^2 + var) Which model produces the most biased estimates? Which model produces the most variable / least stable estimates? Order the models from best to worst with respect to their MSE (combined bias and variance). 5.4.4 Extra IF you finish early and want to play around with the data some more, carefully read the fivethirtyeight article that uses the hate_crimes data. Do you agree with the authors’ conclusions? (Answering this question might require you to fit new models.) IF you have even more time, revisit the bodyfat data from today’s class. Build a model using your new model building &amp; evaluation tools. IF you have even more time &amp; have a decent background in probability theory, prove that MSE can be written as the sum of Variance &amp; Bias2. "],
["homework-3-confidence-intervals-bootstrapping.html", "5.5 Homework 3: Confidence Intervals &amp; Bootstrapping", " 5.5 Homework 3: Confidence Intervals &amp; Bootstrapping Directions: Start a new RMarkdown document. Load the following packages at the top of your Rmd: dplyr, ggplot2, fivethirtyeight, infer, broom. When interpreting visualizations, models, etc, be sure to do so in a contextually meaningful way. This homework is a resource for you. Record all work that is useful for your current learning &amp; future reference. Further, try your best, but don’t stay up all night trying to finish all of the exercises! We’ll discuss any questions / material you didn’t get to tomorrow. Goals: In this homework you will: apply the concepts of confidence intervals; and explore new concepts in statistical inference: bootstrapping; confidence bands; prediction bands; and “significance”. 5.5.1 Warm-up You’ve likely heard about the controversy surrounding NFL players kneeling during the national anthem. Donald Trump has publicly disagreed with the protests. This begs the question - how might fans react? To this end, fivethirtyeight article explored the association between Trump support and interest in various sports in different designated market areas. Load these data and explore the codebook. # Load the data data(&quot;nfl_fandom_google&quot;) # Check out the first 6 rows head(nfl_fandom_google) # Examine the codebook ?nfl_fandom_google Check out the data Each row represents a “designated market area” (dma). Do a little Google search and write a 1 sentence description of what this is. Examine and summarize the first visualization in the original fivethirtyeight article. (You’ll recreate this for “extra credit” at the end of the homework.) NBA For now, we’ll focus on the relationship between nba interest (our response) and the trump_2016_vote (our predictor). Consider the population model represented by nba = \\(\\beta_0\\) + \\(\\beta_1\\) trump_2016_vote + \\(\\varepsilon\\) where we’ll assume that \\(\\varepsilon \\stackrel{ind}{\\sim} N(0,\\sigma^2)\\). Construct a visualization of the relationship between nba and trump_2016_vote. Include a representation of the least squares model of this relationship. Fit the least squares model, nba_mod, of nba by trump_2016_vote. (Take a second to interpret the coefficients.) Assess &amp; comment on whether our model assumptions appear to hold. Confidence intervals As long as the model assumptions hold (which they seem to) and our sample size of \\(n=207\\) is large enough (it probably is), the CLT guarantees that the sampling distribution of the estimate \\(\\hat{\\beta}_1\\) of \\(\\beta_1\\) is \\[\\hat{\\beta}_1 \\sim N\\left(\\beta_1, \\left(\\text{s.e.}(\\hat{\\beta}_1)\\right)^2 \\right)\\] Revisit the model summary table. Report the least squares estimate \\(\\hat{\\beta}_1\\) and its standard error \\(\\text{s.e.}(\\hat{\\beta}_1)\\). Use the CLT in conjunction with the 68-95-99.7 Rule to approximate (and interpret) a 95% CI for \\(\\beta_1\\). Obtain a more accurate CI using the confint() function. Is there enough evidence to conclude that there’s a negative association between a DMA’s NBA interest and Trump election support? 5.5.2 Bootstrapping The quality of our above inference for \\(\\beta_1\\) hinges on the CLT for the sampling distribution of \\(\\hat{\\beta}_1\\) which, in turn, hinges on satisfying the model assumptions. Thus there’s a lot of room for error - we can’t know how well the real sampling distribution of \\(\\hat{\\beta}_1\\) follows the CLT. In turn, we might be wary of the quality of the CI calculated above. Nonparametric inferential techniques offer an alternative - they’re engineered without making any assumptions about the sampling distribution of \\(\\hat{\\beta}_i\\) (ie. no CLT!). We’ll consider one such technique: the bootstrap. The saying “to pull oneself up by the bootstraps” is often attributed to Rudolf Erich Raspe’s 1781 The Surprising Adventures of Baron Munchausen in which the character pulls himself out of a swamp by his hair (not bootstraps). Image source. In short, it means to get something from nothing, through your own effort. We can extend the bootstrap idea to estimate the sampling distribution of \\(\\hat{\\beta}\\) using only our sample data and without making any distributional assumptions (eg: no Normal CLT). To this end, recall that the sampling distribution of \\(\\hat{\\beta}_1\\) is a collection of all possible estimates \\(\\hat{\\beta}_1\\) that we could get from all possible samples of the same size: Of course, we can’t observe this in practice. We only have ONE sample from the population, thus cannot approximate the sampling distribution by taking, say, 1000 different samples from the population and calculating \\(\\hat{\\beta}\\) from each. What we can do is re-sample our sample and calculate an estimate \\(\\hat{\\beta}_1\\) from each re-sample. This produces a bootstrap distribution: The specific bootstrapping algorithm is described below. Bootstrapping Suppose we’re interested in estimating some parameter \\(\\beta\\). Then starting with our sample of size \\(n\\): Take 1000 (many) REsamples of size \\(n\\) WITH REPLACEMENT from the sample. Some cases might appear multiple times, some not at all. This is analogous to seeing several similar cases in a sample from the population. Calculate \\(\\hat{\\beta}\\) using each sample. This gives 1000 estimates: \\[\\hat{\\beta}^{(1)}, ..., \\hat{\\beta}^{(1000)}\\] The distribution of these is our bootstrap distribution. The shape and spread of this distribution (though not the location) will mimic that of the sampling distribution. Use the bootstrap distribution to construct confidence intervals (and, later, hypothesis tests) for \\(\\beta\\) WITHOUT assuming any theory about the distribution, shape, spread of the sampling distribution. Properties: The shape and standard deviation of the \\(\\hat{\\beta}^{(i)}\\) bootstrap distribution will provide a good approximation of the shape and standard error of the \\(\\hat{\\beta}\\) sampling distribution so long as \\(n\\) is “large enough” and the sample is representative of the population. However, the bootstrap distribution will be centered at our sample \\(\\hat{\\beta}\\) whereas the sampling distribution is centered at \\(\\beta\\). bootstrap simulation Conduct a bootstrap simulation with the following steps. (The required syntax is similar to that we used earlier!) Set the random number seed to 2000. Use rep_sample_n() to take 1000 resamples of \\(n\\) rows (market areas) from nfl_fandom_google. Be careful: don’t forget to sample with replacement. For each of the 1000 samples, fit a model of nba by trump_2016_vote. Output the intercept and slope coefficients for each of the 1000 models. Store these in a data frame named boot_data. The output might look something like this (with possibly different numbers): head(boot_data, 3) ## # A tibble: 3 x 6 ## # Groups: replicate [2] ## replicate term estimate std.error statistic p.value ## &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 (Intercept) 34.7 1.63 21.3 7.56e-54 ## 2 1 trump_2016_vote -0.220 0.0288 -7.63 8.87e-13 ## 3 2 (Intercept) 31.4 1.61 19.5 1.10e-48 bootstrap inference: confidence intervals Create a data set, boot_slopes, that only includes the bootstrapped slopes \\(\\hat{\\beta}_1\\) (coefficient of the trump_2016_vote term). Construct and describe a histogram of the 1000 bootstrap slopes. Calculate the mean of the 1000 bootstrap slopes. (This should be close to the original least squares estimate, -0.1785!) Calculate the standard deviation of the 1000 bootstrap slopes. Notice that this is very similar to the standard error \\(\\text{s.e.}(\\hat{\\beta}_1)\\) calculated via complicated formula, 0.02891! Using only the 1000 bootstrap slopes, calculate a 95% CI for \\(\\beta_1\\). (This should be close to what you observed for the classical approach.) NOTE: You weren’t given a formula to do this. Use your intuition and, subsequently, identify the appropriate function! No matter what you do, do NOT make any assumptions about normality, the CLT, etc. This means no intervals of the form ‘estimate \\(\\pm\\) 2 (standard error)’, no use of ‘dnorm / qnorm / pnorm’, etc Extending the Bootstrap Bootstrapping! It’s quite wonderful: When the CLT is a valid assumption, bootstrap and classical results are comparable (as we saw). In cases where the CLT is suspect, a bootstrap analysis can be more reliable (at minimum, it’s a good sanity check). With NO CLT assumptions, we were able to construct CIs. Thus bootstrapping is generalizable &amp; useful in contexts for which we have limited supporting theory. As to the final point, consider the following. Above you bootstrapped inference for the trump_2016_vote model coefficient. Suppose instead we want to construct a bootstrap CI for the \\(R^2\\) of the model for nba interest vs trump_2016_vote. A complicated formula does exist for this CI. However, the quality of the CIs is sensitive to meeting the underlying theoretical assumptions about the sampling distribution for correlations. Luckily, we know how to bootstrap! Do a bootstrap simulation: for each of the 1000 resamples you constructed above, calculate the \\(R^2\\) between nba and trump_2016_vote. The output should look something like this (with possibly different numbers): head(boot_cor, 3) ## # A tibble: 3 x 2 ## # Groups: replicate [3] ## replicate x ## &lt;int&gt; &lt;dbl&gt; ## 1 1 0.221 ## 2 2 0.120 ## 3 3 0.130 HINT: From a generic lm() model, you can get the \\(R^2\\) by summary(my_model)$r.squared. Construct and interpret a 95% bootstrap confidence interval for the \\(R^2\\). 5.5.3 Confidence &amp; Prediction Bands Recall our original sample model of nba interest vs trump_2016_vote: nba_mod &lt;- lm(nba ~ trump_2016_vote, nfl_fandom_google) summary(nba_mod) ## ## Call: ## lm(formula = nba ~ trump_2016_vote, data = nfl_fandom_google) ## ## Residuals: ## Min 1Q Median 3Q Max ## -10.211 -3.647 -0.698 3.457 15.275 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 32.5372 1.6158 20.14 &lt; 2e-16 *** ## trump_2016_vote -0.1785 0.0289 -6.18 3.5e-09 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 5.1 on 205 degrees of freedom ## Multiple R-squared: 0.157, Adjusted R-squared: 0.153 ## F-statistic: 38.1 on 1 and 205 DF, p-value: 3.49e-09 Consider using our original sample model to predict nba interest (\\(y\\)) by trump_2016_vote (\\(x\\)): \\[y = 32.537 - 0.179 x + \\varepsilon\\] where, by the reported Residual standard error, \\(\\varepsilon \\sim N(0, 5.103^2)\\). There are 2 types of predictions we can make for, say, a Trump support of 50%: Model trend: Predict the average nba interest of all DMAs where Trump received 50% of the vote. Individual behavior: Predict the nba interest of a specific DMA (eg: Smalltown) where Trump received 50% of the vote. The values of the two predictions are the same: 32.53716 - 0.17853 * 50 ## [1] 23.61 However, the potential error in these predictions differs. Before moving on, check your intuition: do you think there’s more error in predicting trends or individual behavior? Let’s see… Confidence interval for the trend Calculate and interpret the 95% confidence interval for the average nba interest of all DMAs where Trump received 50% of the vote: predict(nba_mod, newdata = data.frame(trump_2016_vote = 50), interval = &quot;confidence&quot;, level = 0.95) NOTE: fit is the prediction (which due to rounding is slightly different than our “by hand” prediction), lwr gives the lower bound of the CI, and upr gives the upper bound of the CI. If you have time: the CI above was constructed utilizing parametric model assumptions. Construct an alternative nonparametric bootstrap CI. (You might find some helpful syntax in HW 2.) Prediction interval for an individual case Calculate and interpret the 95% prediction interval for the the nba interest of a specific DMA (eg: Smalltown) where Trump received 50% of the vote: predict(nba_mod, newdata = data.frame(trump_2016_vote = 50), interval = &quot;prediction&quot;, level = 0.95) If you have time: Construct an alternative nonparametric bootstrap prediction interval. HINT: You can use rnorm(1, mean = 0, sd = 5.103) to simulate a Normal residual / individual variability from the trend. Visualizing confidence &amp; prediction bands We can visualize these concepts by placing prediction and confidence bands around the entire model. These represent the intervals calculated at each value of the trump_2016_vote variable. To begin, construct confidence bands by omitting se = FALSE in the geom_smooth(): # Confidence bands ggplot(nfl_fandom_google, aes(y = nba, x = trump_2016_vote)) + geom_point() + geom_smooth(method = &quot;lm&quot;) Next, include prediction bands using this more complicated syntax: # Calculate prediction intervals for every trump_2016_vote value pred_int_1 &lt;- data.frame(nfl_fandom_google, predict(nba_mod, newdata = data.frame(trump_2016_vote = nfl_fandom_google$trump_2016_vote), interval = &#39;prediction&#39;)) head(pred_int_1) # Prediction bands ggplot(pred_int_1, aes(y = nba, x = trump_2016_vote)) + geom_point() + geom_smooth(method=&#39;lm&#39;, color=&quot;black&quot;) + geom_ribbon(aes(y=fit, ymin=lwr, ymax=upr, fill=&#39;prediction&#39;), alpha=0.2) Explain: Why is it intuitive that the prediction bands are wider than the confidence bands? Though it’s not as noticeable with the prediction bands, these and the confidence bands are always the most narrow at the same point – in this case the coordinates are (22.8, 54.5). What other meaning do these values have? Provide some proof and explain why it makes intuitive sense that the bands are narrowest at this point. Toward hypothesis testing Tomorrow we’ll start talking about hypothesis testing. We already have the tools and ideas to do some simple tests. Consider the below plots of nba and nfl interest vs trump_2016_vote. Do we have enough evidence to conclude there’s a relationship between a market’s NBA interest &amp; their Trump support? What about the relationship between a market’s NFL interest &amp; their Trump support? Explain. 5.5.4 Extra Recreate the visualization at the top of the fivethirtyeight article. To get started, you’ll first need to reshape the data so that instead of each row representing a DMA, each row represents a DMA &amp; sport combination: nfl_fandom_google %&gt;% gather(activity, market_share, -c(dma, trump_2016_vote)) "],
["homework-4-hypothesis-testing.html", "5.6 Homework 4: Hypothesis Testing", " 5.6 Homework 4: Hypothesis Testing Directions: Install the lubridate and ggmap packages in your console: install.packages(&quot;lubridate&quot;) install.packages(&quot;ggmap&quot;) Start a new RMarkdown document. Load the following packages at the top of your Rmd: dplyr, ggplot2, fivethirtyeight, infer, broom, lubridate, ggmap Goals: In this homework you will solidify your understanding of hypothesis tests; take a bootstrapping approach to hypothesis testing; and explore some common snags in hypothesis testing. 5.6.1 Bootstrap hypothesis testing + Multicollinearity The bf data contains body measurements on 250 adult males. The original data are from K. Penrose, A. Nelson, and A. Fisher (1985), ”Generalized Body Composition Prediction Equation for Men Using Simple Measurement Techniques” (abstract), Medicine and Science in Sports and Exercise, 17(2), 189. bf &lt;- read.csv(&quot;https://www.macalester.edu/~ajohns24/data/bodyfatsub.csv&quot;) BodyFat vs Hip &amp; Weight Construct the sample model of body fat percentage (BodyFat) by Hip circumference (cm) and Weight (lb). Review the Weight coefficient before moving on. Report &amp; interpret the test statistic for the Weight coefficient. Report &amp; interpret the p-value corresponding to the hypothesis that Weight is positively associated with BodyFat. Recall that this p-value is calculated under the assumption that the CLT holds: IF \\(H_0: \\beta = 0\\) holds, then \\(\\hat{\\beta} \\sim N(0, 0.0417^2)\\). Thus the p-value is represented by the area of the shaded region below. Calculate this Normal probability using pnorm(). Bonus: use pt() for the \\(t\\) probability that’s actually reported in the summary table. Bootstrap hypothesis tests Our hypothesis test conclusions depend in part on the p-value which depends upon the CLT which depends upon the validity of our model assumptions. Instead of relying on the parametric CLT assumption, we can calculate a bootstrap p-value. Obtain 1000 bootstrap estimates of the Weight parameter (for the same model as above with both Weight and Hip as explanatory variables). Create a new variable of null_slopes which shifts your bootstrap Weight coefficients so that they’re centered at 0, the null hypothesis value of the Weight coefficient. Construct and describe a density plot of the 1000 bootstrap null_slopes. Using only the 1000 null_slopes with no CLT assumptions, approximate a p-value for the hypothesis test above. (This should be within \\(\\pm 0.03\\) of what you observed for the classical approach!) A new model Noting that the p-value of the Weight coefficient is rather high, your friend concludes that weight isn’t associated with body fat percentage. Construct a model that refutes your friend’s mistake, report the model summary table, and provide specific evidence from this table that contradicts your friend’s claim. Explain why this happened, i.e. why the original model and your new model provide two different insights into the relationship between body fat percentage and weight. TAKE HOME WARNING The coefficient-related hypothesis tests reported in the model summary table must be interpreted in light of the other variables in the model. They indicate the significance of a term on top of / when controlling for other variables in the model, not simply the significance of a term on its own. This is especially important to keep in mind when working with multicollinear predictors. Thus it’s a good idea to start simple - examine models with fewer terms before jumping right into more complicated models! 5.6.2 Simpson’s Paradox The data set diam &lt;- read.csv(&quot;https://www.macalester.edu/~ajohns24/data/Diamonds.csv&quot;) contains data on price, carat (size), color, and clarity of 308 diamonds. We want to explore the association between clarity &amp; price. Clarity is classified as follows (in order from best to worst): Clarity Description IF internally flawless (no internal imperfections) VVS1 very very slightly imperfect VVS2 VS1 very slightly imperfect VS2 Before answering the questions below, think about what relationships you’d expect to see among these variables! Consider the relationship between price &amp; clarity. Construct &amp; describe a plot of the relationship between price and clarity. Construct a sample estimate of the model of price by clarity. Which clarity has the highest predicted value (and what is this value)? Which clarity has the lowest predicted value (and what is this value)? Why are these results surprising? HINT: Don’t forget about the reference level. Examine the p-values for each coefficient in the model. What do you conclude? The surprising results above can be explained by a confounding variable: carat. Construct &amp; describe a visualization of the relationship between price, clarity, and carat. Fit the model of price ~ clarity + carat. If you want the extra practice (if not, move on): Interpret every coefficient, predict the price of a 0.5 carat VVS2 diamond, and write out a model of price by carat for the VVS2 group. When controlling for carat, which clarity has the highest expected price? Which has the lowest expected price? The models with &amp; without carat lead to different conclusions about the relationship between price and clarity. This is called a Simpson’s Paradox. Explain why this happened. Construct a graphic to back you up. 5.6.3 Multiple Testing In a paper published in the Proceedings of the Royal Society B - Biological Sciences (a reputable journal), Matthews et. al. asked mothers about their consumption of 132 different foods. For each food, they tested for an association with a higher incidence of male or female babies. Letting \\(p\\) be the proportion of babies that are male: \\[ \\begin{split} H_0: &amp; p = 0.5 \\\\ H_a: &amp; p \\ne 0.5 \\\\ \\end{split} \\] NOTE: This test is concerned with a single population proportion (that of male babies). The foundations of this test are the same as for those we’ve discussed in class! Suppose that NONE of the 132 foods are truly linked to rates of male and female babies (i.e. \\(H_0\\) is true in all 132 cases). Below you’ll simulate what might happen when we test each of these 132 hypotheses, each using data from a different random sample of 50 mothers. To begin, import the list of 132 types of food: foods &lt;- read.csv(&quot;https://www.macalester.edu/~ajohns24/Data/BabyBoyFood.csv&quot;) head(foods) ## Type ## 1 Beef, ground, regular, pan-cooked ## 2 Shredded wheat cereal ## 3 BF, cereal, rice w/apples, dry, prep w/ water ## 4 BF, sweet potatoes ## 5 Pear, raw (w/ peel) ## 6 Celery, raw Let’s run an experiment. For each of the 132 foods, we’ll randomly assign 50 expectant mothers to a diet heavy in that food. We’ll then record the number of male/female babies born in each food treatment group. For each of the 132 food groups, randomly generate the sex of the 50 babies born to mothers in that group: # Set the seed set.seed(2018) # Randomly generate the number of male &amp; female babies for each food births &lt;- data.frame(nmales = rbinom(n = 132, size = 50, prob = 0.5)) %&gt;% mutate(nfemales = 50 - nmales) %&gt;% mutate(food = foods$Type) Notice that births contains 132 rows. Each row indicates which food you fed to the 50 expectant mothers and the number of male/female babies born to these mothers. Which foods have the greatest discrepancy between male/female babies? It’s reasonable to assume that, in the broader population from which we generated these data, there’s no association between birth sex and diet for any of these food groups. Yet for each food, we’ll use our sample data to test whether the birth rate of males significantly differs from 0.5. In doing so we might make a Type I error. What does this mean in our setting? Conclude that a food is linked to birth sex when it’s not. Conclude that a food is not linked to birth sex when it is. Use the following syntax to test each food group for a significant association with birth sex and add the corresponding p-values to your data set: get_p &lt;- births %&gt;% group_by(1:n()) %&gt;% do(prop.test(x = .$nmales, n = 50) %&gt;% tidy()) births &lt;- births %&gt;% mutate(p.value = get_p$p.value) Check out your births data again. What proportion of the 132 tests produced Type I errors, i.e. had p.value &lt; 0.05 despite the fact that \\(H_0\\) is true? To what foods do these belong? Can you imagine the newspaper headlines that would trumpet these results? In the original Matthews’ study, the researchers found and trumpeted a correlation between male babies &amp; mothers that eat breakfast cereal. The “findings” were picked up in popular media (just 1 example). The multiple testing performed in the study &amp; replicated in our simulation is a real concern. In many applications, we might run thousands of hypothesis tests (eg: gene sequencing). Let’s explore the math behind this phenomenon. Assume we conduct each hypothesis test at the \\(\\alpha=0.05\\) level. If we conduct any single test, what’s the probability of making a Type I error when \\(H_0\\) is true? (Think back to the notes!) What if we conduct two tests? Assuming \\(H_0\\) is true for both, what’s the probability that at least 1 results in a Type I error? What if we conduct 100 tests? In general, as the number of tests increase, the chance of observing at least one Type I error increases. One solution is to inflate the observed p-value from each test. For example, the (very simple) Bonferroni method penalizes for testing “too many” hypotheses by artificially inflating the p-value: \\[\\text{ Bonferroni-adjusted p-value } = \\text{ observed p-value } * \\text{ total # of tests} \\] Calculate the Bonferroni-adjusted p-values for our 132 tests. Of your 132 tests, what percent are still significant? That is, what is the overall Type I error rate? This is better. But can you think of any trade-offs of the Bonferroni correction? TAKE HOME WARNING Don’t fish for significance! If you test enough hypotheses, something will surely pop out as “significant” just by random chance. That is, the more hypotheses you test, the more likely you are to make a Type I error. Interpret others’ work with this in mind - were they fishing for significance? Is it possible that they’re reporting a Type I error? Do you think that you could reproduce their results in a new experiment? 5.6.4 Statistical vs Practical Significance WARNING: Expect RStudio to run a bit slowly in this section. It’s the biggest data set we’ve worked with. If you choose, you can research the cache=TRUE argument for code chunks. If you choose to do so, you need to take care to “uncache” and then “recache” your cached code any time you make changes to that chunk. You’ve likely seen the “NiceRide” bike stations around the UM campus! They’re the bright green bikes that members and casual riders can rent for short rides. NiceRide shared data here on every single rental in 2016. The Rides data below is a subset of just 40,000 of the &gt;1,000,000 rides. Rides &lt;- read.csv(&quot;https://www.macalester.edu/~ajohns24/Data/NiceRide2016sub.csv&quot;) dim(Rides) ## [1] 40000 8 head(Rides, 3) ## Start.date Start.station Start.station.number ## 1 ## 2 ## 3 8/21/2016 11:49 15th Ave SE &amp; Como Ave SE 30110 ## End.date End.station End.station.number ## 1 ## 2 ## 3 8/21/2016 11:55 4th Street &amp; 13th Ave SE 30009 ## Total.duration..seconds. Account.type ## 1 0 ## 2 0 ## 3 343 Member A quick codebook: Start.date = time/date at which the bike rental began Start.station = where the bike was picked up End.date = time/date at which the bike rental ended End.station = where the bike was dropped off Total.duration..seconds. = duration of the rental/ride in seconds Account.type = whether the rider is a NiceRide member or just a casual rider Let’s clean up this data. Try to do the following in 1 continuous series of pipes %&gt;%: Rename “Total.duration..seconds.” as “duration” (or just add a new variable called duration). Keep only the Start.date, Start.station, End.station, duration, and Account.type variables. HINT: Investigate the select() function OR use indexing. Keep only the rides that lasted &gt; 0 seconds. Currently, the Start.date is a factor variable: class(Rides$Start.date) Create 2 variables from this infomation, a variable hours that pulls the hour and a factor variable months that pulls the month. The following will come in handy: mydate &lt;- as.factor(c(&quot;10/26/2016 13:20&quot;)) as.factor(hour(mdy_hm(mydate))) as.factor(month(mdy_hm(mydate))) Duration vs Membership Status Construct a visualization of the relationship between a ride’s log(duration) and the membership status of the rider. (Why log? Try using duration first.) Construct a model of log(duration) by membership status. Summarize the estimated difference in the typical duration of rides for casual riders vs members. Be sure to interpret on the non-log scale. Contextually, what do you think explains this difference? Do these data provide statistically significant evidence that casual riders tend to take longer rides? Duration vs Month Construct a visualization of the relationship between a ride’s log(duration) and the month in which the ride took place. NOTE: be sure that months is a factor/categorical variable! Construct a model of log(duration) by month. Summarize the estimated difference in the typical duration of rides in April and May. On the non-log scale, you should find a difference of about 1 minute. Personally, this isn’t enough for me to think “Wow, people take longer rides in April!” That is, the difference is not practically significant. Do these data provide statistically significant evidence that casual riders tend to take longer rides? HINT: Check out the p-value. Parts b &amp; c illustrate the difference between statistical significance and practical significance: Results are only practically significant if the observed effect size (observed difference between means, the magnitude of a regression coefficient, etc) is large enough to have practical meaning. Results are statistically significant if the p-value is small. However, a small p-value merely reflects the presence of an effect, not necessarily a meaningful magnitude of that effect. Explain why this happened. This is, when might we observe statistically significant results that aren’t practically significant? Play Around! There are a lot of other features of the NiceRide data! Merge the Rides with the locations of the Stations: Stations &lt;- read.csv(&quot;https://www.macalester.edu/~ajohns24/Data/NiceRideStations.csv&quot;) #join the Stations and Rides MergedRides &lt;- Rides %&gt;% left_join(Stations, by=c(Start.station = &quot;Station&quot;)) %&gt;% rename(start_lat=Latitude, start_long=Longitude) %&gt;% left_join(Stations, by=c(End.station = &quot;Station&quot;)) %&gt;% rename(end_lat=Latitude, end_long=Longitude) Plot a map of the NiceRides around Minneapolis: MN&lt;-get_stamenmap(c(-93.375,44.86,-93.05,45.04),zoom=14,maptype=&quot;terrain&quot;) ggmap(MN) + geom_segment(data=MergedRides, aes(x=start_long, y=start_lat, xend=end_long, yend=end_lat), alpha=0.07) Do the route distributions/choice differ by membership status? (Construct a visualization.) How if at all does duration change by time of day? By time of day and membership status? What other questions might we ask? Play around and see if you have any insight to add about riding patterns. "]
]
