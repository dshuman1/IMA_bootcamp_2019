[
["index.html", "IMA Math-to-Industry Bootcamp 2019: Statistics! 1 Home", " IMA Math-to-Industry Bootcamp 2019: Statistics! 1 Home This material was compiled by Alicia Johnson (Macalester College) for the Summer 2018 “Math-to-Industry Boot Camp” at the Institute of Mathematics &amp; Its Applications (IMA) at the University of Minnesota, and updated by David Shuman (Macalester College) for the Summer 2019 “Math-to-Industry Boot Camp.” This work is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License. "],
["syllabus.html", " 2 Syllabus", " 2 Syllabus "],
["statistics-bootcamp-goals-and-approach.html", "2.1 Statistics Bootcamp Goals and Approach", " 2.1 Statistics Bootcamp Goals and Approach Hone your statistical, data, and computing literacy Instead of covering all statistical modeling and inference techniques in 5 days (impossible!), focus on a couple of foundational and generalizable tools: linear regression and simple classification. In doing so, we’ll bypass topics in traditional stat intros Favor statistical applications using real data over statistical theory so that you walk away with a sophisticated set of tools with real applications Play around with the RStudio statistical software. In doing so, focus on the patterns in and potential of this software. Don’t worry about memorizing syntax - this will come with experience. For example, by the end of the week you’ll likely be comfortable with ggplot() and lm() functions simply because we’ll use them a lot! Do some messy stuff. Too often, stat classes are taught with data that are nice and tidy. In the real world, data are messy and require cleaning/wrangling. As discussed in this New York Times article, “Data scientists…spend from 50 percent to 80 percent of their time mired in this more mundane labor of collecting and preparing unruly digital data, before it can be explored for useful nuggets.” Though data wrangling isn’t the focus of this bootcamp, it will be a useful and necessary part of it. Don’t worry / get too distracted by the extra coding this requires. The goal is simply for you to start recognizing the messiness of real world data and to build up confidence in dealing with it "],
["schedule.html", "2.2 Schedule", " 2.2 Schedule The schedule is in flux and likely to change throughout the week. Pre-bootcamp Homework Get up and running with RStudio. Day 1: How can we model/explain the variability in our sample data? Exploring and explaining variability using visualizations and regression models. Topics will include: multivariate visualizations multivariate regression models with covariate, categorical, interaction, and transformation terms estimation of model parameters via least squares Day 2: How do we select a model? How good is the model? Selecting a statistical model using subset selection &amp; measuring model quality. Discussion will include: residual analysis \\(R^2\\) &amp; mean squared prediction error (MSPE) cross validation overfitting bias-variance trade-off Day 2 Bonus: Data wrangling An introduction to six main data verbs in the tidyverse: mutate, select, filter, arrange, group_by, and summarize Day 3: What does this sample model tell us about trends in the broader population? Using data from a sample to make inferences about a broader population of interest. Topics will include: sampling distributions &amp; the Central Limit Theorem standard error bootstrapping prediction &amp; confidence intervals Day 4: What does this sample model tell us about trends in the population? How can we carefully interpret and communicate our conclusions? Continuing inferential techniques (hypothesis testing) and discussing common pitfalls in statistical analyses: Simpson’s Paradox multicollinearity multiple testing statistical vs practical significance errors in hypothesis testing Day 5: What if our linear regression tools aren’t appropriate for my particular analysis? Extending our foundations to nonparametric regression and classification techniques for modeling categorical variables. Topics might include: nonparametric regression (eg: KNN, trees) logistic regression nonparametric classification tools (eg: trees &amp; forests) a nudge into machine learning, the topic of this course’s sequel "],
["software-requirements.html", "2.3 Software Requirements", " 2.3 Software Requirements Statistical applications utilize data. Working with modern (large, messy) data sets requires statistical software. We’ll exclusively use RStudio. Why? it’s free it’s open source it has a huge online community it’s the industry standard it can be used to create reproducible and elegant documents (eg: your bootcamp materials!) Additional benefits of RStudio include: compelling visualization tools (e.g., ggplot2, plotly, leaflet, dygraphs, shiny, DiagrammeR), including dashboards and interactive apps that allow for automatic data updating efficient data wrangling and exploratory data analysis tools (the tidyverse) powerful statistical modeling and machine learning tools large online community continuously developing new, open packages makes workflows reproducible with R Markdown computing codes and narratives stored in the same document -&gt; easy for others to understand or pick up on your analysis results automatically generated from source code -&gt; when data changes, easy to update amenable to collaborative work (e.g., interaction with GitHub, internal package development) Here is a case study on how Airbnb Uses R To get started, take the following two steps in the given order. Further, if you already have R/RStudio, make sure to update to the most recent versions before the bootcamp. Download &amp; install R: https://mirror.las.iastate.edu/CRAN/ Download &amp; install RStudio Desktop (Open Source License): https://www.rstudio.com/products/rstudio/download/ Be sure to download the free version!! What’s the difference between R and RStudio? Mainly, RStudio requires R – thus it does everything R does and more. We will be using RStudio exclusively. You’ll take a quick tour in your pre-bootcamp homework. -->"],
["resources.html", " 3 Resources", " 3 Resources Instructor contact info: David Shuman (dshuman@macalester.edu) Website with all course materials: https://www.macalester.edu/~dshuman1/ima/bootcamp/index.html Additional resources: R for Data Science, by Hadley Wickham and Garrett Grolemund, 2017 Modern Data Science with R, by Benjamin Baumer, Daniel Kaplan, and Nicholas Horton, 2017 An Introduction to Statistical Learning, Gareth James, Daniela Witten, Trevor Hastie, and Robert Tibshirani, 2013 Collection of favorite cheat sheets All RStudio cheat sheets R Markdown: The Definitive Guide by Yihui Xie, J. J. Allaire, and Garrett Grolemund, 2018 R Markdown reference ggplot2 reference R Programming Wikibook Debugging in R: document and video Colors in R A list of useR groups and R-Ladies groups around the world "],
["course-notes.html", " 4 Course Notes", " 4 Course Notes "],
["day-1-visualizing-modeling-variability.html", "4.1 Day 1: Visualizing &amp; Modeling Variability", " 4.1 Day 1: Visualizing &amp; Modeling Variability 4.1.1 Getting Started If you arrive early, now’s a good time to get started! Find today’s notes at https://ajohns24.github.io/IMA_bootcamp_2018/day-1-visualizing-modeling-variability.html &amp; take the following steps: Open a new RMarkdown (Rmd) document: Erase everything in the Rmd. At the top, type the following: # Day 1 Notes ```{r warning = FALSE, message = FALSE} library(ggplot2) library(dplyr) library(readr) library(choroplethr) ``` Knit your document! It’s helpful to do this early and often. If you get any errors, it’s likely because you haven’t yet installed the packages above. If this is the case, return to Exercise 1 of the pre-bootcamp homework. Overall workshop plan (&amp; how it ties in with the machine learning module): Today’s plan: Welcome / intros A motivating example with a pre-bootcamp homework recap Visualizing relationships Modeling relationships with linear regression 4.1.2 Pre-Boot Camp Review Statistics is the practice of using data from a sample to make inferences about some broader population of interest. Data is a key word here. In the pre-bootcamp homework, you explored the first simple steps of a statistical analysis: familiarizing yourself with the structure of your data and conducting some simple univariate analyses. We’ll build upon this foundation in a motivating example. The Story: Since the 2016 election, people have examined the following questions: How did Trump’s support vary from county to county? How does this compare to trends in past elections? In what ways was Trump’s support associated with county demographics? We’ll explore these questions using county-level election outcomes made available by Tony McGovern on github: elect&lt;-read.csv(&quot;https://www.macalester.edu/~dshuman1/ima/bootcamp/data/county_election_results.csv&quot;) Explore the structure of the data # Check out the first rows of elect. ID the cases &amp; variables # How much data do we have? # What are the variable names? # Compute the total votes nationwide for each party Explore win_2016 (categorical) # Construct a bar chart ggplot(___, aes(___)) + geom____() Explore perrep_2016 (quantitative) # Blank canvas ggplot(elect, aes(x = perrep_2016)) # Histogram ggplot(elect, aes(x = perrep_2016)) + geom____() # Density plot ggplot(elect, aes(x = perrep_2016)) + geom____() Numerically summarize the trend &amp; variability in perrep_2016 # Trump&#39;s mean &amp; median support # Variance &amp; st dev in Trump&#39;s support # Calculate the exact 2.5th &amp; 97.5th percentiles 4.1.3 Explaining Variability The main goal in statistical modeling is often to explain the variation in one particular variable from case to case (the response variable) using the values of one or more other variables (the explanatory or predictor variables). THINK We now have a good sense for the trends &amp; variability in Trump’s support from county to county. What other variables (ie. county features) might explain some of this variability? In examining relationships between variables, we distinguish between the response &amp; predictors: response variable: Trump’s percent of the vote (the variable whose variability we would like to explain) predictors (aka explanatory variables aka features): percent white, per capita income, state color, etc (variables that might explain some of the variability in the response) None of these predictors are contained within the elect data, but we’re in luck: the df_county_demographics data set within the choroplethr package contains county level demographic data the RedBlueStates data at https://www.macalester.edu/~ajohns24/Data/RedBluePurple.csv categorizes each county as belonging to a blue/red/purple state based on state categorizations at http://www.270towin.com/. JOINING DATA SETS Step 1: load new data Think: What are the sources of these data? When were they collected? Do they suit our purposes? # Load demographic data from choroplethr package data(&quot;df_county_demographics&quot;) head(df_county_demographics, 3) ## region total_population percent_white percent_black percent_asian ## 1 1001 54907 76 18 1 ## 2 1003 187114 83 9 1 ## 3 1005 27321 46 46 0 ## percent_hispanic per_capita_income median_rent median_age ## 1 2 24571 668 37.5 ## 2 4 26766 693 41.5 ## 3 5 16829 382 38.3 # Load RedBluePurple data RedBlue &lt;- read.csv(&quot;https://www.macalester.edu/~ajohns24/Data/RedBluePurple.csv&quot;) head(RedBlue, 3) ## region polyname abb StateColor ## 1 1001 alabama AL red ## 2 1003 alabama AL red ## 3 1005 alabama AL red Step 2: join the 3 datasets into a single dataset The 3 data sets (elect, df_county_demographics, RedBlue) need to share a common identifying variable. In this case, they all contain the region variable, and we can join them according to it: # Join elect and df_county_demographics all_data &lt;- left_join(df_county_demographics, elect,by=c(&quot;region&quot;=&quot;region&quot;)) # Join all_data with RedBlue all_data &lt;- left_join(all_data, RedBlue,by=c(&quot;region&quot;=&quot;region&quot;)) names(all_data) ## [1] &quot;region&quot; &quot;total_population&quot; &quot;percent_white&quot; ## [4] &quot;percent_black&quot; &quot;percent_asian&quot; &quot;percent_hispanic&quot; ## [7] &quot;per_capita_income&quot; &quot;median_rent&quot; &quot;median_age&quot; ## [10] &quot;county&quot; &quot;total_2008&quot; &quot;dem_2008&quot; ## [13] &quot;gop_2008&quot; &quot;oth_2008&quot; &quot;total_2012&quot; ## [16] &quot;dem_2012&quot; &quot;gop_2012&quot; &quot;oth_2012&quot; ## [19] &quot;total_2016&quot; &quot;dem_2016&quot; &quot;gop_2016&quot; ## [22] &quot;oth_2016&quot; &quot;perdem_2016&quot; &quot;perrep_2016&quot; ## [25] &quot;winrep_2016&quot; &quot;perdem_2012&quot; &quot;perrep_2012&quot; ## [28] &quot;winrep_2012&quot; &quot;polyname&quot; &quot;abb&quot; ## [31] &quot;StateColor&quot; Step 3: if all else fails… If you run into errors (likely due to a missing package), you can load the data from here: ```r all_data &lt;- read.csv(&quot;https://www.macalester.edu/~ajohns24/Data/electionDemographics16.csv&quot;) ``` 4.1.4 Visualizing Relationships Basic Rules for Constructing Visualizations Instead of memorizing which plot is appropriate for which situation, it’s best to recognize patterns in constructing viz: Each quantitative variable requires a new axis. If we run out of axes, we can illustrate the scale of a quantitative variable using color or discretize it into groups &amp; treat it as categorical. Each categorical variable requires a new way to “group” the graphic (eg: using colors, shapes, separate facets, etc to capture the grouping) It is helpful to visual the data table. Here arejust 6 counties: perrep_2016 perrep_2012 median_rent StateColor winrep_2016 abb 73.53 72.52 948 blue TRUE CO 35.82 41.37 742 purple FALSE GA 46.55 44.91 582 blue TRUE MN 79.95 75.11 391 red TRUE MS 80.15 72.84 420 purple TRUE MO 87.94 83.75 367 red TRUE OK THINK Before visualizing the relationships among these variables, we need to understand what features these should have. Sketches: How might we visualize the relationships among the following sets of variables? For each one, draw a sketch of a graphic on paper. perrep_2016 vs perrep_2012 perrep_2016 vs StateColor perrep_2016 vs percent_white and StateColor (in 1 plot) perrep_2016 vs percent_white and median_rent (in 1 plot) Run through the following exercises which introduce different approaches to visualizing relationships. In doing so, don’t just construct a plot, examine what it tells us about relationship trends &amp; strength (degree of variability from the trend) as well as outliers or deviations from the trend. For each plot, think: what’s the take-home message? Scatterplots of 2 quantitative variables: perrep_2016 vs perrep_2012 Each quantitative variable has an axis. Each case is represented by a dot. # Start with a blank canvas ggplot(all_data, aes(y = perrep_2016, x = perrep_2012)) # Add a scatterplot layer ggplot(all_data, aes(y = perrep_2016, x = perrep_2012)) + geom_point() # Use text labels instead of points ggplot(all_data, aes(y = perrep_2016, x = perrep_2012)) + geom_text(aes(label = abb)) # Another predictor ggplot(all_data, aes(y = perrep_2016, x = median_rent)) + geom_point() # Another predictor ggplot(all_data, aes(y = perrep_2016, x = percent_white)) + geom_point() Side-by-side plots of 1 quantitative variable vs 1 categorical variable: perrep_2016 vs StateColor # Density plots by group ggplot(all_data, aes(x = perrep_2016, fill = StateColor)) + geom_density() # Add transparency &amp; fix colors ggplot(all_data, aes(x = perrep_2016, fill = StateColor)) + geom_density(alpha = 0.5) + scale_fill_manual(values = c(&quot;blue&quot;,&quot;purple&quot;,&quot;red&quot;)) # Split groups into separate plots ggplot(all_data, aes(x = perrep_2016, fill = StateColor)) + geom_density() + scale_fill_manual(values = c(&quot;blue&quot;,&quot;purple&quot;,&quot;red&quot;)) + facet_wrap( ~ StateColor) # Histograms instead ggplot(all_data, aes(x = perrep_2016, fill = StateColor)) + geom_histogram() + scale_fill_manual(values = c(&quot;blue&quot;,&quot;purple&quot;,&quot;red&quot;)) + facet_wrap( ~ StateColor) # Side-by-side box plots ggplot(all_data, aes(x=StateColor,y = perrep_2016, fill = StateColor)) + geom_boxplot() + scale_fill_manual(values = c(&quot;blue&quot;,&quot;purple&quot;,&quot;red&quot;)) # Change to individual states ggplot(all_data, aes(x=abb,y = perrep_2016, fill = StateColor)) + geom_boxplot() + scale_fill_manual(values = c(&quot;blue&quot;,&quot;purple&quot;,&quot;red&quot;)) Scatterplots of 1 quantitative variable vs 1 categorical &amp; 1 quantitative variable: perrep_2016 vs percent_white and StateColor If percent_white and StateColor both explain some of the variability in perrep_2016, why not include both in our analysis?! Let’s. # Scatterplot colored by group ggplot(all_data, aes(y = perrep_2016, x = percent_white, color = StateColor)) + scale_color_manual(values = c(&quot;blue&quot;,&quot;purple&quot;,&quot;red&quot;)) + geom_point(alpha = 0.5) # Scatterplots split by group ggplot(all_data, aes(y = perrep_2016, x = percent_white, color = StateColor)) + geom_point(alpha = 0.5) + scale_color_manual(values = c(&quot;blue&quot;,&quot;purple&quot;,&quot;red&quot;)) + facet_wrap( ~ StateColor) Plots of 3 quantitative variables: perrep_2016 vs percent_white and median_rent # Scatterplot: use color to represent 3rd variable ggplot(all_data, aes(y = perrep_2016, x = percent_white, color = median_rent)) + geom_point(alpha=0.5) # Scatterplot: discretize the 3rd variable ggplot(all_data, aes(y = perrep_2016, x = percent_white, color = cut(median_rent, 2))) + geom_point(alpha = 0.5) Extra: Maps! There is, of course, a geographical component to these data. Though we won’t cover spatial models in bootcamp, the visuals still help us tell a story. If the required choroplethrMaps package isn’t working for you, work with a neighbor (don’t spend too much time with this picky package). # Load choroplethrMaps library(choroplethrMaps) # Map of Trump wins all_data &lt;- mutate(all_data, value = winrep_2016) county_choropleth(all_data) # Map of Trump support all_data &lt;- mutate(all_data, value = perrep_2016) county_choropleth(all_data) # Map of percent white all_data &lt;- mutate(all_data, value=percent_white) county_choropleth(all_data) Glyph-Ready Data In the layered grammar of graphics used by ggplot, we can add different aesthetics (features such as position, size, shape, color) to each glyph (mark/symbol such as a point or bar or density curve) It is important to recognize that we first need to rearrange the data so that one row (case) of the table corresponds to each glyph. The data we just used were already in this so called “glyph-ready form” We’ll talk more as the bootcamp proceeds about how to rearrange data so that they are in this form using data wrangling commands 4.1.5 Linear Regression Models Just as when exploring single variables, there are limitations in relying solely on visualizations to analyze relationships among 2+ variables. Statistical models provide rigorous numerical summaries of relationship trends. Before going into details, examine the plots below and draw a model that captures the trend of the relationships being illustrated. Linear regression can be used to model each of these relationships. “Linear” here indicates that the linear regression model of a response variable is a linear combination of explanatory variables. It does not mean that the relationship itself is linear!! In general, let \\(y\\) be our response variable and (\\(x_1, x_2, ..., x_k\\)) be \\(k\\) explanatory variables. Then the (population) linear regression model of \\(y\\) vs the \\(x_i\\) is \\[y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\cdots + \\beta_k x_k\\] where \\(\\beta_0\\) = intercept coefficient average \\(y\\) value when \\(x_1=x_2=\\cdots=x_k=0\\) \\(\\beta_i\\) = \\(x_i\\) coefficient when holding constant all other \\(x\\), the change \\(y\\) when we increase \\(x_i\\) by 1 In RStudio, we construct sample estimates of linear regression models using the lm() (linear model) function. Consider a simple example: my_model &lt;- lm(y ~ x1 + x2, data = mydata) summary(my_model) Today we’ll focus on visualizing, constructing, and interpreting models. We’ll talk more tomorrow about model quality &amp; deviations from the model trend. IMPORTANT: Be sure to interpret the coefficients in a contextually meaningful way that tells the audience about the relationships of interest (as opposed to simply providing a definition). Models with 1 quantitative predictor: perrep_2016 vs median_age Visualize the relationship, add a regression line, &amp; construct the regression model: ggplot(all_data, aes(x = median_age, y = perrep_2016)) + geom_point(alpha = 0.25) + geom_smooth(method = &quot;lm&quot;) model1 &lt;- lm(perrep_2016 ~ median_age, data=all_data) summary(model1) Write out the estimated model formula: perrep_2016 = ___ + ___ median_age Interpret all model coefficients. What is the model’s estimate of the percentage that voted Republican in 2016 for a county with a median age of 40? a median age of 22? # Method 1 new_ages&lt;-c(22,40) model1$coefficients[1]+model1$coefficients[2]*new_ages # Method 2 new_vals=data.frame(median_age=new_ages) predict(model1,new_vals) # Method 3 library(mosaic) f&lt;-makeFun(model1) f(new_ages) Models with 1 categorical predictor: perrep_2016 vs StateColor Visualize the relationship. ggplot(all_data, aes(x = perrep_2016, fill = StateColor)) + geom_density(alpha = 0.5)+ scale_fill_manual(values=c(&quot;blue&quot;,&quot;purple&quot;,&quot;red&quot;)) % group_by(StateColor) %>% summarize(means = mean(perrep_2016, na.rm = TRUE)) ``` --> Construct the regression model and write out the estimated model formula: perrep_2016 = ___ + ___ StateColorpurple + ___ StateColorred model2 &lt;- lm(perrep_2016 ~ StateColor, data = all_data) summary(model2) Huh?! RStudio splits categorical predictors up into a reference group (the first alphabetically) and indicators for the other groups. Here, blue states are the reference group and \\[\\text{StateColorpurple} = \\begin{cases} 1 &amp; \\text{ if purple} \\\\ 0 &amp; \\text{ otherwise} \\\\ \\end{cases} \\;\\;\\;\\; \\text{ and } \\;\\;\\;\\; \\text{StateColorred} = \\begin{cases} 1 &amp; \\text{ if red} \\\\ 0 &amp; \\text{ otherwise} \\\\ \\end{cases}\\] In other words, the StateColor variable is turned into 3 “dummy variables”: \\[\\left(\\begin{array}{c} \\text{red} \\\\ \\text{purple} \\\\ \\text{purple} \\\\ \\text{blue} \\\\ \\text{red} \\end{array}\\right) \\;\\;\\; \\to \\;\\;\\; \\text{StateColorblue} = \\left(\\begin{array}{c} 0 \\\\ 0 \\\\ 0 \\\\ 1 \\\\ 0 \\end{array}\\right), \\;\\; \\text{StateColorpurple} = \\left(\\begin{array}{c} 0 \\\\ 1 \\\\ 1 \\\\ 0 \\\\ 0 \\end{array}\\right), \\;\\; \\text{StateColorred} = \\left(\\begin{array}{c} 1 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 1 \\end{array}\\right)\\] Since these sum to 1, we only need to put 2 into our model and leave the other out as a reference level. With these ideas in mind, interpret all coefficients in your model. With these ideas in mind, we can interpret all coefficients in our model. Specifically, we can plug in 0’s and 1’s to obtain 3 separate model values for the blue, purple, and red states. For example, for purple states, we have that the predicted value is Intercept + 1*StateColorpurple + 0*StateColorred = 55.665309 + 6.476411 = 62.14172, which not surprisingly is equal to the mean percentage Republican in all counties in purple states. Models with 1 quantitative predictor &amp; 1 categorical predictor: perrep_2016 vs median_age and StateColor Construct the regression model, visualize the relationship, and add regression lines for each state color. NOTE: Unfortunately, as you’ll see in homework, the geom_smooth lines don’t directly correspond to the model, so we need to add lines manually. model3 &lt;- lm(perrep_2016 ~ median_age + StateColor, data = all_data) summary(model3) ggplot(all_data, aes(x = median_age, y = perrep_2016, color = StateColor)) + geom_point(alpha = 0.25) + geom_line(aes(y=model3$fitted.values))+ scale_color_manual(values=c(&quot;blue&quot;,&quot;purple&quot;,&quot;red&quot;)) Write out the estimated model formula: perrep_2016 = ___ + ___ median_age + ___ StateColorpurple + ___ StateColorred Again, the presence of a categorical variable results in the separation of the model formula by group. To this end, plug in 0’s and 1’s to obtain 3 separate model formulas for the blue, purple, and red states: perrep_2016 = ___ + ___ median_age. Putting it all together, interpret all coefficients in your model. NOTE The median_age coefficient in model3 differs from that in model1 and the StateColor coefficients in model3 differ from those in model2. This is because a predictor’s coefficients are defined / interpreted differently depending on what other predictors are in the model. Models with 2 quantitative predictors: perrep_2016 vs median_age and median_rent Visualize the relationships and construct the regression model: ggplot(all_data, aes(y = perrep_2016, x = median_age, color = median_rent)) + geom_point(alpha = 0.5) model4 &lt;- lm(perrep_2016 ~ median_age + median_rent, data = all_data) summary(model4) Write out the estimated model formula: perrep_2016 = ___ + ___ median_age + ___ median_rent Is this the formula for a line? Multiple lines? A plane? Interpret the coefficient on median_age. Models with 2 categorical predictors: perrep_2016 vs StateColor and income_bracket For illustration’s sake, let’s split county per_capita_income (in $) into 2 income brackets: above or below $25,000: # Define income brackets all_data &lt;- all_data %&gt;% mutate(income_bracket = cut(per_capita_income, breaks = c(0,25000,65000), labels = c(&quot;low&quot;,&quot;high&quot;))) Visualize the relationship and construct the model. ggplot(all_data, aes(x = perrep_2016, fill = income_bracket)) + geom_density(alpha = 0.5) + facet_wrap(~ StateColor) model5 &lt;- lm(perrep_2016 ~ StateColor + income_bracket, data = all_data) summary(model5) Write out the estimated model formula. How many possible combinations are there of these two explanatory variables? What are the predicted values for all combinations? -->"],
["homework.html", " 5 Homework", " 5 Homework "],
["pre-bootcamp-homework-intro-to-r-rstudio-and-r-markdown.html", "5.1 Pre-Bootcamp Homework: Intro to R, RStudio, and R Markdown", " 5.1 Pre-Bootcamp Homework: Intro to R, RStudio, and R Markdown 5.1.1 Objectives Welcome to statistic and data science portion of the Math-to-Industry Bootcamp! In this bootcamp, you will build your statistical, data, and computing literacy. Doing statistics and data science requires statistical software. Preparing for this aspect of the bootcamp is the focus of your pre-bootcamp homework. Specifically, the objectives of this assignment are to Download and install R and RStudio on your machine. Become familiar with the RStudio environment and learn how to perform basic calculations in the console. Begin working with data in RStudio, including importing data, examining the structure of data, visualizing single variables, and numerically computing summary statistics. Become familiar with using R Markdown to organize, communicate, and save your work in a reproducible format. You should allocate 2-3 hours for this assignment prior to the bootcamp. If you are stuck or have questions, feel free to email David Shuman (dshuman1@macalester.edu). David will also be available to answer any questions at the IMA from 2:30-3:30 on Wednesday, June 26th, the day before this section of the bootcamp begins. 5.1.2 Introduction to RStudio As you might guess from the name, “Data Science” requires data. Working with modern (large, messy) data sets requires statistical software. We’ll exclusively use RStudio. Why? it’s free it’s open source it has a huge online community it’s the industry standard it can be used to create reproducible and lovely documents (In fact, this tutorial that you’re currently reading was constructed entirely within RStudio!) 5.1.2.1 Download R &amp; RStudio To get started, take the following two steps in the given order. Further, if you already have R/RStudio, make sure to update to the most recent versions. Download &amp; install R: https://mirror.las.iastate.edu/CRAN/ Download &amp; install RStudio: https://www.rstudio.com/products/rstudio/download/ Be sure to download the free version!! What’s the difference between R and RStudio? Mainly, RStudio requires R – thus it does everything R does and more. We will be using RStudio exclusively. 5.1.2.2 RStudio Basics Once you open RStudio, you’ll see four panes, each serving a different function: The short video below provides a quick tour of RStudio and summarizes some basic features of the console (the main points of which are summarized below). Video summary: In the RStudio console we can perform simple calculations 2 + 3 ## [1] 5 utilize built-in RStudio functions to which we supply the necessary arguments: function(arguments) sqrt(9) ## [1] 3 sum(2, 3) ## [1] 5 sum(3, 2) ## [1] 5 rep(2, 3) ## [1] 2 2 2 rep(3, 2) ## [1] 3 3 install new (open source) RStudio packages that contain specialized functions written by other RStudio users. For example: install.packages(&quot;tidyverse&quot;, dependencies = TRUE) Debugging &amp; Anti-Frustration Tips Making mistakes is inevitable and necessary when learning a new language. The following will save you some time and frustration: Spelling and capitalization matter. this and ThiS are different. With the cursor at the &gt; in the console, use the up arrow to access previous lines without re-typing. You can also find previous lines in the History tab in the upper-right panel of RStudio. Type ?rep to get help and examples for the rep function (for example). Find help online! There’s a massive RStudio community at http://stackoverflow.com/ If you have a question, somebody’s probably already written about it. Or just google. Exercise 5.1 (Warm Up) Use RStudio as a simple calculator to do the following: a. Perform a simple calculation: calculate 90/3. b. RStudio has built-in functions to which we supply the necessary arguments: function(arguments). Use the built-in function sqrt to calculate the square root of 25. c. Use the built-in function rep to repeat the number “5” eight times. d. Use the seq function to create the vector (0, 3, 6, 9, 12). (The video doesnt cover this!) e. Create a new vector by concatenating three repetitions of the vector from the previous part. Exercise 5.2 (Assignment) We often want to store our output for later use (why?). The basic idea in RStudio: `name &lt;- output` Try the following syntax line by line. NOTE: RStudio ignores any content after the #. Thus we use this to ‘comment’ and organize our code. #type square_3 square_3 #calculate 3 squared 3^2 #store this as &quot;square_3&quot; square_3 &lt;- 3^2 #type square_3 again! square_3 #do some math with square_3 square_3 + 2 5.1.3 Working with Data in RStudio 5.1.3.1 Getting Started RStudio provides a powerful tool for working with data. The following video illustrates some of the basics. It features data related to the fivethirtyeight.com article Hip-Hop is Turning on Donald Trump that we can access through the fivethirtyeight RStudio package. A summary of the video is provided below. Video summary: Import and name data How we import data into RStudio depends on its format (eg: Excel spreadsheet, csv file, txt file) and storage locations (eg: online, within Wiki, desktop). In this example, we work with the hiphop_cand_lyrics data that are stored in the fivethirtyeight RStudio package. We store this under a shorter name (HipHop) using the assignment operator &lt;-. NOTE: RStudio ignores any content after the #. Thus we use this to ‘comment’ and organize our code. # load the fivethirtyeight package library(fivethirtyeight) # store hiphop_cand_lyrics under a shorter name HipHop &lt;- hiphop_cand_lyrics Tidy data Tidy data tables have three key features: Each row represents a unit of observation. Each column represents a variable (ie. an attribute of the cases that can vary from case to case). Each variable is one of two types: quantitative = numerical categorical = discrete possibilities/categories Each entry contains a single data value; no analysis, summaries, footnotes, comments, etc., and only one value per cell In the video example, cases are individual mentions of a presidential candidate in a song, and variables include the song name, artist, candidate, sentiment, etc. Though not the focus of this course, we can always tidy up our data if it comes to us in a non-tidy format! Examining data structure We can examine the basic structure of our data using the following functions: head(HipHop) # the first 6 rows ## # A tibble: 6 x 8 ## candidate song artist sentiment theme album_release_d… line url ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;ord&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; ## 1 Mike Huck… None… Aesop… neutral &lt;NA&gt; 2011 Withe… http:… ## 2 Mike Huck… Well… Soul … negative &lt;NA&gt; 2012 Might… http:… ## 3 Jeb Bush Awe Dez &amp;… neutral &lt;NA&gt; 2006 I hea… http:… ## 4 Jeb Bush The … Diabo… negative poli… 2006 What … http:… ## 5 Jeb Bush Mone… Goril… negative pers… 2007 I&#39;m c… http:… ## 6 Jeb Bush Hidd… K-Rino negative poli… 2012 The R… http:… dim(HipHop) # dimensions = number of cases &amp; variables ## [1] 377 8 names(HipHop) # variable labels/names ## [1] &quot;candidate&quot; &quot;song&quot; &quot;artist&quot; ## [4] &quot;sentiment&quot; &quot;theme&quot; &quot;album_release_date&quot; ## [7] &quot;line&quot; &quot;url&quot; Examining a single variable To access and focus on a single variable, we can use the $ notation: HipHop$candidate HipHop$album_release_date It’s important to understand the format/class of each variable (quantitative, categorical, date, etc) in both its meaning and its structure within RStudio: class(HipHop$candidate) ## [1] &quot;character&quot; class(HipHop$album_release_date) ## [1] &quot;integer&quot; If a variable is categorical (either in character or factor format), we can determine its levels / category labels: levels(HipHop$candidate) ## NULL levels(factor(HipHop$candidate)) ## [1] &quot;Ben Carson&quot; &quot;Bernie Sanders&quot; &quot;Chris Christie&quot; ## [4] &quot;Donald Trump&quot; &quot;Hillary Clinton&quot; &quot;Jeb Bush&quot; ## [7] &quot;Mike Huckabee&quot; &quot;Ted Cruz&quot; 5.1.3.2 Univariate Graphical Summaries Once we understand its structure, we can examine and tell a story with our data! Data visualization is the first natural step. Why? Visualizations help us understand what we’re working with: What are the scales of our variables? Are there any outliers, i.e. unusual cases? What are the patterns among our variables? This understanding will inform our next steps: What statistical tool / model is appropriate? Once our analysis is complete, visualizations are a powerful way to communicate our findings and tell a story. We’ll start with univariate visualizations using the ggplot() function. Though the learning curve is steep, its “grammar” is intuitive and generalizable once mastered. ggplot() is stored in the ggplot2 package. You only have to type this once every time you open RStudio (and once in your markdown document): library(ggplot2) IMPORTANT: The best way to learn about ggplot() is to just play around - don’t worry about memorizing the syntax. Rather, focus on the patterns and potential of their application. 5.1.3.2.1 Categorical Variables The appropriate visualization depends on whether the variable is categorical or quantitative. Consider the categorical candidate variable which contains the candidate mentioned in each song in the sample. A table provides a simple summary of the number of mentions of each candidate: table(HipHop$candidate) ## ## Ben Carson Bernie Sanders Chris Christie Donald Trump ## 1 2 2 268 ## Hillary Clinton Jeb Bush Mike Huckabee Ted Cruz ## 92 9 2 1 A bar chart provides a visualization of this table. In examining the bar chart, keep your eyes on variability (how are cases spread among the categories?) and take-home message. Try the code below that builds up from a simple to a customized bar chart. At each step determine how each piece of code contributes to the plot. Upon examining these plots, notice that Donald Trump is mentioned much more frequently than the other candidates. Hillary Clinton has far fewer mentions than Trump but far more than any other candidate. library(ggplot2) # set up a plotting frame ggplot(HipHop, aes(x = candidate)) # add a layer with the bars ggplot(HipHop, aes(x = candidate)) + geom_bar() # add axis labels ggplot(HipHop, aes(x = candidate)) + geom_bar() + labs(x = &quot;Mentioned candidate&quot;, y = &quot;Number of songs&quot;) # rotate the text ggplot(HipHop, aes(x = candidate)) + geom_bar() + labs(x = &quot;Mentioned candidate&quot;, y = &quot;Number of songs&quot;) + theme(axis.text.x = element_text(angle = 45, hjust = 1)) 5.1.3.2.2 Quantitative Variables Let’s focus on just “Hillary Clinton” mentions in hip hop (don’t worry about the code for doing this yet): justHC &lt;- HipHop %&gt;% filter(candidate == &quot;Hillary Clinton&quot;) The quantitative album_release_date variable contains the release date of each song that mentions Clinton. Quantitative variables require different summary tools than categorical variables. For example, a table of the quantitative album_release_date numbers isn’t very illuminating: table(justHC$album_release_date) ## ## 1993 1994 1996 1997 1998 1999 2000 2001 2002 2003 2004 2005 2007 2008 ## 2 2 2 2 2 3 3 3 1 1 2 2 4 17 ## 2009 2010 2011 2012 2013 2014 2015 2016 ## 5 5 4 11 8 4 6 3 Here we’ll focus on 2 of many methods for visualizing the distribution of a quantitative variable: histograms and density plots. In examining these viz, keep your eyes on the following: center (what’s a typical value?) variability (how spread out are the values?) shape (how are values distributed?) outliers take-home message 5.1.3.2.2.1 Histograms Histograms are constructed by (1) dividing up the observed range of the variable into ‘bins’ of equal width; and (2) counting up the number of cases that fall into each bin. Try out the code below that builds up from a simple to a customized histogram. At each step determine how each piece of code contributes to the plot. Upon examination, note the take-home message of the histograms: Clinton has been part of the hip hop landscape since the early 1990s, though her “popularity” peaked around 2008 and has decreased since. library(ggplot2) # set up a plotting frame ggplot(justHC, aes(x = album_release_date)) # add a histogram ggplot(justHC, aes(x = album_release_date)) + geom_histogram() # add axis labels ggplot(justHC, aes(x = album_release_date)) + geom_histogram() + labs(x = &quot;Release date&quot;, y = &quot;Number of songs&quot;) # make the bins wider ggplot(justHC, aes(x = album_release_date)) + geom_histogram(binwidth=5) + labs(x = &quot;Release date&quot;, y = &quot;Number of songs&quot;) 5.1.3.3 Density Plots Density plots are essentially smooth versions of the histogram. Instead of sorting cases into discrete bins, the “density” of cases is calculated across the entire range of values. The greater the number of cases, the greater the density! The density is then scaled so that the area under the density curve always equals 1 and the area under any fraction of the curve represents the fraction of cases that lie in that range. If you’ve taken Probability, you can think of the sample density curve as an estimate of the population probability density function. Try the following code. library(ggplot2) # set up the plotting frame ggplot(justHC, aes(x = album_release_date)) # add a density curve ggplot(justHC, aes(x = album_release_date)) + geom_density() # add color and transparency ggplot(justHC, aes(x = album_release_date)) + geom_density(fill=&quot;maroon&quot;,alpha=.75) # add axis labels ggplot(justHC, aes(x = album_release_date)) + geom_density(fill=&quot;maroon&quot;,alpha=.75) + labs(x = &quot;Album release date&quot;) After completing the above exercises, you should recognize some ggplot() patterns! ggplot() patterns ggplot(), short for “grammar of graphics” plot, is a powerful function. We’ll focus on the patterns of this function over the memorization of its syntax. For example, all of our ggplot() code had the following patterns: Line 1 of the code specified the name of our data and the variable of interest within that data: ggplot(MY DATA, aes(x = VARIABLE ON X AXIS)) where aes is short for aesthetics. Except for the last line of the plot code, lines end with +. This tells RStudio that we’re not done with the plot code yet. The second row of the code indicates what kind of plot we want to make. Thus far, we’ve seen geom_bar(), geom_histogram(), geom_density(). We can change the x-axis and y-axis labels using labs(x = &quot;MY X LABEL&quot;, y = &quot;MY Y LABEL&quot;) 5.1.3.3.0.1 Numerical Summaries We can also use RStudio to compute summary statistics, such as the mean, median, and standard deviation of a variable. Here is an example for the density plot above: mean(justHC$album_release_date) ## [1] 2008 median(justHC$album_release_date) ## [1] 2008 sd(justHC$album_release_date) ## [1] 6.049 Recall that the mean and median are two measures of the trend or typical value of a variable: mean The arithmetic average. Example: the mean of \\((x_1,...,x_5) = (1, 6, 3, 20, 6)\\) is \\[\\overline{x} = \\frac{\\sum_{i=1}^5 x_i}{5} = 7.2\\] median The 50th percentile. Example: the median of \\((x_1,...,x_5) = (1, 6, 3, 20, 6)\\) is 6 since this is the middle number when we sort the sample: \\((1, 3, 6, 6, 20)\\). Beyond the simple range of scores, variance and standard deviation are common measures of variability. Suppose we have \\(n\\) sample values \\((x_1,...,x_n)\\) with a sample mean of \\(\\overline{x}\\). Sample variance is loosely interpreted as the typical squared deviations of individual cases from the mean: \\[var(x) = \\frac{\\sum_{i=1}(x_i - \\overline{x})^2}{n-1}\\] Note that the variance has (original units)2 Sample standard deviation is loosely interpreted as the typical deviation of individual cases from the mean: \\[sd(x) = \\sqrt{var(x)}\\] Note that the standard deviation has units on the original scale, and is therefore easier to interpret than the variance! 5.1.4 R Markdown and Reproducible Research By this point, you’ve tried out some syntax in the RStudio console. There are pros and cons to the console: The console is good for testing out RStudio code and other temporary work. The console is bad for organizing, communicating, and saving your work. In the video below, you’ll learn about R Markdown which is great for organizing, communicating, and saving your work! In fact, this entire tutorial that you’re reading now was constructed using R Markdown! The basic idea is that you can use R Markdown to create an html / pdf file that includes your text, LaTeX equations, R code (what you type into RStudio), and R output (the corresponding results). In doing so… there’s no need to copy and paste between RStudio and Word you have documentation of how you arrived at your conclusions revisions are easy; you can change your code and update your entire document at the click of a button! your work is reproducible by others! Reproducible research is the idea that data analyses, and more generally, scientific claims, are published with their data and software code so that others may verify the findings and build upon them. - Reproducible Research, Coursera To use R Markdown, you will write an R Markdown formatted file in RStudio and then ask RStudio to knit it into an HTML document (or occasionally a PDF or MS Word document). The following video on R Markdown was compiled for a different course, but the majority of its content translates here! Other useful R Markdown resources: R Markdown Quick Tour R Markdown Cheatsheet Exercise 5.3 (Deduce the R Markdown Format) Look at this Sample RMarkdown in RStudio, and the HTML webpage it creates. How are bullets, italics, and section headers represented in the R Markdown file? How does R code appear in the R Markdown file? In the HTML webpage, do you see the R code, the output of the R code, or both? 5.1.5 Practice Work through each exercise below. Exercise 5.4 (Install packages) We’ll use several packages throughout the bootcamp. So that we can maximize our time together in class, it’s important that you install these packages before we meet. To this end, copy and paste the following into your RStudio CONSOLE (not an RMarkdown doc). It will take a while to run - better now than later! ``` install.packages(&quot;dplyr&quot;, dependencies = TRUE) install.packages(&quot;ggplot2&quot;, dependencies = TRUE) install.packages(&quot;fivethirtyeight&quot;, dependencies = TRUE) install.packages(&quot;devtools&quot;, dependencies = TRUE) install.packages(&quot;readr&quot;, dependencies = TRUE) install.packages(&quot;mosaic&quot;, dependencies = TRUE) install.packages(&quot;rvest&quot;, dependencies = TRUE) install.packages(&quot;tidyr&quot;, dependencies = TRUE) install.packages(&quot;DAAG&quot;, dependencies = TRUE) install.packages(&quot;infer&quot;, dependencies = TRUE) install.packages(&quot;boot&quot;, dependencies = TRUE) install.packages(&quot;gapminder&quot;, dependencies = TRUE) install.packages(&quot;lubridate&quot;, dependencies = TRUE) install.packages(&quot;ggmap&quot;, dependencies = TRUE) # Try but don&#39;t worry if the following don&#39;t work # We&#39;ll only use these a couple of times install.packages(&quot;choroplethr&quot;, dependencies = TRUE) install.packages(&quot;choroplethrMaps&quot;, dependencies = TRUE) ``` Exercise 5.5 (Start a new R Markdown document) Open a new R Markdown document in which you’ll record your work (both code &amp; text). Give this document an appropriate title and author name (you!). Knit your document to an html to make sure that process works before you try anything else. Exercise 5.6 (Require packages within your R Markdown file) Even though you have installed packages on your computer, you need to tell RStudio which packages you want to actively use in each R Markdown file. In the code chunk that starts with r setup at the top of this document, add the following three lines to load the packages that you’ll need to complete the exercises: ``` library(ggplot2) library(tidyverse) library(dplyr) ``` You can also delete everything below that first code chunk. Knit again. Hot tip: Knit often. This will make it easier to debug your code / identify errors. Exercise 5.7 (Load and explore data) The “World Prison Brief” conducted by the International Centre for Prison Studies provides insight into how incarceration rates vary from country to country. Statistics from the 2010 brief (courtesy chartsbin.com) are stored at http://www.macalester.edu/~ajohns24/data/WorldIncarceration.csv where incarceration rates are reported as the number of present incarcerations per 100,000 persons. You will need to use this data for the remaining exercises. Since it’s stored as a csv file on the internet, you can import the data into RStudio and store it under the name Prison with the following: Prison &lt;- read_csv(&quot;https://www.macalester.edu/~ajohns24/data/WorldIncarceration.csv&quot;) What are the cases in this data set? Use RStudio functions to: summarize the number of cases in Prison examine the first 6 cases of Prison list out the names of all variables in Prison Exercise 5.8 (Explore a categorical variable) a. Construct a table of the number of cases that fall into each continent. b. Construct a single visualization of the table above. For practice, change the axis labels. Exercise 5.9 (Explore a quantitative variable) a. Construct a histogram of all incarceration rates. i. change the bin width to 0.5 per 100,000 ii. change the bin width to 500 per 100,000 b. Comment on the “goldilocks” problem of choosing a bin width that’s neither too small nor too big. c. Construct a density plot of incarceration rates. d. Examine the histogram and density plot. Which visualization do you prefer? Why? What are the pros and cons of each? Exercise 5.10 (Summary statistics) These plots allow us to eyeball age trends and variability. Let’s numerically summarize some of these features. a. Calculate the mean and median incarceration rate across all countries. What are the units? b. Calculate the variance and standard deviation of incarceration rates among all countries. What are the units? c. Use quantile() to calculate the exact range of the middle 95% of ages. Exercise 5.11 (Modifications) No class will teach you everything you need to know. Thus, being able to find help online is an important skill. To this end, learn how to make the following modifications to your histogram from above. a. Add a title. b. To better distinguish between the bars, use color to outline each bar in &quot;white&quot;. c. Change the fill color of the bars from black (the default) to &quot;red&quot;. d. Add two vertical lines, one representing the mean and the other representing the median incarceration rate. Use two different colors. e. Change the limits of the x-axis to range from 0-1000. Exercise 5.12 (Knit document) Knit your R Markdown document. It should yield an html file with all of your completed work. Exercise 5.13 (Complete survey) I’d like to learn a bit more about you before the start of our time together. Please complete this short survey by June 26th. "],
["homework-1-visualizing-modeling-variability.html", "5.2 Homework 1: Visualizing &amp; Modeling Variability", " 5.2 Homework 1: Visualizing &amp; Modeling Variability Directions: There are two options to complete this homework: Option A. Write your answers in a blank document: Start a new RMarkdown document Load the following packages at the top of your Rmd: dplyr, ggplot2, fivethirtyeight, mosaic Type your answers for each exercise, with some sort of identifying header in between Option B. Write your answers in between the questions: Download the Rmd file for this homework, which you can find on the course website Knit the document before writing any answers to make sure there are no compilation errors After each exercise that requires a response, enter your text and/or code chunks. It might be helpful to first include an empty code chunk with the word “solution” replacing the “r” just after the three tick marks, and then write your answer. An example is included in the Rmd file Notes: When interpreting visualizations, models, etc, be sure to do so in a contextually meaningful way. This homework is a resource for you. Record all work that is useful for your current learning &amp; future reference. Further, try your best, but don’t stay up all night trying to finish all of the exercises! We’ll discuss any questions / material you didn’t get to tomorrow. Goals: The goal of this homework is to get extra practice with visualizing relationships, constructing models, and interpreting models. You’ll also explore three new ideas: interactions between predictors controlling for covariates residuals &amp; least squares estimation 5.2.1 Interaction In their research into the “campaign value of incumbency,” Benoit and Marsh (2008) collected data on the campaign spending of 464 candidates for the 2002 elections to the Irish Dail. The authors provide the following data set campaigns &lt;- read.csv(&quot;https://www.macalester.edu/~ajohns24/data/CampaignSpending.csv&quot;) which, for all 464 candidates, measures many variables including variable meaning votes1st number of “1st preference” votes the candidate received incumb No if the candidate was a challenger, Yes if the candidate was an incumbent totalexp number of Euros spent by the candidate’s campaign The votes received varies by candidate. Our goal is to explain some of this variability: ggplot(campaigns, aes(x = votes1st)) + geom_histogram(color = &quot;white&quot;) Warm-Up: Votes vs Incumbency We might be able to explain some of the variability in votes received by a candidate’s incumbency status. Construct a visualization of the relationship between votes1st and incumb. Construct a model of votes1st by incumb, write out the model formula, &amp; interpret all coefficients. From the model coefficients, compute the average votes received by incumbents and the average votes received by challengers. We’ll learn another method to do these computations tomorrow. Warm-Up: Votes vs Incumbency Status &amp; Campaign Spending Let’s add campaign spending to our analysis. Construct 1 visualization of the relationship of votes1st vs incumb and totalexp. Construct a model of votes1st by incumb and totalexp. Store this as model2. Write out the following formulas: the full model formula a simplified formula for challengers a simplified formula for incumbents Interpret all coefficients. Remember: your interpretation of the incumb coefficient here should be different than in the first model since the models contain different sets of predictors. Use this model to predict the number of votes received by the following candidates. HINT: plug in the correct values in your model formula. Candidate 1: a challenger that spends 10,000 Euros Candidate 2: an incumbent that spends 10,000 Euros You can check your predictions using the predict() function: predict(model2, newdata = data.frame(incumb=&quot;No&quot;, totalexp = 10000)) predict(model2, newdata = data.frame(incumb=&quot;Yes&quot;, totalexp = 10000)) Check out a plot of model2. (RStudio code is included FYI, but don’t worry about it for now!) ggplot(campaigns, aes(x = totalexp, y = votes1st, color = incumb)) + geom_point(size = 0.5) + geom_abline(intercept = 1031, slope = 0.1745, color = &quot;red&quot;) + geom_abline(intercept = 3795, slope = 0.1745, color = &quot;cyan3&quot;) Since the lines are parallel, this model assumes that incumbents and challengers enjoy the same return on campaign spending. However, notice from the plot that this may not be the best assumption. Rather, without the parallel model constraint, the trend looks more like this (you’ll make this plot later): To allow our models for challengers and incumbents to have different intercepts and different slopes, we can type totalexp * incumb instead of totalexp + incumb in the lm function: new_model &lt;- lm(votes1st ~ totalexp * incumb, campaigns) summary(new_model) The totalexp:incumbYes term that you see in the model summary is called an interaction term. Mathematically, it’s the product of these two variables, totalexp * incumbYes. With this in mind, write down the model formulas: the full model formula (of the form votes1st = a + b totalexp + c incumbYes + d totalexp * incumbYes) a simplified formula for challengers (of the form votes1st = a + b totalexp) a simplified formula for incumbents (of the form votes1st = a + b totalexp) Use this model to predict the number of votes received by the following candidates. Calculate these by hand &amp; then check your work using makeFun. Candidate 1: a challenger that spends 10,000 Euros Candidate 2: an incumbent that spends 10,000 Euros predict(new_model, newdata = data.frame(incumb=&quot;No&quot;, totalexp = 10000)) predict(new_model, newdata = data.frame(incumb=&quot;Yes&quot;, totalexp = 10000)) You can visualize this model by adding a geom_smooth() to your scatterplot: ggplot(campaigns, aes(x = totalexp, y = votes1st, col = incumb)) + geom_point() + geom_smooth(method = &quot;lm&quot;) You should notice the interaction between campaign spending &amp; incumbency status, i.e. that the relationship between votes &amp; spending differs for incumbents and challengers. With this in mind, comment on what the differing intercepts and differing slopes indicate about the relationship between the three variables of interest. Putting this all together, interpret all four model coefficients from new_model. Hint: As we have for models in the past, look back to the equations for the incumbent &amp; challenger models. Interaction In modeling \\(y\\), predictors \\(x_1\\) and \\(x_2\\) interact if the relationship between \\(x_1\\) and \\(y\\) differs for different values of \\(x_2\\). 5.2.2 Covariates In examining multivariate models, we’ve seen that adding explanatory variables to the model helps to better explain variability in the response. For example, compare the plot on the left which ignores the grouping variable vs the plot on the right that includes it: However, explaining variability isn’t the only reason to include multiple predictors in a model. When exploring the relationship between response \\(y\\) and predictor \\(x_1\\), there are typically covariates for which we want to control. For example, in comparing the effectiveness of 2 drug treatments, we might want to control for patients’ ages, health statuses, etc. We’ll explore the concept of controlling for covariates using the CPS85 data in the mosaic package. These data, obtained through the Current Population Survey, contains labor force characteristics for a sample of workers in 1985. Though out of date, these data provide important illustrations of key modeling concepts: data(CPS85) head(CPS85, 3) ## wage educ race sex hispanic south married exper union age sector ## 1 9.0 10 W M NH NS Married 27 Not 43 const ## 2 5.5 12 W M NH NS Married 20 Not 38 sales ## 3 3.8 12 W F NH NS Single 4 Not 22 sales You can access the “codebook” (description of the variables) by typing the following in your console: ?CPS85 We’ll use these data to explore the pay gap between married and single workers, illustrated and modeled below: ggplot(CPS85, aes(y=wage, x=married)) + geom_boxplot() cpsmod1 &lt;- lm(wage ~ married, data = CPS85) msummary(cpsmod1) ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 9.398 0.274 34.36 &lt;2e-16 *** ## marriedSingle -1.087 0.466 -2.33 0.02 * ## ## Residual standard error: 5.12 on 532 degrees of freedom ## Multiple R-squared: 0.0101, Adjusted R-squared: 0.00826 ## F-statistic: 5.44 on 1 and 532 DF, p-value: 0.0201 From the model coefficients we see that: On average, married workers make $9.40 per hour and single workers make $1.09 less per hour than married workers. Correlation does not imply causation If you’re single, this model probably didn’t inspire you to go out and find a spouse. Why? Just because there’s a relationship between wages and marital status doesn’t mean being single causes a person to earn less. List a few confounding variables that might explain this relationship between wages and marital status. Including covariates / confounding variables One variable that might explain the observed relationship between wages &amp; marital status is years of experience - people with greater years of experience both tend to make more money and to be older &amp; married. We can control for this covariate by including it in our model. Fill in the blanks to fit a model of wage by married and exper that includes an interaction term. cpsmod2 &lt;- lm(___, data = CPS85) summary(cpsmod2) Construct a visualization of this relationship that eliminates individual data points and focuses on the trend. Use this to explain what it means for exper &amp; married to interact. ggplot(CPS85, aes(y = wage, x = exper, color = married)) + geom_smooth(method = &quot;lm&quot;) Compare two workers that both have 10 years of experience but one is married and the other is single. By how much do their predicted wages to differ? Use the model formula to calculate this difference and the plot to provide intuition. Compare two workers that both have 20 years of experience but one is married and the other is single. By how much do their predicted wages to differ? If you’d like extra practice, interpret every coefficient in this model. Controlling for more covariates Taking experience level into account added some insight into the discrepancy between single and married workers’ wages. Let’s see what happens when we control for even more variables. To this end, model wages (wage) by marital status (married) while controlling for experience (exper), years of education (educ), and the job sector (sector) in which one works. For simplicity, we’ll eliminate all interaction terms: cpsmod3 &lt;- lm(wage ~ exper + educ + sector + married, data = CPS85) summary(cpsmod3) Note: This is difficult model to visualize since there are 2 quantitative variables and 2 categorical variables with a possible 16 category combinations (2 marital statuses * 8 sectors). If you had to draw it, it would look like 16 parallel planes. Compare two workers that both have 10 years of experience, 16 years of education, and work in the service industry. If one is married and the other is single, by how much do their predicted wages to differ? Compare two workers that both have 20 years of experience, 12 years of education, and work in the manuf (manufacturing) industry. If one is married and the other is single, by how much do their predicted wages to differ? In light of a &amp; b, interpret the marriedSingle coefficient. In conclusion, we saw a marriedSingle coefficient of -1.09 in cpsmod1 and a marriedSingle coefficient of -0.40 in cpsmod3. Explain the significance of the difference between these two measurements - what insight does it provide? Extra interpretation practice If you’d like extra practice, answer the following questions related to the coefficients in cpsmod3. What is the reference level of the sector variable? HINT: You need to know what the levels of this variable are. For fixed educ, married status, and exper, in what sector do workers make the most money? The least? Interpret the educ coefficient. Interpret the manag coefficient. 5.2.3 Least Squares Estimation Thus far you’ve been using RStudio to construct models and have focused on interpreting the output. Now let’s discuss how this first step happens, ie. how sample data are used to estimate population models. Due to their simplicity and the fact that these data helped inspire Francis Galton’s development of regression methodology in the 1880’s, we’ll use the data Galton collected on the heights of a person and their parents. From the mosaic package: data(Galton) ?Galton Let response variable \\(y\\) be a person’s height and \\(x_1\\) be the height of their father. Then the (population) linear regression model of \\(y\\) vs the \\(x_1\\) is \\[y = \\beta_0 + \\beta_1 x_1\\] Note that the \\(\\beta_i\\) represent the population coefficients. Galton didn’t have data for the entire population of interest thus didn’t know the “true” values of the \\(\\beta_i\\). Rather, he used sample data to estimate the \\(\\beta_i\\) by \\(\\hat{\\beta}_i\\). That is, the sample estimate of the population model is \\[y = \\hat{\\beta}_0 + \\hat{\\beta}_1 x_1\\] In choosing the “best” estimates \\(\\hat{\\beta}_i\\), he was looking for the coefficients that best described the relationship among the sample subjects. In the visual below, we can see that the red line does a better job at capturing this relationship than the blue line does: Mainly, on average, the individual points fall closer to the red line than the blue line. The distance between an individual observation and its model value (prediction) is called a residual. Residuals Let case \\(i\\) have observed response \\(y_i\\) and predictor \\(x_i\\). Then the model / predicted value for this case is \\[\\hat{y}_i = \\hat{\\beta}_0 + \\hat{\\beta}_1 x_i\\] The difference between the observed and predicted value is the residual \\(r_i\\): \\[r_i = y_i - \\hat{y}_i\\] We can use Galton’s data to estimate the population model: \\[y = \\hat{\\beta}_0 + \\hat{\\beta}_1 x = 39.11039 + 0.39938 x\\] htmodel &lt;- lm(height ~ father, data = Galton) summary(htmodel) ## ## Call: ## lm(formula = height ~ father, data = Galton) ## ## Residuals: ## Min 1Q Median 3Q Max ## -10.268 -2.669 -0.209 2.634 11.933 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 39.1104 3.2271 12.12 &lt;2e-16 *** ## father 0.3994 0.0466 8.57 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 3.45 on 896 degrees of freedom ## Multiple R-squared: 0.0758, Adjusted R-squared: 0.0748 ## F-statistic: 73.5 on 1 and 896 DF, p-value: &lt;2e-16 Consider the following 2 sample subjects with the following measurements and plotted below: ## family father mother sex height nkids ## 14 4 75 64 F 64.5 5 ## 849 193 65 64 M 67.0 6 Calculate the residuals (the length of the vertical lines) for both of these subjects. htmodel is an lm “object”. Not only does it contain info about the model coefficients, it contains the numerical values of the residuals (residuals) and model predictions (fitted.values) for each case in the data set. Create a data frame htmodel_results that stores the observed height, model predicted height, and residual for each case in Galton: htmodel_results &lt;- data.frame(observed = Galton$height, predicted = htmodel$fitted.values, residual = htmodel$residuals) head(htmodel_results) ## observed predicted residual ## 1 73.2 70.46 2.738 ## 2 69.2 70.46 -1.262 ## 3 69.0 70.46 -1.462 ## 4 69.0 70.46 -1.462 ## 5 73.5 69.26 4.236 ## 6 72.5 69.26 3.236 What is the relationship between the observed, predicted, and residual variables in htmodel_results? (This shouldn’t be a surprise - it’s just a confirmation of the definition of a residual.) Confirm that, within rounding error, the mean residual equals 0. This property always holds for regression models! Obtain summary statistics of the residuals. Where does this information appear in summary(htmodel)? EXTRA: A COMMENT ON THEORY I hope you learned about linear regression in linear algebra! Suppose we have a sample of \\(n\\) subjects. For subject \\(i \\in \\{1,...,n\\}\\) let \\(y_i\\) denote the observed response value and \\((x_{i1}, x_{i2},...,x_{ik})\\) denote the observed values of the \\(k\\) predictors. Then we can collect our response values into a vector \\(y\\), our predictor values into a matrix \\(X\\), and our regression coefficients into a vector \\(\\beta\\). Note that a column of 1s is included for an intercept term in \\(X\\): \\[ y = \\left(\\begin{array}{c} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_n \\end{array} \\right) \\;\\;\\;\\; \\text{ and } \\;\\;\\;\\; X = \\left(\\begin{array}{ccccc} 1 &amp; x_{11} &amp; x_{12} &amp; \\cdots &amp; x_{1k} \\\\ 1 &amp; x_{21} &amp; x_{22} &amp; \\cdots &amp; x_{2k} \\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\cdots &amp; \\vdots \\\\ 1 &amp; x_{n1} &amp; x_{n2} &amp; \\cdots &amp; x_{nk} \\\\ \\end{array} \\right) \\;\\;\\;\\; \\text{ and } \\;\\;\\;\\; \\beta = \\left(\\begin{array}{c} \\beta_1 \\\\ \\beta_2 \\\\ \\vdots \\\\ \\beta_k \\end{array} \\right)\\] Then we can express the model \\(y_i = \\beta_0 + \\beta_1 x_{i1} + \\cdots + \\beta_k x_{ik}\\) for \\(i \\in \\{1,...,n\\}\\) using linear algebra: \\[y = X\\beta\\] Further, let \\(\\hat{\\beta}\\) denote the vector of sample estimated \\(\\beta\\) and \\(\\hat{y}\\) denote the vector of predictions / model values: \\[\\hat{y} = X \\hat{\\beta}\\] Thus the residual vector is \\[y - \\hat{y} = X\\beta - X\\hat{\\beta}\\] and the sum of squared residuals is \\[(y - \\hat{y})^T(y - \\hat{y})\\] Challenge: Prove that the following formula for sample coefficients \\(\\hat{\\beta}\\) are the least squares estimates of \\(\\beta\\), ie. they minimize the sum of squared residuals: \\[\\hat{\\beta} = (X^TX)^{-1}X^Ty\\] -->"]
]
